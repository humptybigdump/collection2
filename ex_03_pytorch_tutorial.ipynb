{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr-Dy_f8wwI7"
   },
   "source": [
    "# Short PyTorch Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhrrIqTxwwI8"
   },
   "source": [
    "Wir gehen hier kurz auf die wichtigsten Konzepte von PyTorch ein, da wir es in der nächsten Übung verwenden werden. In dieser kurzen Introduction schauenwir uns kurz ein paar der wichtigsten Konzepte in PyTorch an und trainieren ein einfaches neuronales Netzwerk. Tiefergehende Informationen findet ihr in der PyTorch Dokumentation zum Beispiel [hier](https://pytorch.org/tutorials/beginner/basics/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MJ7O3DaSwwI9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensoren\n",
    "\n",
    "Tensoren sind das Fundament in PyTorch und vergleichbar mit Numpy-Arrays. Im Gegensatz zu Numpy-Arrays haben Tensoren eine `.device` Eigenschaft, die angibt, auf welchem Gerät (CPU oder GPU) sie sich befinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qsw8zmFfwwI9",
    "outputId": "594f5b91-8b70-40e5-a488-284ef2f976b5"
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "test_tensor = torch.tensor(data)\n",
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JVD8DJO5wwI9",
    "outputId": "4ce1aef0-8727-4b60-eeb6-ef3a8b5bf808"
   },
   "outputs": [],
   "source": [
    "print('Test Tensor on device: ', test_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können mit dem folgenden Code prüfen, ob eine GPU verfügbar ist, und verschieben die Daten ggf. auf die GPU (Wenn ihr Google colab verwendet, könnt ihr eine Runtime mit GPU auswählen): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbKkNM1QwwI-"
   },
   "outputs": [],
   "source": [
    "print('GPU available: ', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    test_tensor = test_tensor.to(\"cuda\")\n",
    "\n",
    "print('Test Tensor on device: ', test_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten laden\n",
    "Wir laden und verarbeiten hier wie in der Übung den MNIST-Datensatz, der handgeschriebene Ziffern enthält. Die Daten werden für das Training vorbereitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GV0mbPI0wwI-",
    "outputId": "b84c760f-92a8-4ce5-a211-117a6ba02b7c"
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: x.squeeze(0))  # Remove the single channel dimension\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOR4B8lWwwI-",
    "outputId": "6bd12649-8814-4a7d-b38d-4b3d63518b6d"
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "tDE9fLc2wwI-",
    "outputId": "428cf862-70ea-41ce-e4b0-57cdfa4e2006"
   },
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "features, label = train_dataset[0]\n",
    "plt.imshow(features, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GIkEWkMwwI-"
   },
   "source": [
    "### Batch-Verarbeitung mit DataLoader\n",
    "Der Datensatz gibt uns für ein gegebenen index die Features und Label von genau einem sample zurück. Beim Trainieren eines Modells wollen wir mehrere Samples in „Minibatches“ an das Modell übergeben, die Daten bei jeder Epoche neu mischen, um eine Overfitting des Modells zu vermeiden, und das Multiprocessing von Python nutzen, um den Datenabruf zu beschleunigen.\n",
    "\n",
    "DataLoader abstrahiert die diese Komplexität für uns in einer einfachen API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nByoOw-awwI_"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "9a2pMFFowwI_",
    "outputId": "8a64b6a4-4d8a-4dc5-a0d0-cf78f5c5258e"
   },
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trJK73BywwI_"
   },
   "source": [
    "### Neuronales Netz definieren\n",
    "Wir definieren unser neuronales Netz als `nn.Module` und initialisieren die Struktur des neuronalen Netzes in `__init__`. Jede `nn.Module`-Unterklasse implementiert die Operationen mit den Eingabedaten in der Forward-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vvu8j8ciwwI_"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, structure):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        layers = []\n",
    "\n",
    "        for i in range(len(structure) - 1):\n",
    "            layers.append(nn.Linear(structure[i], structure[i + 1]))\n",
    "            if i < len(structure) - 2:  # Add activation for all layers except the last\n",
    "                layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.model(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCnD0BRQwwI_"
   },
   "outputs": [],
   "source": [
    "structure = [784, 128, 10]\n",
    "learning_rate = 0.001 # Testet das Modell z.B. auch mal mit einer größeren Learning Rate, z.B. 0.1. Wie ändert sich die Performance?\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSDw5YKBwwI_"
   },
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = NeuralNetwork(structure).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainings- und Testschleife\n",
    "\n",
    "Wir trainieren das Modell und evaluieren es in jeder Epoche. Die Trainingsschleife berechnet die Verluste und aktualisiert die Gewichte. Die Backpropagation findet in den folgenden drei Zeilen statt:\n",
    "- `loss.backward()` : Berechnet die Gradienten der Verlustfunktion bezüglich der Gewichte\n",
    "- `optimizer.step()` : Aktualisiert die Gewichte basierend auf den Gradienten und der Lernrate\n",
    "- `optimizer.zero_grad()` : Setzt die Gradienten, die in den `.grad`-Attributen der Parameter gespeichert sind, wieder auf Null zurück für den nächsten Durchlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UN18FwO_wwI_",
    "outputId": "e3fcaca7-6be5-48f5-aaa0-1b3740ab5737"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "    ### Train loop\n",
    "\n",
    "    size = len(train_loader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_loader):\n",
    "        # Move data and labels to the same device as the model\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    ### Test loop\n",
    "\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(test_loader.dataset)\n",
    "    num_batches = len(test_loader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # Move data and labels to the same device as the model\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation mit `torch.autograd` verstehen\n",
    "\n",
    "Um diese Gradienten zu berechnen, hat PyTorch eine integrierte Differentiation Engine namens `torch.autograd`. \n",
    "Wir betrachten um das zu veranschaulichen hier das einfachste einschichtige neuronale Netz mit der Eingabe `x`, den Parametern `w` und `b` und einer Lossfunktion. \n",
    "In diesem Netz sind `w` und `b` Parameter, die wir optimieren müssen. Daher müssen wir in der Lage sein, die Gradienten der Verlustfunktion in Bezug auf diese Variablen zu berechnen. Aus diesem Grund setzen wir die Eigenschaft `requires_grad` dieser Tensoren auf `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKltEfyNwwI_"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z9A9N6IwwI_"
   },
   "source": [
    "![](comp-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für Funktionen, die wir in PyTorch auf Tensoren anwenden, um den Computational Graph zu konstruieren,  ist unterliegend nicht nur definiert wie die Vorwärtsrichtung zu berechnen ist, sondern auch wie ihre Ableitung während des Backpropagationsschritts zu berechnen ist. Ein Verweis auf die zugehörige Backpropagationsfunktion in dem Computational Graph wird in der Eigenschaft `grad_fn` eines Tensors gespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUZpy7SZwwI_",
    "outputId": "9e0bc0f7-3dde-4937-f601-e73345fb7eca"
   },
   "outputs": [],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Gewichte der Parameter im neuronalen Netz zu optimieren, müssen wir die Ableitungen unserer Verlustfunktion in Bezug auf die Parameter berechnen, d. h. wir benötigen \n",
    "$ \\frac{\\partial L}{\\partial w} $ und $ \\frac{\\partial L}{\\partial b} $.\n",
    "Um diese Ableitungen über unsern computational graph zu berechnen, rufen wir `loss.backward()` auf.\n",
    "Anschließend könenn wir dien Gradienten für die einzelnen Parameter in `w.grad` und `b.grad` finden.\n",
    "Bis zu diesem Punkt haben wir nur die Gradienten berechnet, aber noch keine Gewichtsaktualisierung durchgeführt. \n",
    "Um die Gewichte zu aktualisieren, müssten wir zuvor einen Optimizer initialisieren, der die Gewichte aktualisiert.\n",
    "Dann könnten wir wie zuvor in dem Trainingloop `optimizer.step()` aufrufen, um die Gewichte basierend auf den Gradienten und der Lernrate zu aktualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gz64pw9vwwI_",
    "outputId": "91263405-3e9d-435e-d217-5dd6a94fb7fb"
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPpaQudMwwI_"
   },
   "source": [
    "Pytorch's Autograd hält die Daten (Tensoren) und alle durchgeführten Operationen (zusammen mit den daraus resultierenden neuen Tensoren) in einem gerichteten azyklischen Graphen (DAG) fest, der aus Funktionsobjekten besteht. In diesem DAG sind die Blätter die Eingabe-Tensoren und die Wurzeln die Ausgabe-Tensoren. Wenn man diesen Graphen von den Wurzeln zu den Blättern zurückverfolgt, kann man die Gradienten unter Verwendung der Kettenregel automatisch berechnen.\n",
    "\n",
    "In einem Forward Pass führt autograd zwei Dinge gleichzeitig aus:\n",
    "- die angeforderte Operation durchführen, um einen resultierenden Tensor zu berechnen\n",
    "- Beibehaltung der Gradientenfunktion der Operation in der DAG.\n",
    "\n",
    "Der Backward Pass beginnt, wenn .backward() auf der DAG-Wurzel (loss) aufgerufen wird. Autograd\n",
    "- berechnet die Gradienten von jedem .grad_fn,\n",
    "- akkumuliert sie im Attribut .grad eines jeden Tensors bis zum Input"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gki_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
