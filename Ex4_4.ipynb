{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Exercise 4 Multilayer Perceptron Implementation </center>\n",
    "\n",
    "<span style=\"background: yellow\"><b>Edits</b></span>:\n",
    "<ul>\n",
    "    <li><span style=\"background: yellow\"> v2 (May 18): Edits in documentation of 5.2. and 5.3., fixed target values in test case of 5.3.</span>\n",
    "    <li><span style=\"background: yellow\"> v3 (May 19): Test case of 6.2: Random seed should be 42, not 6</span>\n",
    "</ul>\n",
    "\n",
    "</br>\n",
    "On this exercise, you are implementing a feedforward neural network from scratch to do the MNIST problem. This one is using <code>Numpy</code>. As an exercise for Backpropagation as you learned from the lecture, you should work through the derivation of the loss function to the weights or biases with pen and paper. \n",
    "</br>\n",
    "\n",
    "We will try to modularize our neural network as it consists of $L$ layers. **Read the following description carefully, otherwise you would have problems implementing it later**. Each layer $l$ has a weight matrix $W^{(l)}$ with the size of $(n^{(l)}, n^{(l-1)})$ where $n^{(l)}$ is the number of neurons (outputs) of the layer $l$, a bias vector $b^{(l)}$ with the size of $(n^{(l)}, 1)$. Each layer $l$ has an activation function $f$. Each layer $l$ can be the last layer or not (the error term is calculated differently in that case). First, we will implement those parts for one layer:\n",
    "    \n",
    "* Initialize a layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization. (This is already given in the code)\n",
    "* Do the forward pass through the layer based on its activation function:\n",
    "    *  $$ z^{(l)} = W^{(l)}\\cdot a^{(l-1)} + b^{(l)} $$\n",
    "        $$ a^{(l)} = f(z^{(l)}) $$\n",
    "    *  Where:<br/>\n",
    "        $z^{(l)}$: intermediate net output of layer $l$<br/>\n",
    "        $a^{(l)}$: activation of layer $l$, is the input of the next layer $l+1$, $a^{(0)} = X$<br/>\n",
    "* Do the backward pass through the layer. Using the convenient notation $\\check{w} = \\frac{\\partial \\mathcal{L}}{\\partial w}$ (in the code, $\\check{w}$ is denoted by dw):<br/>\n",
    "$$\\check{w}^{(l)} = \\check{a}^{(l)} \\frac{\\partial a^{(l)}}{\\partial z^{(l)}}\\frac{\\partial z^{(l)}}{\\partial w^{(l)}}= \\check{a}^{(l)}f^{(l)'}a^{(l-1)}$$\n",
    "* Update the weights/biases based on the gradient descent principle.\n",
    "\n",
    "Then training the network is divided into several steps:\n",
    "* Initialize the whole network (by initilizing every layer)\n",
    "* Split training data into minibatches.\n",
    "* Iteratively do following steps in $E$ epochs:\n",
    "    * Shuffle minibatches.\n",
    "    * Do the forward pass.\n",
    "    * Calculate the loss.\n",
    "    * Do the backward pass.\n",
    "    * Update weights.\n",
    "    \n",
    "With the trained model, perform inference steps on the test data.  \n",
    "  \n",
    "\n",
    "The implementation uses mini-batches, i.e. multiple input samples are aggregated into one tensor and passed through the forward and backward pass all at once. This means, if a single data sample is a vector $x \\in R^n$, then the vectorized implementation has to work on inputs $x \\in R^{n \\times m}$, where $m$ is the batch size (in this exercise, we put the batch dimension at the end of the tensor). Thanks to `numpy`, such vectorized code often looks \n",
    "the same or very similar to the non-vectorized version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import `numpy` and explore some basic methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement basic activation functions (vectorized version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperbolic Tangent__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax for batch__:\n",
    "</br>\n",
    "\n",
    "Note that if we do minibatch update, we must compute the softmax along some dimension that is not the batch dimension. From our convention, batch dimension is the second (axis=1), so we must compute the softmax along the first dimension (axis=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_batch(z, axis=0):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "z = np.random.randn(100,2)\n",
    "\n",
    "sigmoid_z = sigmoid(z)\n",
    "tanh_z = tanh(z)\n",
    "my_tanh_z = np.tanh(z)\n",
    "relu_z = relu(z)\n",
    "linear_z = linear(z)\n",
    "softmax_z = softmax(z)\n",
    "\n",
    "\n",
    "print(\"Sigmoid results of z[0]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_z[0,0], sigmoid_z[6,0], sigmoid_z[8,0], sigmoid_z[22,0], sigmoid_z[98,0]))\n",
    "print(\"Tanh results of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_z[3,0], tanh_z[9,0], tanh_z[18,0], tanh_z[34,0], tanh_z[99,0]))\n",
    "print(\"Relu results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_z[8,0], relu_z[16,0], relu_z[38,0], relu_z[72,0], relu_z[88,0]))\n",
    "print(\"Linear results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(linear_z[2,0], linear_z[6,0], linear_z[8,0], linear_z[22,0], linear_z[98,0]))\n",
    "print(\"Softmax results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(softmax_z[2,0], softmax_z[6,0], softmax_z[8,0], softmax_z[22,0], softmax_z[98,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "Sigmoid results of z[0]=0.615723, z[6]=0.721783, z[8]=0.599997, z[22]=0.561633, z[98]=0.459803 <br/>\n",
    "Tanh results of z[3]=0.696046, z[9]=0.867072, z[18]=0.124070, z[34]=0.208769, z[99]=0.280640 <br/>\n",
    "Relu results of z[8]=0.405453, z[16]=1.047579, z[38]=0.226963, z[72]=1.176812, z[88]=2.123692 <br/>\n",
    "Linear results of z[2]=-0.720589, z[6]=0.953324, z[8]=0.405453, z[22]=0.247792, z[98]=-0.161137 <br/>\n",
    "Softmax results of z[2]=0.001570, z[6]=0.008374, z[8]=0.004842, z[22]=0.004136, z[98]=0.002748 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement the derivations of basic activation functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tanh__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__: \n",
    "\n",
    "This is hard to come up with without knowing the loss function. Most of the implementations out there are assuming we have cross entropy (or negative log likelihood) loss function. We will do the same, as the derivative of Softmax followed directly by Cross-Entropy Loss is far easier (see below). So, no need to implement Softmax derivative here. For your interest: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "\n",
    "This constrains the use of softmax to the last layer (where the loss function is applied directly on top of it), but that's fine for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8888)\n",
    "z = np.random.randn(100,1)\n",
    "\n",
    "sigmoid_prime_z = sigmoid_prime(z)\n",
    "tanh_prime_z = tanh_prime(z)\n",
    "relu_prime_z = relu_prime(z)\n",
    "\n",
    "print(\"Sigmoid prime results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_prime_z[2,0], sigmoid_prime_z[6,0], sigmoid_prime_z[8,0], sigmoid_prime_z[22,0], sigmoid_prime_z[98,0]))\n",
    "print(\"Tanh prime of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_prime_z[3,0], tanh_prime_z[9,0], tanh_prime_z[18,0], tanh_prime_z[34,0], tanh_prime_z[99,0]))\n",
    "print(\"Relu prime results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_prime_z[8,0], relu_prime_z[16,0], relu_prime_z[38,0], relu_prime_z[72,0], relu_prime_z[88,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "Sigmoid prime results of z[2]=0.247928, z[6]=0.228343, z[8]=0.181437, z[22]=0.249350, z[98]=0.133364<br/>\n",
    "Tanh prime of z[3]=0.027258, z[9]=0.928484, z[18]=0.357361, z[34]=0.621576, z[99]=0.568175<br/>\n",
    "Relu prime results of z[8]=1.000000, z[16]=0.000000, z[38]=1.000000, z[72]=0.000000, z[88]=1.000000<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implement basic cost (error) function and its derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are going to calculate the error for a (mini)batch of $m$ training examples, which we assume they are [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) (i.i.d), so the (normalized) total errors of those $m$ training examples (or $m$ data points) is $E = \\frac{1}{m}\\sum_{i=1}^mE^{(i)}$, where $E^{(i)}$ is an error value of the $i^{th}$ training example. However, in order to vectorize the error over a (mini)batch, we should come up with the direct version of error function of $E$ instead of $E^{(i)}$.\n",
    "\n",
    "We implement Cross Entropy loss here, since we are trying to solve a classification problem with multiple classes (0-9). In the two class case, we could use Binary Cross Entropy, and for regression problems, Mean Squared Error, for instance. If you want to know more, look e.g. [here](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Categorical) Cross Entropy Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyError(t, y, epsilon=1e-15):\n",
    "    # t: target label of shape(n,m)\n",
    "    # y: predicted value of shape(n,m)\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: <b>Derivative of Cross Entropy</b><br/>\n",
    "The cross entropy-function expects its input $y$ to be a normalized probability distribution (or a batch thereof). Thus, the activiation function directly before the loss (i.e. the last activation function of the network) is usually the softmax function. You could calculate the derivate $\\frac{dL}{dy}$ and then $\\frac{dy}{dz}$ step by step, where $L = CE(t, y)$ is the CE loss and $y = \\text{softmax}(z)$. The input $z$ to the softmax is usually called the \"logits\". However, if you compute $\\frac{dL}{dz}$ analytically, you can show the following simple formula:<br/>\n",
    "\n",
    "$\\frac{dL}{dz} = y - t$\n",
    "\n",
    "You can implement this directly, just don't forget to properly handle the normalization due to the batched implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dCE_dz(t, y):    \n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handy helper function\n",
    "def create_random_onehot(n, m):\n",
    "    index = np.eye(n)\n",
    "    return index[np.random.choice(index.shape[1], size=m)].T\n",
    "t = create_random_onehot(3, 7)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward\n",
    "\n",
    "np.random.seed(1111)\n",
    "z = np.random.randn(100,50)\n",
    "sum_z = np.sum(z, axis=0, keepdims=True)\n",
    "y = z / sum_z # Create softmax-like distribution\n",
    "t = create_random_onehot(100,50)\n",
    "\n",
    "# Test forward\n",
    "myCE = CrossEntropyError(t,y)\n",
    "print(\"My Cross Entropy Error: \" + str(myCE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Cross Entropy Error: 15.8303339039 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backward\n",
    "\n",
    "dCE_dz1 =  dCE_dz(t, z)\n",
    "    \n",
    "print(\"My Cross Entropy Derivation: dCE_dz at dCE_dz[2,4]=%f, dCE_dz[6,5]=%f, dCE_dz[8,20]=%f, dCE_dz[22,7]=%f, dCE_dz[98,40]=%f\" \\\n",
    "      %(dCE_dz1[2,4], dCE_dz1[6,5], dCE_dz1[8,20], dCE_dz1[22,7], dCE_dz1[98,40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Cross Entropy Derivation: dCE_dz at dCE_dz[2,4]=-0.000196, dCE_dz[6,5]=0.020035, dCE_dz[8,20]=0.025941, dCE_dz[22,7]=-0.001366, dCE_dz[98,40]=0.013498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Layer Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Initialization\n",
    "\n",
    "Initialize a layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n_in, n_out, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the initialization for our layer\n",
    "\n",
    "    Arguments:\n",
    "    n_in -- size of previous layer\n",
    "    n_out -- size of current layer\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_string == 'sigmoid' or activation_string == 'tanh' or activation_string == 'softmax':\n",
    "        # Xavier Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(2. / (n_in + n_out)))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    else: \n",
    "        # He Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(1. / n_in))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background: yellow\"><b>Edit</b>:\n",
    "    activation_string should be part of cache. The code documentation of 5.3. was also updated accordingly.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A_prev, W, b, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for our layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data X): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- values of (A_prev, Z, W, activation_string) we store for computing backward propagation efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    \n",
    "    #return A, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test forward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2222)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "\n",
    "A, cache = forward(A_prev, W, b, \"sigmoid\")\n",
    "print(\"With sigmoid: Z = %s , A = %s\" % (str(cache[1]), str(A)))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"tanh\")\n",
    "print(\"With tanh: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"softmax\")\n",
    "print(\"With softmax: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "With sigmoid: Z = [[ 1.1337088  -1.20334105]] , A = [[ 0.75652269  0.2308814 ]]<br/>\n",
    "With tanh: A = [[ 0.81228478 -0.83467086]]<br/>\n",
    "With ReLU: A = [[ 1.1337088  0.       ]]<br/>\n",
    "With softmax: A = [[ 1. 1.]]\n",
    "\n",
    "Can you explain why the softmax output is only ones?\n",
    "What would have to be changed to get more \"useful\" values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hint</b>: Keep in mind that, if the activation function is softmax, you can assume that dA is already dz (see the explanation for CE derivative above). This assumes that softmax is used only as the last layer of the network, but that is ok for this exercise.\n",
    "\n",
    "<span style=\"background: yellow\"><b>Edit</b>:\n",
    "    activation_string is part of the cache. See 5.2. above.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for our layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- activation gradient for current layer l \n",
    "    cache -- values of (A_prev, Z, W, activation_string) we store for computing backward propagation efficiently, where\n",
    "        activation_string is the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\", \"tanh\" or \"softmax\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    \n",
    "    #return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Backward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1111)\n",
    "dA = np.random.randn(1,2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "\n",
    "cache_sigmoid = (A_prev, Z, W, \"sigmoid\")\n",
    "cache_tanh = (A_prev, Z, W, \"tanh\")\n",
    "cache_relu = (A_prev, Z, W, \"relu\")\n",
    "cache_softmax = (A_prev, Z, W, \"softmax\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_sigmoid)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_tanh)\n",
    "print (\"tanh:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_relu)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_softmax)\n",
    "print (\"softmax:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "sigmoid:<br/>\n",
    "dA_prev = [[ 0.38601836  0.4093035 ]<br/>\n",
    " [ 0.04975095  0.05275199]<br/>\n",
    " [ 0.37804364  0.40084773]]<br/>\n",
    "<span style=\"background: yellow\">dW = [[ 0.01722623 -0.1803237   0.09144594 ]]</span><br/>\n",
    "db = [[-0.25163402]]<br/>\n",
    "<br/>\n",
    "tanh:<br/>\n",
    "dA_prev = [[ 0.74460672  1.47719476]<br/>\n",
    " [ 0.09596665  0.19038431]<br/>\n",
    " [ 0.72922394  1.4466775 ]]<br/>\n",
    "<span style=\"background: yellow\">dW = [[ 0.22431658 -0.34262738  0.38665635]]</span><br/>\n",
    "db = [[-0.70296173]]<br/>\n",
    "<br/>\n",
    "relu:<br/>\n",
    "dA_prev = [[ 2.05442539  0.        ]<br/>\n",
    " [ 0.26477914  0.        ]<br/>\n",
    " [ 2.01198317  0.        ]]<br/>\n",
    "<span style=\"background: yellow\">dW = [[-0.51363355 -0.97619193 -0.17936819]]</span><br/>\n",
    "db = [[-0.65000516]]<br/>\n",
    "\n",
    "softmax:<br/>\n",
    "dA_prev = [[2.05442539 1.69566033]<br/>\n",
    " [0.26477914 0.21854066]<br/>\n",
    " [2.01198317 1.66062981]]<br/>\n",
    "dW = [[-0.04244893 -0.96335391  0.3390964 ]]<br/>\n",
    "db = [[-1.18649968]]<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Update parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(lr, W, b, dW, db):\n",
    "    W1 = W - lr * dW \n",
    "    b1 = b - lr * db\n",
    "    return W1, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Network Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_initialize(layer_sizes = [768,100,50,10], activations = [\"relu\",\"relu\",\"softmax\"], seed=9999):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of a network.\n",
    "    \n",
    "    Arguments:\n",
    "    layer_sizes -- A list of layer sizes. \n",
    "    activations -- A list of corresponding activation functions.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Dictionary of initialized weights and biases for every layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "        \n",
    "    #return parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "parameters = network_initialize([2,3,4],[\"relu\",\"softmax\"])\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "{ &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W1': array([[-0.36418784, &nbsp;  &nbsp;  0.41853418], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [ 0.03385161, &nbsp;  &nbsp; -0.34535427], <br/>\n",
    "   &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.25098956, &nbsp;  &nbsp; -0.27705387]]), <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W2': array([[ 0.99648945, &nbsp;  &nbsp; -0.73309211, &nbsp;  &nbsp;  1.18999424], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [-0.06313226,  &nbsp;  &nbsp; 0.06406165, &nbsp;  &nbsp;  0.09760321], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.35157642, &nbsp;  &nbsp; -0.87994782, &nbsp;  &nbsp; -0.61020235], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.76922564, &nbsp;  &nbsp;  0.39821514, &nbsp;  &nbsp; -0.92965227]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;    'act1': 'relu', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'act2': 'softmax', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'b1': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;   'b2': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]])} <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Network Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Do the forward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- The input of the size (n, m): n features and m instances (m = batch size).\n",
    "    parameters -- The weights and biases of every layers in the network.\n",
    "    \n",
    "    Returns:\n",
    "    A -- Final activations (output activations).\n",
    "    caches -- the cached values for faster calculation of the backward step later.\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev = X\n",
    "    L = len(parameters) // 3 # Number of layers in the network\n",
    "    caches = []\n",
    "    \n",
    "    for l in range(L):\n",
    "        # Your implementation here!\n",
    "        pass\n",
    "    \n",
    "    #return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(5,4)\n",
    "W1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "W2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(3,3)\n",
    "b3 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"relu\",\n",
    "              \"act3\": \"softmax\"}\n",
    "AL, caches = network_forward(X, parameters)    \n",
    "print(\"AL = \" + str(AL))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <pre>\n",
    "AL = [[0.422804   0.04774435 0.01231299 0.3644841 ]\n",
    " [0.15449364 0.18041113 0.0384848  0.05945517]\n",
    " [0.42270236 0.77184451 0.94920221 0.57606073]]</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Calculate the error and error term of the last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(T, AL, error_string):\n",
    "    \"\"\"\n",
    "    Calculate the error and the error term of the last layer.\n",
    "    \n",
    "    Arguments:\n",
    "    T -- The target labels of the size (n, m): n features and m instances (m = batch size).\n",
    "    AL -- Final activations (output activations from the last layer).\n",
    "    error_string -- The string representing the error function: \"ce\".\n",
    "    \n",
    "    Returns:\n",
    "    error -- The error value.\n",
    "    dAL -- the error term (the derivative of the error w.r.t the logits) of the last layer.\n",
    "    \"\"\"\n",
    "    if error_string == 'ce':\n",
    "        error = CrossEntropyError(T, AL)\n",
    "        dAL = dCE_dz(T, AL)\n",
    "    else:\n",
    "        raise NameError(\"Your error string '%s' is undefined!\" % error_string)\n",
    "        \n",
    "    return error, dAL\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Network Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_backward(dAL, caches):\n",
    "    \"\"\"\n",
    "    Do the backward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    dAL -- the error term (the derivative of the error w.r.t the logits) of the last layer.\n",
    "    caches -- the cached values from the forward step before.\n",
    "   \n",
    "    Returns:\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the weights and biases.\n",
    "    \"\"\"\n",
    "    L = len(caches) # Number of layers in the network\n",
    "    grads = {}\n",
    "    \n",
    "    dA = dAL\n",
    "    \n",
    "    for l in reversed(range(1,L+1)):\n",
    "        # Your implementation here!\n",
    "        pass\n",
    "    \n",
    "    #return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,6)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(3,3)\n",
    "b3 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"sigmoid\",\n",
    "              \"act3\": \"softmax\"}\n",
    "\n",
    "t = create_random_onehot(1,6)\n",
    "\n",
    "\n",
    "\n",
    "AL, caches = network_forward(X, parameters)\n",
    "\n",
    "print(\"My activation last layer AL:\\n\", AL)\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\" My error:\" + str(error))\n",
    "\n",
    "my_A2, my_Z3, _, _ = caches[2]\n",
    "\n",
    "#my_dZ3 = dAL * sigmoid_prime(my_Z3)\n",
    "\n",
    "my_dZ3 = dAL * softmax(my_Z3)\n",
    "s = my_dZ3.sum(axis=my_dZ3.ndim - 1, keepdims=True)\n",
    "my_dZ3 -= s * softmax(my_Z3) \n",
    "\n",
    "\n",
    "\n",
    "m = 1 # my_A2.shape[1]\n",
    "my_dW3 = np.dot(my_dZ3, my_A2.T)\n",
    "my_db3 = np.sum(my_dZ3, axis=1, keepdims=True)\n",
    "my_dA2 = np.dot(W3.T, my_dZ3)\n",
    "\n",
    "\n",
    "my_A1, my_Z2, _, _ = caches[1]\n",
    "my_dZ2 = my_dA2 * relu_prime(my_Z2)\n",
    "my_dW2 = np.dot(my_dZ2, my_A1.T)\n",
    "my_db2 = np.sum(my_dZ2, axis=1, keepdims=True)\n",
    "my_dA1 = np.dot(W2.T, my_dZ2)\n",
    "\n",
    "my_A0, my_Z1, _, _ = caches[0]\n",
    "my_dZ1 = my_dA1 * relu_prime(my_Z1)\n",
    "my_dW1 = np.dot(my_dZ1, my_A0.T)\n",
    "my_db1 = np.sum(my_dZ1, axis=1, keepdims=True)\n",
    "my_dA0 = np.dot(W1.T, my_dZ1)\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\"My W1 grad dW1:\\n\" + str(my_dW1)) \n",
    "print(\"My b1 grad db1:\\n\" + str(my_db1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <pre>\n",
    "My activation last layer AL:\n",
    " [[0.09088608 0.07286784 0.05814823 0.05320167 0.11602662 0.07744425]\n",
    " [0.14055201 0.12887469 0.09788826 0.07209716 0.03170778 0.08802502]\n",
    " [0.76856191 0.79825747 0.84396351 0.87470117 0.8522656  0.83453074]]\n",
    "========================\n",
    " My error:5.247787227725874\n",
    "========================\n",
    "My W1 grad dW1:\n",
    "[[-3.55816200e-03 -2.03557370e-02  2.62526818e-04  6.29708234e-04\n",
    "  -1.58682250e-02 -6.07803260e-03  7.23596699e-03]\n",
    " [-4.12026165e-03  1.70212378e-03  3.21669546e-04  2.18363292e-03\n",
    "   1.19729208e-03  1.92457456e-03  1.78100582e-03]\n",
    " [ 9.82892265e-05  4.11771156e-03 -6.45936229e-04  1.02518342e-03\n",
    "   1.50412162e-03 -4.95592210e-03 -9.98074469e-03]\n",
    " [ 3.71604024e-03 -1.18149747e-03  2.67016538e-03  2.26150449e-03\n",
    "  -8.44693518e-04 -5.23969785e-03 -6.96325589e-03]\n",
    " [ 4.52891773e-04  4.01409213e-04  6.15389899e-04  1.16153033e-03\n",
    "   9.93284482e-04  3.40887741e-04 -6.78793063e-04]]\n",
    "My b1 grad db1:\n",
    "[[-0.01554985]\n",
    " [ 0.00288528]\n",
    " [ 0.0033295 ]\n",
    " [ 0.00676918]\n",
    " [ 0.0018925 ]]</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4. Update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_update(lr, parameters, grads):\n",
    "    \"\"\"\n",
    "    Update the parameters of all layers.\n",
    "    \n",
    "    Arguments:\n",
    "    lr -- learning rate.\n",
    "    parameters -- the parameters of the network to be updated.\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the parameters.\n",
    "   \n",
    "    Returns:\n",
    "    parameters -- the updated parameters of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 3 # Number of layers in the network\n",
    "    for l in range(L):\n",
    "        # Your implementation here!\n",
    "        pass\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,42)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"softmax\"}\n",
    "\n",
    "t = create_random_onehot(3,42)\n",
    "\n",
    "\n",
    "AL, caches = network_forward(X, parameters)\n",
    "assert (np.sum(AL,axis=0) - 1.0 < 1e-7).all()\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "grads = network_backward(dAL, caches)\n",
    "\n",
    "parameters = network_update(0.5, parameters, grads)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <pre>\n",
    "{   'W1': array([[-0.06433109,  0.10138926, -1.15591263, -0.68064486, -0.1148751 ,\n",
    "         0.32687147, -0.16838151],\n",
    "       [ 1.39317158,  0.74379015,  0.81427678, -1.75633047, -0.39980185,\n",
    "         0.20249417, -0.37764711],\n",
    "       [ 0.22868152, -0.56824287, -0.83499866, -0.04913708,  0.08748532,\n",
    "        -0.29328866, -0.04104647],\n",
    "       [ 0.96739929,  0.50522137,  0.81623356,  0.60461667,  0.11949221,\n",
    "        -0.31723795,  1.84408903],\n",
    "       [ 0.154503  ,  0.80559264, -0.58818036, -1.38825624, -1.18751226,\n",
    "        -0.2753193 , -1.14249442]]),\n",
    "    'W2': array([[-0.41547604,  2.71436799,  0.46541267,  0.14986723, -0.38547428],\n",
    "       [ 1.30158278,  1.23631875, -2.34764109,  0.02390676,  0.32490526],\n",
    "       [ 0.62120113, -1.22033908,  0.03224472,  1.77747704,  1.20463806]]),\n",
    "    'act1': 'relu',\n",
    "    'act2': 'softmax',\n",
    "    'b1': array([[ 0.82798283],\n",
    "       [-0.37387181],\n",
    "       [-0.4301672 ],\n",
    "       [ 1.65896485],\n",
    "       [-0.31465256]]),\n",
    "    'b2': array([[-0.92800218],\n",
    "       [ 0.39610917],\n",
    "       [-0.66433073]])}</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Read MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Data/mnist_seven.csv\"\n",
    "data = np.genfromtxt(data_path, delimiter=\",\", dtype=\"uint8\")\n",
    "train, dev, test = data[:4000], data[4000:4500], data[4500:]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    X = dataset[:, 1:] / 255.     # Normalize input features\n",
    "    Y_temp = dataset[:, 0]\n",
    "    \n",
    "    # Convert labels to one-hot vectors\n",
    "    n_values = np.max(Y_temp) + 1\n",
    "    Y = np.eye(n_values)[Y_temp]\n",
    "\n",
    "    return X.T, Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = normalize(train)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "X_test, Y_test = normalize(test)\n",
    "print(X_test.shape)\n",
    "print(Y_test[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "# Train\n",
    "print(\"Training...\")\n",
    "np.random.seed(1234)\n",
    "parameters = network_initialize(layer_sizes = [784,100,50,10], activations = [\"sigmoid\", \"sigmoid\", \"softmax\"])\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    L = 0\n",
    "    \n",
    "    # Split into minibatches into a *list* of sub-arrays\n",
    "    # we want to split along the number of instances, so axis = 1\n",
    "    X_minibatch = np.array_split(X_train, batch_size, axis = 1)\n",
    "    Y_minibatch = np.array_split(Y_train, batch_size, axis = 1) \n",
    "\n",
    "    # We shuffle the minibatches of X and Y in the same way\n",
    "    nmb = len(X_minibatch) # number of minibatches\n",
    "    np.random.seed(8888)\n",
    "    shuffled_index = np.random.permutation(range(nmb))\n",
    "\n",
    "    # Now we can do the training, we cannot vectorize over different minibatches\n",
    "    # They are like our \"epochs\"\n",
    "    for i in range(nmb):\n",
    "        X_current = X_minibatch[shuffled_index[i]]\n",
    "        Y_current = Y_minibatch[shuffled_index[i]]         \n",
    "            \n",
    "    #   Those two commented lines are for training Batch GD   \n",
    "    #   AL, caches = network_forward(X_train, parameters)\n",
    "    #   error, dAL = calculate_error(Y_train, AL, \"ce\")\n",
    "        AL, caches = network_forward(X_current, parameters)\n",
    "        error, dAL = calculate_error(Y_current, AL, \"ce\")\n",
    "        grads = network_backward(dAL, caches)\n",
    "        parameters = network_update(15, parameters, grads)\n",
    "        L += error\n",
    "        \n",
    "    print(\"Error of the epoch {0}: {1}\".format(epoch + 1, L/batch_size))\n",
    "\n",
    "print(\"...Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test = X_test.shape[1]\n",
    "AL_test, _ = network_forward(X_test, parameters)\n",
    "correct = (np.argmax(Y_test, axis=0) == np.argmax(AL_test, axis=0)).sum()\n",
    "   \n",
    "print('Accuracy on the test images: %d %%' % (100 * correct / m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should produce a 94% accuracy, which is not bad. Notice the learning rate. Change it and play with it. In our simple SGD-based model, it is an important hyperparameter and the performance of our model is learning rate-sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
