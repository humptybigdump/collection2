{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 6 - Maschinelles Lernen: Grundlagen und Algorithmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Package notes:**\n",
    "We will use different packages in this exersice:\n",
    "1. Scipy:\n",
    "    We will use scipy to solve linear systems of equations, given the Cholesky decomposition of the system matrix.\n",
    "2. Sklearn:\n",
    "    Sklearn is a package providing different machine learning algorithms and tools. We will not use it for machine learning algorithms here but for loading the handwritten image data set, which we will use for applying probabilistic PCA.\n",
    "    \n",
    "You can install all those packages using pip (or conda or whatever)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) k-Means (6 Points)\n",
    "Here, we are going to implement one of the most basic appraoches to clustering - the $k$-Means algorithm. \n",
    "Let us start with some basic imports and implementing functionality to visualize our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def visualize_2d_clustering(data_points: np.ndarray, assignments_one_hot: np.ndarray, centers: np.ndarray, k: int,\n",
    "                            centers_history: Optional[np.ndarray] = None, title: Optional[str] = None):\n",
    "    \"\"\"Visualizes clusters, centers and path of centers\"\"\"\n",
    "    plt.figure(figsize=(6, 6), dpi=100)\n",
    "    assignments = np.argmax(assignments_one_hot, axis=1)\n",
    "\n",
    "    for i in range(k):\n",
    "        # get next color\n",
    "        c = next(plt.gca()._get_lines.prop_cycler)['color']\n",
    "        # get cluster\n",
    "        cur_assignments = assignments == i\n",
    "        # plot clusters\n",
    "        plt.scatter(data_points[cur_assignments, 0], data_points[cur_assignments, 1], c=c, \n",
    "                    label=\"Cluster {:02d}\".format(i))        \n",
    "        #plot history of centers if it is given\n",
    "        if centers_history is not None:\n",
    "            plt.scatter(centers_history[:, i, 0], centers_history[:, i, 1], marker=\"x\", c=c)\n",
    "            plt.plot(centers_history[:, i, 0], centers_history[:, i, 1], c=c)\n",
    "\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], label=\"Centers\", color=\"black\", marker=\"X\")\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to implement the actual algorithm. As a quick reminder, $k$-Means works by iterating the following steps until convergence (i.e., until assignment does not change anymore):\n",
    "\n",
    "Start with $k$ random cluster centers.\n",
    "* 1.) **Assignment step**: Given current cluster centers, assign each data point to the cluster with closest center.\n",
    "* 2.) **Adjustment step**: Given current assignment, set cluster centers to the average of all data points assigned to it.\n",
    " \n",
    "Implement the following two functions, which are supposed to perform these two steps.\n",
    "\n",
    "***Hint***: as always, no for-loops are allowed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_step(data_points: np.ndarray, centers: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Assignment step: computes assignments to cluster with closest center.\n",
    "    :param data_points: data points (shape: [N x data_dim])\n",
    "    :param centers: current cluster centers (shape: [k, data_dim])\n",
    "    :return: new assignments (one-hot encoded) (shape: [N, k])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Implement the assignment step of the k-Means algorithm\n",
    "    ############################################################\n",
    "\n",
    "    return assignments_one_hot\n",
    "\n",
    "def adjustment_step(data_points: np.ndarray, assignments_one_hot: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adjustment step: sets cluster centers to mean of data points assigned to them.\n",
    "    :param data_points: data points (shape: [N x data_dim])\n",
    "    :param assignments_one_hot: current assignment (one-hot encoded) (shape: [N, k])\n",
    "    :return: new cluster centers (shape: [k, data_dim])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Implement the adjustment step of the k-Means algorithm\n",
    "    ############################################################\n",
    "\n",
    "    return new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can stitch together the $k$-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(data_points: np.ndarray, k: int, max_iter: int = 100, vis_interval: int = 3) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simple k-Means implementation.\n",
    "    :param data_points: data points  (shape: [N x data_dim])\n",
    "    :param k: number of clusters\n",
    "    :param max_iter: maximum number of iterations to run if convergence is not reached\n",
    "    :param vis_interval: after how many iterations to generate the next plot\n",
    "    :return: - cluster labels (shape: [N])\n",
    "             - means of clusters (shape: [k, data_dim])\n",
    "             - history of SSDs over iterations (shape: [num_iters + 1])\n",
    "             - history of means over iterations (shape: [num_iters + 1, k, data_dim])\n",
    "    \"\"\"\n",
    "    # bookkeeping\n",
    "    i = 0\n",
    "    means_history = []\n",
    "    ssd_history = []\n",
    "    \n",
    "    # start with dummy assignments (s.t. we enter loop)\n",
    "    old_assignments = np.zeros(shape=[data_points.shape[0], k])\n",
    "    new_assignments = np.ones(shape=[data_points.shape[0], k]) \n",
    "\n",
    "    # initialize with k random data points\n",
    "    initial_idx = np.random.choice(len(data_points), k, replace=False)\n",
    "    centers = data_points[initial_idx]\n",
    "    means_history.append(centers.copy())\n",
    "\n",
    "    # iterate while not converged and max number iterations not reached\n",
    "    while np.any(old_assignments != new_assignments) and i < max_iter:            \n",
    "        old_assignments = new_assignments\n",
    "        \n",
    "        # perform assignment \n",
    "        new_assignments = assignment_step(data_points, centers)\n",
    "        \n",
    "        # plot before adjustment\n",
    "        if i % vis_interval == 0:\n",
    "            visualize_2d_clustering(data_points, new_assignments, centers, k, title=\"Iteration {:02d}\".format(i))\n",
    "        \n",
    "        # compute SSD before adjustment\n",
    "        diffs = np.sum(np.square(data_points[:, None, :] - centers[None, :, :]), axis=-1)\n",
    "        ssd_history.append(np.sum(new_assignments * diffs))\n",
    "        \n",
    "        # store cluster means before adjustment \n",
    "        means_history.append(centers.copy())\n",
    "    \n",
    "        # perform adjustment\n",
    "        centers = adjustment_step(data_points, new_assignments)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # compute final SSD\n",
    "    diffs = np.sum(np.square(data_points[:, None, :] - centers[None, :, :]), axis=-1)\n",
    "    ssd_history.append(np.sum(new_assignments * diffs))\n",
    "    \n",
    "    # store final cluster means before adjustment \n",
    "    means_history.append(centers.copy())        \n",
    "        \n",
    "    print(\"Took\", i, \"iterations to converge\")\n",
    "    return new_assignments, centers, np.array(ssd_history), np.stack(means_history, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run $k$-Means on some datasets and visualize the results. We provide four datasets, each containing $500$ $2D$ samples. You can play around with the number of clustes, $k$, as well as the seed of the random number generator which influences the placement of the initial cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"samples_u.npy\")  # choose samples_1.npy, samples_2.npy, samples_3.npy, samples_u.npy\n",
    "seed = 42\n",
    "k = 3\n",
    "\n",
    "np.random.seed(42)\n",
    "cluster_labels, centers, ssd_history, centers_history = k_means(data, k)\n",
    "\n",
    "# plot final clustering with history of centers over iterations\n",
    "visualize_2d_clustering(data, cluster_labels, centers, k=k, centers_history=centers_history, title=\"Final Clustering\")\n",
    "\n",
    "# plot SSD\n",
    "plt.figure(\"SSD\")\n",
    "plt.semilogy(ssd_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"SSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Expectation Maximization for Gaussian Mixture Models (7 Points)\n",
    "\n",
    "In this exercise, we implement the expectation aximization (EM) algorithm to fit a Gaussian mixture model (GMM) to data. We start with an implementation of the log-density of a single Gaussian. We already saw an implementation of this in the first exercise and noted there that it was not the \"proper\" way of doing it. Here, we provide a better implementation. Compare the two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_density(samples: np.ndarray, mean: np.ndarray, covariance: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the log-density of samples under a Gaussian distribution in a stable and efficient manner.    \n",
    "    :param samples: samples (shape: [N x dim])\n",
    "    :param mean: mean of the distribution (shape: [dim])\n",
    "    :param covariance: covariance of the distribution (shape: [dim x dim])\n",
    "    :return: log N(x | mean, covariance) (shape: [N])\n",
    "    \"\"\"\n",
    "    dim = mean.shape[0]\n",
    "    chol_covariance = np.linalg.cholesky(covariance)\n",
    "    \n",
    "    # compute constant term\n",
    "    const_term = dim * np.log(2 * np.pi)\n",
    "    \n",
    "    # compute log-determinant\n",
    "    logdet_term = 2 * np.sum(np.log(np.diagonal(chol_covariance) + 1e-25))\n",
    "    \n",
    "    # compute exponential term\n",
    "    sol = scipy.linalg.solve_triangular(chol_covariance, (samples-mean).T, lower=True)\n",
    "    exp_term = np.sum(np.square(sol.T), axis=-1)\n",
    "    return -0.5 * (const_term + logdet_term + exp_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide some plotting functionaliy for $2D$ GMMs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_2d_gmm(samples, weights, means, covs, title):\n",
    "    \"\"\"Visualize the model and the samples.\"\"\"\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.title(title)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], label=\"Samples\", c=next(plt.gca()._get_lines.prop_cycler)['color'])\n",
    "\n",
    "    for i in range(means.shape[0]):\n",
    "        c = next(plt.gca()._get_lines.prop_cycler)['color']\n",
    "\n",
    "        (largest_eigval, smallest_eigval), eigvec = np.linalg.eig(covs[i])\n",
    "        phi = -np.arctan2(eigvec[0, 1], eigvec[0, 0])\n",
    "\n",
    "        plt.scatter(means[i, 0:1], means[i, 1:2], marker=\"x\", c=c)\n",
    "\n",
    "        a = 2.0 * np.sqrt(largest_eigval)\n",
    "        b = 2.0 * np.sqrt(smallest_eigval)\n",
    "\n",
    "        ellipse_x_r = a * np.cos(np.linspace(0, 2 * np.pi, num=200))\n",
    "        ellipse_y_r = b * np.sin(np.linspace(0, 2 * np.pi, num=200))\n",
    "\n",
    "        R = np.array([[np.cos(phi), np.sin(phi)], [-np.sin(phi), np.cos(phi)]])\n",
    "        r_ellipse = np.array([ellipse_x_r, ellipse_y_r]).T @ R\n",
    "        plt.plot(means[i, 0] + r_ellipse[:, 0], means[i, 1] + r_ellipse[:, 1], c=c,\n",
    "                 label=\"Component {:02d}, Weight: {:0.4f}\".format(i, weights[i]))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, you need to implement 4 functions:\n",
    "- The log joint densities $\\log \\left( p(x | z = k) p(z = k) \\right)$ for all components $k$ of a GMM,\n",
    "- the log-likelihood $\\log p(x) = \\log \\sum_k p(x | z = k) p(z = k) $ of a GMM,\n",
    "- the E-Step of the EM algorithm for GMMs,\n",
    "- the M-Step of the EM algorithm for GMMs.\n",
    "\n",
    "***Hints***:\n",
    "- for-loops are only allowed in ```log_joint_densities()```!\n",
    "- Consider using the ```logsumexp```-trick for a stable implementation of ```gmm_log_likelihood()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_log_joint_densities(samples: np.ndarray, weights: np.ndarray, means: np.ndarray, covariances: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the log joint densities of samples under a GMM, given model parameters.    \n",
    "    :param samples: samples (shape: [N x dim])\n",
    "    :param weights: mixture weights, i.e., p(z) (shape: [num_components])\n",
    "    :param means: component means, i.e., means of p(x|z) (shape: [num_components, dim])\n",
    "    :param covariances: component covariances, i.e., means of p(x|z) (shape: [num_components, dim, dim])\n",
    "    :return: log joint densities, i.e., log (p(x|z = k)p(z = k)) (shape: [N, k])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Compute the log joint densities log ( p(x|z = k) p(z = k) ) of a GMM.\n",
    "    ############################################################\n",
    "    \n",
    "    return log_joint_densities\n",
    "    \n",
    "def compute_gmm_log_likelihood(log_joint_densities: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Computes the log-likelihood of a GMM based on the log joint densities.\n",
    "    :param log_joint_densities: log joint densities, i.e., log p(x | z = k) p(z = k) (shape: [N, k])\n",
    "    :param weights: mixture weights, i.e., p(z) (shape: [num_components])\n",
    "    :return: log-likelihood, i.e., log p(x) = log sum_k p(x | z = k) p(z = k) (shape: [N])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Implement the log-likelihood log p(x) = log sum_k p(x | z = k) p(z = k) for GMMs.\n",
    "    ############################################################\n",
    "   \n",
    "    return marginal_densities\n",
    "\n",
    "\n",
    "def e_step(samples: np.ndarray, weights: np.ndarray, means: np.ndarray, covariances: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    E-Step of EM for fitting GMMs. Computes responsibilities, p(z|x), given samples and model parameters.\n",
    "    :param samples: samples on which to compute the responsibilities (shape: [N, dim])\n",
    "    :param weights: weights (i.e., p(z)) of model (shape: [num_components])\n",
    "    :param means: means of components p(x|z) (shape: [num_components, dim])\n",
    "    :param covariances: covariances of model components p(x|z) (shape: [num_components, dim, dim])\n",
    "    :return: responsibilities p(z|x) (shape: [N x num_components])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO Implement the E-Step of EM for GMMs.\n",
    "    ############################################################\n",
    "    \n",
    "    return np.exp(log_responsibilities)\n",
    "    \n",
    "\n",
    "def m_step(samples: np.ndarray, responsibilities: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    M-Step of EM for fitting GMMs. Computes new parameters given samples and responsibilities p(z|x).\n",
    "    :param samples: samples (shape: [N, dim])\n",
    "    :param responsibilities: p(z|x) (shape: [N x num_components])\n",
    "    :return: - new weights p(z) (shape: [num_components])\n",
    "             - new means of components p(x|z) (shape: [num_components, dim])\n",
    "             - new covariances of components p(x|z) (shape: [num_components, dim, dim]\n",
    "    \"\"\"\n",
    "    #########################################################\n",
    "    # TODO: Implement the M-Step for EM for GMMs.\n",
    "    #########################################################\n",
    "\n",
    "    return new_weights, new_means, new_covs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the EM-algorithm to fit a GMM to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_gaussian_mixture(samples: np.ndarray, num_components: int, num_iters: int = 30, vis_interval: int = 5):\n",
    "    \"\"\"\n",
    "    Fit a Gaussian Mixture Model using the Expectation Maximization Algorithm.\n",
    "    :param samples: samples to fit the GMM to (shape: [N, dim])\n",
    "    :param num_components: number of components of the GMM\n",
    "    :param num_iters: number of iterations\n",
    "    :param vis_interval: after how many iterations to generate the next plot\n",
    "    :return: - final weights p(z) (shape: [num_components])\n",
    "             - final means of components p(x|z) (shape: [num_components, dim])\n",
    "             - final covariances of components p(x|z) (shape: [num_components, dim, dim])\n",
    "             - log_likelihoods: log-likelihood of data under model after each iteration (shape: [num_iters])\n",
    "    \"\"\"\n",
    "    # Initialize model: \n",
    "    #  We initialize with means randomly picked from the data, unit covariances and uniform\n",
    "    #  component weights. In general, smarter initialization techniques might be necessary, e.g., k-Means.\n",
    "    initial_idx = np.random.choice(len(samples), num_components, replace=False)\n",
    "    means = samples[initial_idx]\n",
    "    covs = np.tile(np.eye(data.shape[-1])[None, ...], [num_components, 1, 1])\n",
    "    weights = np.ones(num_components) / num_components\n",
    "\n",
    "    # bookkeeping\n",
    "    log_likelihoods = np.zeros(num_iters + 1)\n",
    "\n",
    "    # iterate E- and M-steps\n",
    "    for i in range(num_iters):\n",
    "        # plotting, bookkeeping\n",
    "        if i % vis_interval == 0:\n",
    "            visualize_2d_gmm(data, weights, means, covs, title=\"Before Iteration {:02d}\".format(i))            \n",
    "        log_likelihoods[i] = np.mean(compute_gmm_log_likelihood(compute_log_joint_densities(samples, weights, means, covs)))\n",
    "            \n",
    "        # perform step\n",
    "        responsibilities = e_step(samples, weights, means, covs)\n",
    "        weights, means, covs = m_step(samples, responsibilities)\n",
    "    \n",
    "    # final plotting, bookkeeping\n",
    "    visualize_2d_gmm(data, weights, means, covs, title=\"Final model\".format(i))            \n",
    "    log_likelihoods[-1] = np.mean(compute_gmm_log_likelihood(compute_log_joint_densities(samples, weights, means, covs)))\n",
    "        \n",
    "    return weights, means, covs, log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load some data and run the algorithm. Again, feel free to play around with the parameters a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "seed = 42\n",
    "num_components = 3\n",
    "num_iters = 30\n",
    "vis_interval = 5\n",
    "\n",
    "# dataset\n",
    "data = np.load(\"samples_u.npy\")   # choose between samples_1.npy, samples_2.npy, samples_3.npy, samples_u.npy.\n",
    "\n",
    "# running and ploting\n",
    "np.random.seed(seed)\n",
    "final_weights, final_means, final_covariances, log_likeihoods = \\\n",
    "    fit_gaussian_mixture(data, num_components, num_iters, vis_interval)\n",
    "visualize_2d_gmm(data, final_weights, final_means, final_covariances, title=\"Final Model\")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Log-likelihoods over iterations\")\n",
    "plt.plot(log_likeihoods)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Probabilistic PCA with Expectation Maximization (7 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement probabilistic PCA (PPCA) as discussed in the lecture. We will apply it on a toy task and the handwritten digit data set. We will also generate our own images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from typing import Union, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some utilities for plotting. You don't need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.scatter(X[:, 0], X[:, 1], color='b')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(0, 7)\n",
    "    plt.ylim(0, 7)\n",
    "\n",
    "def draw_2d_gaussian(mu: np.ndarray, sigma: np.ndarray, plt_std: float = 2, *args, **kwargs) -> None:\n",
    "    (largest_eigval, smallest_eigval), eigvec = np.linalg.eig(sigma)\n",
    "    phi = -np.arctan2(eigvec[0, 1], eigvec[0, 0])\n",
    "\n",
    "    plt.scatter(mu[0:1], mu[1:2], marker=\"x\", *args, **kwargs)\n",
    "\n",
    "    a = plt_std * np.sqrt(largest_eigval)\n",
    "    b = plt_std * np.sqrt(smallest_eigval)\n",
    "\n",
    "    ellipse_x_r = a * np.cos(np.linspace(0, 2 * np.pi, num=200))\n",
    "    ellipse_y_r = b * np.sin(np.linspace(0, 2 * np.pi, num=200))\n",
    "\n",
    "    R = np.array([[np.cos(phi), np.sin(phi)], [-np.sin(phi), np.cos(phi)]])\n",
    "    r_ellipse = np.array([ellipse_x_r, ellipse_y_r]).T @ R\n",
    "    plt.plot(mu[0] + r_ellipse[:, 0], mu[1] + r_ellipse[:, 1], *args, **kwargs)\n",
    "\n",
    "def plot_ev(mu, eig_vec_1, eig_vec_2):\n",
    "    arrow_1_end = mu + eig_vec_1\n",
    "    arrow_1_x = [mu[0], arrow_1_end[0]]\n",
    "    arrow_1_y = [mu[1], arrow_1_end[1]]\n",
    "\n",
    "    arrow_2_end = mu + eig_vec_2\n",
    "    arrow_2_x = [mu[0], arrow_2_end[0]]\n",
    "    arrow_2_y = [mu[1], arrow_2_end[1]]\n",
    "\n",
    "    plt.plot(mu[0], mu[1], 'xr')\n",
    "    plt.plot((mu + eig_vec_1)[0], (mu + eig_vec_1)[1], 'xr')\n",
    "    plt.plot(arrow_1_x, arrow_1_y, 'red')\n",
    "    plt.plot(arrow_2_x, arrow_2_y, 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1) E-Step in Probabilistic PCA (3 Points)\n",
    "We will implement the E-step in this exercise. Remember the equations in the E-Step read\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{\\mu}_{\\boldsymbol{z}|\\boldsymbol{x}_i} &= (\\boldsymbol{W^TW}+\\sigma^2\\boldsymbol{I})^{-1}\\boldsymbol{W}^T(\\boldsymbol{x}_i-\\boldsymbol{\\mu}) \\\\\n",
    "     \\boldsymbol{\\Sigma_{\\boldsymbol{z}|\\boldsymbol{x}_i}} &=\\sigma^2(\\boldsymbol{W^TW}+\\sigma^2\\boldsymbol{I})^{-1},\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{x}_i$ is one sample of the data, $\\boldsymbol{W}$ is the transformation matrix, $\\sigma^2$ is the variance and $\\boldsymbol{\\mu}$ is the mean of the likelihood model.\n",
    "\n",
    "Implement the E-step of the EM-Algorithm for dimensionality reduction, according to the equations stated.\n",
    "\n",
    "***Hint***: \n",
    "- As always, avoid matrix inversions if possible!\n",
    "- Use the Cholesky decomposition to sample from a multivariate Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(W: np.ndarray, mu: np.ndarray, X: np.ndarray, sigma_quad: float):\n",
    "    \"\"\"\n",
    "    Compute posterior parameters and samples, given transformation matrix W and data X.\n",
    "    :param W: transformation matrix W (shape: [DxM], where D is data dimension, M is latent dimension)\n",
    "    :param X: data matrix containing the data (shape: [NxD])\n",
    "    :param sigma_quad: sigma^2, the variance of the likelihood (shape: float)\n",
    "    :return: - mu_z, the mean of the posterior for each sample x (shape: [NxM])\n",
    "             - z_samples, the latent variables (shape: [MxN])\n",
    "             - cov_z, the covariance of the posterior (shape: [MxM])\n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Implement the E-step for PPCA\n",
    "    ############################################################\n",
    "    return mu_z, z_samples, cov_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2) M-Step in Probabilistic PCA (4 Points)\n",
    "Now, we implement the M-step. Recall the following equations from the lecture,\n",
    "\\begin{align*}\n",
    "    \\left(\n",
    "    \\begin{array}{c}\n",
    "    \\boldsymbol{\\mu}\\\\\n",
    "    \\boldsymbol{W}\n",
    "    \\end{array}\n",
    "    \\right) = (\\boldsymbol{Z^TZ})^{-1}\\boldsymbol{Z}^T\\boldsymbol{X},\n",
    "\\end{align*}\n",
    "where \n",
    "\\begin{align*}\n",
    "\\boldsymbol{X}=\n",
    "    \\left(\n",
    "    \\begin{array}{c}\n",
    "    \\boldsymbol{x}_1^T\\\\\n",
    "    \\vdots\\\\\n",
    "    \\boldsymbol{x}_n^T\n",
    "    \\end{array}\n",
    "    \\right),\n",
    "\\boldsymbol{Z}=\n",
    "    \\left(\n",
    "    \\begin{array}{c}\n",
    "    1, \\boldsymbol{z}_1^T\\\\\n",
    "    \\vdots\\\\\n",
    "    1, \\boldsymbol{z}_n^T\n",
    "    \\end{array}\n",
    "    \\right).\n",
    "\\end{align*}\n",
    "Here, $\\boldsymbol{Z}$ is the matrix containing the bias and all the latent variable samples $\\boldsymbol{z}_i$ and $\\boldsymbol{X}$ is the matrix containing all data points $\\boldsymbol{x}$. \n",
    "We further need to implement the variance:\n",
    "\\begin{align*}\n",
    "    \\sigma^2 = \\frac{1}{ND}\\sum_{i=1}^{N}\\sum_{k=1}^{D}(y_{ik}- x_{ik})^2,\n",
    "\\end{align*}\n",
    "where $\\boldsymbol{y}_i=\\boldsymbol{W}\\boldsymbol{z}_i + \\boldsymbol{\\mu}$ and N is the number of data points and D is the dimension of the data $\\boldsymbol{x}$. \n",
    "\n",
    "***Hint***: as always, avoid matrix inversions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(z_samples: np.ndarray, X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the variance and the transformation matrix W given the latent vectors in z_samples and the data \n",
    "    in matrix X.\n",
    "    :param Z: latent variable vectors stored in z_samples (shape: [NxM])\n",
    "    :param X: matrix containg the data (shape: [NxD])\n",
    "    :return: the variance sigma_quad (shape: float), and the transformation matrix W (shape: [DxM]) \n",
    "    \"\"\"\n",
    "    ############################################################\n",
    "    # TODO: Implement the M-step for PPCA\n",
    "    ############################################################\n",
    "    return sigma_quad, mu, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the EM-loop, where the E-step and the M-step alternates. You do not need to implement anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ppca(X: np.ndarray, n_principle_comps: int, num_iters: int = 50):\n",
    "    np.random.seed(0)\n",
    "    W = np.random.normal(size=(X.shape[1], n_principle_comps))\n",
    "    mu_X = np.mean(X, axis=0)\n",
    "    mu = mu_X.copy()\n",
    "    sigma_quad = 1\n",
    "    for i in range(num_iters):\n",
    "        mu_z, z_samples, var_z = e_step(W, mu, X, sigma_quad)\n",
    "        sigma_quad, mu, W = m_step(z_samples, X)\n",
    "    return W, z_samples, var_z, sigma_quad, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Toy Task\n",
    "We will first apply PPCA on the toy task, which we also studied in the lecture. Here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x = np.random.uniform(1,5, size=(120, 1))\n",
    "y = x + 1 + np.random.normal(0, 0.7, size=x.shape)\n",
    "\n",
    "X = np.concatenate((x, y), axis = 1)\n",
    "plot_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform the algorithm on the data. You do not need to change anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plot_data(X)\n",
    "\n",
    "W, z_samples, var_z, sigma_quad, mu = do_ppca(X, n_principle_comps=1)\n",
    "\n",
    "x_tilde = np.dot(W, z_samples.T).T + mu                         # reproject to high-dim space\n",
    "\n",
    "C = np.dot(W, W.T) + sigma_quad*np.eye(W.shape[0])      # covariance of p(x) (reconstructed)\n",
    "\n",
    "v, U = np.linalg.eig(np.cov(X.T))\n",
    "mu_X = np.mean(X, axis=0)\n",
    "plot_ev(mu_X, 2*np.sqrt(v[0])*U[:, 0], 2*np.sqrt(v[1])*U[:, 1])\n",
    "\n",
    "draw_2d_gaussian(mu_X, C)\n",
    "\n",
    "plt.plot(x_tilde[:, 0], x_tilde[:, 1], 'o', color='orange', alpha=0.2)   # reprojected data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-Written Digits\n",
    "Next, we apply PPCA on the handwritten digits data set. We will consider the digit 3 only. Here is how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "targets = digits.target\n",
    "\n",
    "# get the images for digit 3 only\n",
    "digits_3_indx = np.where(targets == 3)[0]\n",
    "digit_3_data = digits.data[digits_3_indx]       # shape: (183, 64)  -> (8 x 8)\n",
    "digit_3_targets = digits.target[digits_3_indx]       # only needed to verify that we load digit 3\n",
    "\n",
    "mu_X_im = np.mean(digit_3_data, axis=0)\n",
    "\n",
    "# plot the original digit 3 images\n",
    "plt.figure()\n",
    "fig, axes = plt.subplots(10, 10, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "     ax.imshow(digit_3_data[i].reshape(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform PPCA on the data\n",
    "n_principle_comps = 10\n",
    "W_im, z_samples_im, var_z_im, sigma_quad_im, mu_im = do_ppca(digit_3_data, n_principle_comps=n_principle_comps)\n",
    "x_tilde_im = np.dot(W_im, z_samples_im.T).T + mu_im\n",
    "\n",
    "considered_im = digit_3_data[15]\n",
    "considered_im_x_tilde = x_tilde_im[15, :]\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.title('Original')\n",
    "plt.imshow(considered_im.reshape(8, 8))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Reprojection')\n",
    "plt.imshow(considered_im_x_tilde.reshape(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the reprojected data does not look same, you should definitely see the similarity to the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Image Generation\n",
    "One advantage of PPCA is that we can generate random images. The generative process, as described in the lecture, is implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some vectors z\n",
    "z = np.random.normal(size=(5, n_principle_comps))\n",
    "\n",
    "# Project back to D-dim space\n",
    "y = np.dot(W_im, z.T).T + mu_im\n",
    "\n",
    "# Sample noise\n",
    "eps = np.random.normal(scale=sigma_quad, size=y.shape)\n",
    "\n",
    "# Get image\n",
    "x = y + eps\n",
    "\n",
    "# Visualize\n",
    "plt.figure('Sampled Image')\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "     ax.imshow(x[i].reshape(8, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
