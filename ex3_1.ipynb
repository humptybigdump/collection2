{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4c51e0",
   "metadata": {},
   "source": [
    "# Exercise 3, SnaQe\n",
    "\n",
    "## Lab Instructions\n",
    "Your answers for **tasks a - f** should be written **in this notebook**.\n",
    "Your answers for **tasks g - i** should be written **in your solution\n",
    " pdf file**.\n",
    "\n",
    "You shouldn't need to write or modify any other files.\n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "This exercise was developed by Philipp Dahlinger for the KIT Cognitive Systems Lecture, Juli 2022. The pygame implementation of the snake environment was adapted from this source: [GitHub](https://github.com/jl4r1991/SnakeQlearning)\n",
    "\n",
    "Exercise 3, a-f:\n",
    "\n",
    "This jupyter notebook offers a framework of Approximate Q-learning with linear function approximation in the popular Snake game. Read the instructions carefully and complete the unfinished functions. Afterwards, you can run the training procedure and then verify your implementation.\n",
    "\n",
    "Detailed instructions:\n",
    "\n",
    "0. You may need to install pygame to run this notebook. Either you install it globally by opening a terminal and type in \"pip3 install -U pygame\" or you create a virtual environment (venv) and install pygame there. If you use a virtual environment, you may need to create a new kernel for the notebook. Detailled instructions for that can be found here: [StackOverflow](https://stackoverflow.com/questions/33496350/execute-python-script-within-jupyter-notebook-using-a-specific-virtualenv).\n",
    "\n",
    "1. In script \"snake.py\", you can find the definition of the \"Snake\" class.\n",
    "2. Keywords and rules of the game:\n",
    "    - **state**: A state is given by the positions of the snake elements on the screen (width: 30, height: 20) and by the food position. At the beginning of the episode, the game is initialized with a snake of length 1 and a random position of the food. Each time the snake head is at the same position as the food, the snake length and the score is increased by 1 and a new food spawns at a random location. <br>\n",
    "    - **action**: In every state, there are 4 possible actions, namely the direction where the snake head should move 1 space (up, right, down, left).  <br>\n",
    "    - **terminatation**: When the snake moves inside its tail or outside the screen, the game terminates. <br>\n",
    "    - **features**: Similar to the Tetris features in the Lecture Slides, we extract handcrafted features of the current state upon which the Q-values can be computed. The in total 6 features are the following:\n",
    "        - `pos_x`: If the food is right of the snake, this is 1. If the food is left of the snake, this is -1. If it has the same y-coordinate as the snake, this is 0.\n",
    "        - `pos_y`: If the food is below the snake, this is 1. If the food is above the snake, this is -1. If it has the same y-coordinate as the snake, this is 0.\n",
    "        - `surrounding`: Contains 4 entries. Each entry represents the space directly above, right, below or left to the snake head. If this space is occupied (either by the snake or a wall), this entry is 1, otherwise it is 0.\n",
    "    \n",
    "    <br>\n",
    "\n",
    "\n",
    "3. **How to represent the Q-values**:\n",
    "    Given a state s, we will always have 4 Q-values $Q(s, a) \\in \\mathbb{R}$, one for each action. We represent them by a four dimensional vector and compute them with the formula\n",
    "    \n",
    "    $$ Q(s) = \\phi(s) \\beta \\in \\mathbb{R}^4.$$\n",
    "Here, $\\phi(s) \\in \\mathbb{R}^6$ is the feature representation of the state. Our learnable weights $\\beta \\in \\mathbb{R}^{6 \\times 4}$ is a matrix, where each row is responsible for the Q-value computation of 1 of the 4 actions.\n",
    "    \n",
    "\n",
    "\n",
    "<!-- 4. You can focus on the TD learning, and the action selection part, the\n",
    "remaining code is provided. If you are interested in the entire Tetris game with TD\n",
    "learning, please see the pseudo-code below:\n",
    "    - Initialize the game (Game UI, Environment etc.)<br>\n",
    "    - While True (game runs forever)<br>\n",
    "        - Initialize an episode (score, level, initial board, initial piece,\n",
    "        timer, etc.)<br>\n",
    "        - While the episode is not terminated<br>\n",
    "            - If there is no piece falling down:<br>\n",
    "                - next piece ->> current piece<br>\n",
    "                - Get a new next piece<br>          \n",
    "                - If episode is terminal:<br>\n",
    "                    - print \"Game Over\"\n",
    "                    - break<br>\n",
    "                - Else:<br>\n",
    "                    - Get action (rotations, and column) to place the\n",
    "                    current piece (**use 1-step lookahead**)<br> \n",
    "                    - Update the Value Function (**use TD Learning**)\n",
    "            - Decompose the action and get one movement to take (key = up, down, left,\n",
    "    right)<br>\n",
    "            - Perform the movement to current piece and compute its new\n",
    "            coordinates <br>\n",
    "            - Update the board (check lines completed, merge piece if possible),\n",
    "    score, level etc.<br>\n",
    "            - Plot board, piece, text, score, etc.<br><br>\n",
    " -->\n",
    "4. Other function specific instructions can be found above the unfinished\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a37ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "# Include some python packages and snake env\n",
    "\n",
    "import pygame\n",
    "import random\n",
    "from ex3.snake import Snake\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a304bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "# initialization\n",
    "\n",
    "# fixed seed for deterministic behavior:\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "\n",
    "# dimensions\n",
    "feature_size = 6\n",
    "action_size = 4\n",
    "\n",
    "\n",
    "# learnable weight initialization\n",
    "# these are the global weights we update during the learning\n",
    "betas = np.random.rand(feature_size, action_size)\n",
    "\n",
    "# number of transitions the agent performs Q learning\n",
    "len_epoch = 20000 \n",
    "# hyperparameters\n",
    "temp = 100.0  \n",
    "lr = 0.2 \n",
    "gamma = 0.5 \n",
    "\n",
    "# action translator. The snake environment expects string keywords, but for the RL agent it is simpler to output\n",
    "# indices between 0 and 3.\n",
    "actions = {\n",
    "    0: \"left\",\n",
    "    1: \"up\",\n",
    "    2: \"right\",\n",
    "    3: \"down\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd9f950",
   "metadata": {},
   "source": [
    "### a) Value Function Approximation <br>\n",
    "Please finish the function **\"approximate_q_values\"** below. This function is to\n",
    " perform **\"linear function approximation\"** to compute the 4 Q-values $Q(s, a)$ for a state.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6c286",
   "metadata": {},
   "source": [
    "Hint:\n",
    "- You should use the parameters **\"betas\"**, as well as some useful numpy\n",
    "functions like **\"np.matmul\"** or **\"np.dot\"**.<br> Note that the linear\n",
    "function approximation case can be implemented by a scalar product.\n",
    "- For the correct formula see the top instructions, bullet point 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def approximate_q_values(phi_s):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector with shape (feature_size,). Extracted features from a state s.\n",
    "    \n",
    "    return: Vector containing the 4 Q-values Q(s,a) for every possible action a.\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "    approximated_q_value = ...\n",
    "    ########    Your code ends here    ########\n",
    "    return approximated_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833dea18",
   "metadata": {},
   "source": [
    "### b) Exploration vs. Exploitation <br>\n",
    "Please finish the function **\"select_action\"**  and **\"softmax\"** below. The function **\"select_action\"** samples an action (get its index) following the **\"Soft-Max Policy\"**\n",
    "exploration strategy (See Slide 33 in the Reinforcement Learning chapter). Recall that this exploration strategy has a **\"temperature\"** value (in our Notebook called `temp`) that scales the Q-values. A higher temperature results in more similar inputs for the Soft-Max function which will yield more uniform selection probabilities. A temperature closer to 0 increases the sharpness of the probability distribution resulting in almost always selecting the action with the highest Q-value. <br>\n",
    "\n",
    "- Start by implementing the **\"softmax\"** function:\n",
    "$$\n",
    "f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\n",
    "$$\n",
    "$$\n",
    "f(x)_i = \\frac {\\exp(x_i)}{\\sum_j \\exp(x_j)}.\n",
    "$$\n",
    "Note that the input and the output are both vectors with the same shape.\n",
    "\n",
    "- Then, implement the **\"select_action\"** function. It has two arguments: The current feature representation `phi_s` as well as a boolean `sample`. If `sample` is true, return the sampled action based on the Soft-Max policy, otherwise return the deterministic action with the highest Q-value (and hence ignoring the temperature). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056bb2f6",
   "metadata": {},
   "source": [
    "Hints:\n",
    "- The softmax function can be implemented differently to the given formula above for a better numerical stability. In this exercise however you may use the simpler implementation following the formula.\n",
    "- You can use **\"np.argmax\"** function to get the index of the max value in a np.array.\n",
    "- Use the softmax function to obtain the selection probabilites. Remeber to scale the Q-values with the temperature, which is a global variable `temp`.\n",
    "- Once you have the selection probabilities, you can use the function np.random.multinomial(...) for sampling. Check its documentation here: [np.random.multinomial](https://numpy.org/doc/stable/reference/random/generated/numpy.random.multinomial.html).\n",
    "- Make sure that the return value is an integer and not a np.array with length 4!\n",
    "- You can check your implementation of your softmax function with the test in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20890e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTIONS IN THIS BLOCK\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: 1-dimensional numpy array.\n",
    "    returns: softmax(x), 1-dimensional numpy.\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "    result = ...\n",
    "    ########    Your code ends here    ########\n",
    "    return result\n",
    "\n",
    "def select_action(phi_s, sample=True):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector with shape (feature_size,). Extracted features from a state s.\n",
    "    sample: Boolean flag. If true, sample an action based on the Soft-Max policy, otherwise return the action with\n",
    "            the highest Q-value.\n",
    "    \n",
    "    return: Integer action index a_idx (in [0,1,2,3]) of the selected action.\n",
    "    \"\"\"\n",
    "    # Q-values for the current state\n",
    "    qs = approximate_q_values(phi_s)\n",
    "    if sample:\n",
    "        ########   Your code begins here   ########\n",
    "        a_idx = ...\n",
    "        ########    Your code ends here    ########\n",
    "    else:\n",
    "        ########   Your code begins here   ########\n",
    "        a_idx = ...\n",
    "        ########    Your code ends here    ########\n",
    "        \n",
    "    return a_idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test for softmax:\n",
    "# example input\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "output = softmax(x)\n",
    "print(output)\n",
    "# result: [0.0320586  0.08714432 0.23688282 0.64391426]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e8dc6",
   "metadata": {},
   "source": [
    "### c) Compute Temporal Difference Error <br>\n",
    "Please finish the function **\"compute_delta\"** below. This function will\n",
    "compute the Temporal Difference Error $\\delta$ given in the algorithm of slide 50 of the RL chapter. However, we have to also incorporate terminal states which is not shown on the slide. In case the state $s$ is terminal, we compute $\\delta$ by the formula\n",
    "$$\n",
    "\\delta = r(s,a) - Q(s,a).\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90033f72",
   "metadata": {},
   "source": [
    "Hints:\n",
    "- Use your implemented approximate_q_values() function.\n",
    "- You can use **\"np.max\"** function to get the maximum value of a np.array.\n",
    "- Use the globally initialized variable gamma for the discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130edf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def compute_delta(phi_s, a_idx, r, phi_new_s, is_terminal):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector of state s with shape (feature_size,).\n",
    "    a_idx: action index a_idx (in [0,1,2,3]) of the action selected in state s.\n",
    "    r: reward r(s,a) of the action with index a_idx in state s (float).\n",
    "    phi_new_s: Feature vector of state new_s with shape (feature_size,). \n",
    "               new_s is the state after selection action a_idx in state s.\n",
    "    is_terminal: boolean which indicates if s is a terminal state.\n",
    "    \n",
    "    return: td_error delta of type float\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "    ...\n",
    "    ########    Your code ends here    ########\n",
    "    return delta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c973c78",
   "metadata": {},
   "source": [
    "### d) Compute the derivative of the Q-value with respect to beta <br>\n",
    "Please finish the function **\"compute_d_qsa_d_beta\"** below. This function will\n",
    "compute the derivative of the Q-value with respect to the global parameter **\"betas\"**. Therefore, you first need to understand what the derivative $\\frac {\\text{d}Q(s,a)}{\\text{d} \\beta}$ exactly is before implementing it.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd35fa",
   "metadata": {},
   "source": [
    "Hints:\n",
    "- In order to derive the exact formulation for the derivative, it helps to write down the formula for the Q-value \n",
    "$$\n",
    "Q(s,a) = \\phi(s)\\beta \\,[\\text{a_idx}]\n",
    "$$\n",
    "Make sure that you understand that the single Q-Value $Q(s,a) \\in \\mathbb{R}$ is the entry with index `a_idx` of the vector $\\phi(s) \\beta \\in \\mathbb{R}^4$. Therefore, only the column with index `a_idx` of the matrix $\\beta \\in \\mathbb{R}^{6 \\times 4}$ has an influence on the Q-value $Q(s,a)$. \n",
    "- The shape of the derivative has to be the same as the shape of $\\beta$ ($6 \\times 4$). \n",
    "- You can use `np.zeros(shape)` to initialize a np array with zeros with a given `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18807d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def compute_d_qsa_d_beta(phi_s, a_idx):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector of state s with shape (feature_size,).\n",
    "    a_idx: action index a_idx (in [0,1,2,3]) of the action selected in state s.\n",
    "    \n",
    "    return: Derivative of the q_value Q(s, a) wrt. betas. It has shape (feature_size, action_size)\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "    d_qsa_d_beta = ...\n",
    "    ########    Your code ends here    ########\n",
    "    return d_qsa_d_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9284cc",
   "metadata": {},
   "source": [
    "### e) Gradient Descent <br>\n",
    "Please finish the function **\"update_betas\"** below. This function will\n",
    "update the parameters **\"betas\"** used in the Q-value function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178293f2",
   "metadata": {},
   "source": [
    "Hints:\n",
    "- You should use your implementation of the td_error $\\delta$ and of the derivative of the Q-value function $\\frac {\\text{d}Q(s,a)}{\\text{d} \\beta}$.\n",
    "- Because **\"betas\"** is a global variable (numpy vector), if you want to\n",
    "modify its values in a local function, you should add a declaration **\"global\n",
    "betas\"** before your modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def update_betas(phi_s, a_idx, r, phi_new_s, is_terminal):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector of state s with shape (feature_size,).\n",
    "    a_idx: action index a_idx (in [0,1,2,3]) of the action selected in state s.\n",
    "    r: reward r(s,a) of the action with index a_idx in state s (float).\n",
    "    phi_new_s: Feature vector of state new_s with shape (feature_size,). \n",
    "               new_s is the state after selection action a_idx in state s.\n",
    "    is_terminal: boolean which indicates if s is a terminal state.\n",
    "    \n",
    "    return: None, but you have to change the value of the global betas\n",
    "    \"\"\"\n",
    "    # we want to update the global variable beta, hence the global \"import\"\n",
    "    global betas\n",
    "    ########   Your code begins here   ########\n",
    "    betas = ...\n",
    "    ########   Your code ends here   ########\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514f636",
   "metadata": {},
   "source": [
    "Helper Functions for evaluating and playing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df336ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "# helper function to test the current policy\n",
    "def test_policy(num_games=5):\n",
    "    av_score = 0\n",
    "    g = 0\n",
    "    snake = Snake(FRAMESPEED=50000)\n",
    "    while g < num_games:\n",
    "        s = snake.get_feature_representation()\n",
    "        a_idx = select_action(s, sample=False)\n",
    "        a = actions[a_idx]\n",
    "        is_terminal = snake.step(a)\n",
    "        if is_terminal:\n",
    "            av_score += snake.last_score\n",
    "            g += 1\n",
    "    pygame.quit()\n",
    "\n",
    "    av_score /= num_games\n",
    "    print(f\"Average score: {av_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a3380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "# helper function to simulate one game with normal speed\n",
    "def play_single_game(framespeed=20):\n",
    "    snake = Snake(FRAMESPEED=framespeed)\n",
    "    while True:\n",
    "        s = snake.get_feature_representation()\n",
    "        a_idx = select_action(s, sample=False)\n",
    "        is_terminal = snake.step(actions[a_idx], init_new_game_after_terminal=False)\n",
    "        if is_terminal:\n",
    "            print(f\"Total Score: {snake.last_score}\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073f26f",
   "metadata": {},
   "source": [
    "### f) Q-Learning <br>\n",
    "Now, every single module of the Q-learning has been prepared. It is time to\n",
    "finish the training loop! Insert the missing function calls to stitch together the complete Q-Learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ec4fe",
   "metadata": {},
   "source": [
    "Hints:\n",
    "- Step 3-5 is already implemented, so do not modify these parts.\n",
    "- Every step can be implemented in one line by calling a specific function with the correct function arguments.\n",
    "- The snake class saves the current state internally and updates this state with the snake.step() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE DESCRIBED STEPS IN THIS BLOCK\n",
    "\n",
    "snake = Snake(FRAMESPEED=50000)\n",
    "\n",
    "for i in range(len_epoch):\n",
    "    \n",
    "    ########   Your code begins here   ########\n",
    "    # 1. get the feature representation of the current state by calling snake.get_feature_representation()\n",
    "    phi_s = snake.get_feature_representation()\n",
    "    # 2. Select the action using the soft-max policy exploration strategy\n",
    "    a_idx = select_action(phi_s, sample=True)\n",
    "    # 3. Get the action string which is needed for the snake environment. (already implemented)\n",
    "    a = actions[a_idx]\n",
    "    # 4. Ask for the current reward (already impelmented)\n",
    "    r = snake.get_reward(a)\n",
    "    # 5. Perform one step. This returns the boolean is_terminal if the snake died during this step.\n",
    "    # (already implemented)\n",
    "    is_terminal = snake.step(a)\n",
    "    # 6. Get the feature representation of the updated state by calling snake.get_feature_representation()\n",
    "    phi_new_s = snake.get_feature_representation()\n",
    "    # 7. Update betas by calling the update_betas(...) method\n",
    "    update_betas(phi_s, a_idx, r, phi_new_s, is_terminal)\n",
    "    \n",
    "    ########   Your code ends here   ########\n",
    "    \n",
    "    # To see how well the current policy works we test it every 5000 updates\n",
    "    if i % 5000 == 0:   \n",
    "        pygame.quit()\n",
    "        test_policy()\n",
    "        snake = Snake(FRAMESPEED=50000)\n",
    "    \n",
    "    # update hyperparameters\n",
    "    temp *= 0.999\n",
    "    temp = max(temp, 0.1)\n",
    "    #print(epsilon)\n",
    "    lr *= 0.9999\n",
    "\n",
    "test_policy()\n",
    "pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE DESCRIBED STEPS IN THIS BLOCK\n",
    "\n",
    "snake = Snake(FRAMESPEED=50000)\n",
    "\n",
    "for i in range(len_epoch):\n",
    "    \n",
    "    ########   Your code begins here   ########\n",
    "    # 1. get the feature representation of the current state by calling snake.get_feature_representation()\n",
    "    ...\n",
    "    # 2. Select the action using the soft-max policy exploration strategy\n",
    "    a_idx = ...\n",
    "    # 3. Get the action string which is needed for the snake environment. (already implemented)\n",
    "    a = actions[a_idx]\n",
    "    # 4. Ask for the current reward (already impelmented)\n",
    "    r = snake.get_reward(a)\n",
    "    # 5. Perform one step. This returns the boolean is_terminal if the snake died during this step.\n",
    "    # (already implemented)\n",
    "    is_terminal = snake.step(a)\n",
    "    # 6. Get the feature representation of the updated state by calling snake.get_feature_representation()\n",
    "    ...\n",
    "    # 7. Update betas by calling the update_betas(...) method\n",
    "    ...\n",
    "    \n",
    "    ########   Your code ends here   ########\n",
    "    \n",
    "    # To see how well the current policy works we test it every 5000 updates\n",
    "    if i % 5000 == 0:   \n",
    "        pygame.quit()\n",
    "        test_policy()\n",
    "        snake = Snake(FRAMESPEED=50000)\n",
    "    \n",
    "    # update hyperparameters\n",
    "    temp *= 0.999\n",
    "    temp = max(temp, 0.1)\n",
    "    #print(epsilon)\n",
    "    lr *= 0.9999\n",
    "\n",
    "test_policy()\n",
    "pygame.quit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your learned strategy. You can adjust the speed of the animation with the framespeed argument.\n",
    "play_single_game(framespeed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa27322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to close the pygame window:\n",
    "pygame.quit()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f81583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (snake)",
   "language": "python",
   "name": "snake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
