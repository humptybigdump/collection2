{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e90be3f-1e56-4de8-91e3-d6b62a5e75a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise worksheet no 4\n",
    "\n",
    "# Hackathon I\n",
    "\n",
    "### Learning an ozone parameterization for an Earth system model\n",
    "\n",
    "*Machine learning in climate and environmental sciences, winter semester 2023, Jun.-Prof. Peer Nowack, peer.nowack@kit.edu*\n",
    "\n",
    "*Chair for AI in Climate and Environmental Sciences, https://ki-klima.iti.kit.edu*\n",
    "\n",
    "**Learning objectives:** The goal of this hackathon is to provide you with a realistic climate science example for training and assessing a range of machine learning (ML) algorithms. You will tackle an actual research question: *what is the best ML set-up to parameterize ozone variability in a global Earth System Model (ESM)?* For this, you receive training data from simulations conducted with the United Kingdom Earth System Model (UKESM), which participated in the [Coupled Model Intercomparison Project phase 6](https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6).\n",
    "#### Background\n",
    "\n",
    "From the UKESM data, you will learn to predict ozone - [an important atmospheric trace gas](https://www.ametsoc.org/index.cfm/ams/about-ams/ams-statements/statements-of-the-ams-in-force/atmospheric-ozone1/) - as a function of the state of the atmosphere one day earlier. These functions could later be used to model ozone within UKESM at much reduced computational cost, because simulating ozone is one of the [most expensive ESM components](https://gmd.copernicus.org/articles/11/3089/2018/).\n",
    "\n",
    "Specifically, you will compare at least **three different ML regression algorithms of your choice** to predict daily mean ozone (O$_3$) concentrations in six distinct regions of the atmosphere (i.e. in six grid boxes of UKESM, which is a numerical model). In other words, you will learn functions $f_{ML}$\n",
    "\n",
    "$$\n",
    "\\text{O}_3(t)_{i,j,k} = f_{ML}(\\mathbf{X};t-1)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}$ contains a large number of meteorological variables from the previous day ($t-1$) that might represent processes driving ozone variability. The indices $i,j,k$ indicate the position (latitude, longitude, height) of the six grid boxes for which we aim to predict ozone. The predictor variables included in $\\mathbf{X}$ are: \n",
    "\n",
    "- atmospheric temperature\n",
    "- [chlorine content](https://www.revistascca.unam.mx/atm/index.php/atm/article/view/38656) (important for chemical ozone loss in the stratosphere)\n",
    "- [zonal wind](https://glossary.ametsoc.org/wiki/Zonal_wind) (characterizes wind speed and direction along the east-west direction; key for dynamics, transport of ozone)\n",
    "- [meridional wind](https://glossary.ametsoc.org/wiki/Meridional_wind) (characterizes wind speed and direction along the north-south direction)\n",
    "- [Eliassen-Palm flux](https://www.gfdl.noaa.gov/bibliography/related_files/dga8301.pdf) (characterizes wave propagation and angular momentum transfer onto the stratosphere, which drives the stratospheric [Brewer-Dobson circulation](https://www.fz-juelich.de/en/iek/iek-7/research/atmospheric-coupling-processes/brewer-dobson-circulation))\n",
    "- specific humidity (the atmospheric water vapour distribution carries imprints of current and past atmospheric circulation variability and water vapour is involved in atmospheric chemistry driving changes in ozone)\n",
    "- shortwave heating rates (characterizes heating of the grid box due to absorption of sunlight by the atmosphere, coupled to ozone photochemistry)\n",
    "- longwave heating rates (characterizes heating of the grid box due to the absorption of terrestrial outgoing longwave radiation by the atmosphere)\n",
    "- pressure on theta/model levels (characterizes the atmospheric dynamical state)\n",
    "\n",
    "These predictors are provided to you across the entire **atmospheric column** around the ozone grid point in question (an example of such a model column is illustrated below, image source: UK Met Office). Simply speaking, we use column information because we can: ESMs are numerical models which require a high level of parallelization of computations. Due to how parallelization is implemented in UKESM, we will always have the vertical column information available at runtime to predict ozone at a given grid location, but horizontally distant information might not be accessible before other parallel computations have been completed. For more details, do get in touch :)\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/metofficegovuk.jpeg\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "#### Hackathon set-up and rules\n",
    "\n",
    "Everyone of you has access to the same training data for the predictand time series (ozone concentrations at six grid locations of the atmosphere) and predictors (the meteorological variables across matching atmospheric columns). **This creates six separate regression problems, one for each ozone target grid cell.**\n",
    "\n",
    "**Your task** is to use this data to build the **best possible ML model** in terms of generalizable predictive skill. The only *restrictions* are that we ask you to \n",
    "\n",
    "- submit one final best model type. For example, you are allowed to submit six different feedforward neural networks or six different random forest regressors for your \"best model\", but not a mix of these two options.\n",
    "- use the same set of predictors for all grid cells, i.e. don't use temperature only for one grid cell and temperature and specific humidity for another.\n",
    "- not take ozone from the previous timesteps as input feature as this can lead to runaway instabilities in actual ESM simulations! Submissions which include ozone as predictor itself will not be accepted.\n",
    "\n",
    "Apart from that, you have complete freedom to use the training data provided as you wish, i.e. you can choose \n",
    "- the regression model\n",
    "- the cross-validation strategy\n",
    "- the variables you include in your fit\n",
    "- the variable scaling approach\n",
    "- if you want, potential dimension reduction pre-processing methods such [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for the predictors, which might be helpful to speed-up your calculations for more complex algorithms.\n",
    "- to experiment with e.g. logarithmic or exponential transformations of the predictors\n",
    "\n",
    "You may use regression algorithms already discussed in the module, such as [multiple linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), [lasso regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), [elastic nets](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html), [random forest regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), [XGBoost](https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn) and [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html), or [feedforward neural networks](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html). However, you are also invited to try out algorithms you might be personally particularly interested in, e.g. as part of your Master's thesis project. For more flexible neural network implementations, we have additionally included *three simple ways to implement a feedforward neural network in Python* at the end of this notebook. Feel free to experiment with the one(s) you like most, and to adapt to more complex network architectures.\n",
    "\n",
    "You are also allowed to use [multi-output/multivariate regression](https://machinelearningmastery.com/multi-output-regression-models-with-python/) approaches, i.e. to predict all six grid cells with one model object at once. For example, the `sklearn` implementations of `Ridge()` and `RandomForestRegressor()` allow for that, you simply need to use output matrices Y of dimensions (nr_samples, nr_grid_cells). You can read more about this in their function documentations.\n",
    "\n",
    "Below we run through the entire process once using a `Ridge()` regession model with only temperatures as predictors. We also include an example for the preparation of your submission files: once you have found your personal \"best model\", save the model object(s) using packages such as `joblib` to the subfolder `model_objects`. In the saved object, or in the notebook, make sure to include information on how we can easily re-create your predictor matrix. Then upload the entire folder, including your notebook with three solutions, as usual as zip file to Ilias. In your Jupyter notebook, make sure to include an example of how we can load the best model and use it to predict (ideally with helpful comments). If you needed to install additional Python `packages` then please include an extra commented cell with `!conda` statements to install them.\n",
    "\n",
    "#### How we create a hackathon ranking\n",
    "The only way to objectively rank your solutions is to hold back separate test data, which we will apply your submitted best solution to. We will measure prediction skill in terms of the `mean_squared_error`. The errors will be compared on standard-scaled ozone time series so that, despite very different ozone concentrations across the different locations, all six grid points will be considered approximately equally in the final scoring function.\n",
    "\n",
    "The best performing solutions will be discussed in the exercise class and will be - as is characteristic for this module - rewarded with chocolate (or, if desired, an appropriate alternative). If successful, you might even inspire the approach for a current research project on ozone parameterizations.\n",
    "\n",
    "## Good luck!\n",
    "\n",
    "Below an example run through the data, plus the different ways to fit a feedforward neural network. In case of questions, feel free to post on the **Discussion Board** on Ilias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf871c5-fade-4e6f-a6c9-611d878d90cf",
   "metadata": {},
   "source": [
    "#### Load Python and the \"ML-climate\" kernel\n",
    "\n",
    "As always: if you are working on your own computer, now select the \"ML-climate\" kernel. This option should exist for you if you followed the Anaconda 3 and subsequent installation instructions provided on Ilias. Alternatively, you can run the notebook on Google Colab. As usual, you will need to use e.g. the Colab data loader whenever files need to be read from the exercise folder (see Worksheets 1 and 2), and certain packages might still need to be installed using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad32a8-8b46-4713-805d-f1838605edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## On Google Colab uncomment the following lines\n",
    "# !pip install netcdf4\n",
    "# !pip install keras==2.12.0\n",
    "# !pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7c941-18d7-4f8f-95f2-9aa7b1521414",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please comment out after successful installation\n",
    "!conda install tensorflow -y\n",
    "!conda install -c conda-forge py-xgboost -y\n",
    "!conda install pytorch -y\n",
    "!conda install -c conda-forge skorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4b8a1-1346-4540-bc9e-d47e0dbbbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a few Python packages that might be useful for you\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from matplotlib import rcParams\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "import netCDF4\n",
    "### neural network packages\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.regularizers import l2\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import skorch\n",
    "from skorch import NeuralNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec507711-5325-4c72-b7af-44f417e512e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need to adjust the paths depending on your operating system\n",
    "# on Google Colab you will need the manual loader function (as in previous Worksheets)\n",
    "file_X_cell1 = netCDF4.Dataset('./data/X_cell1_students.nc','r')\n",
    "file_X_cell2 = netCDF4.Dataset('./data/X_cell2_students.nc','r')\n",
    "file_X_cell3 = netCDF4.Dataset('./data/X_cell3_students.nc','r')\n",
    "file_X_cell4 = netCDF4.Dataset('./data/X_cell4_students.nc','r')\n",
    "file_X_cell5 = netCDF4.Dataset('./data/X_cell5_students.nc','r')\n",
    "file_X_cell6 = netCDF4.Dataset('./data/X_cell6_students.nc','r')\n",
    "#\n",
    "file_Y_cell1 = netCDF4.Dataset('./data/Y_cell1_students.nc','r')\n",
    "file_Y_cell2 = netCDF4.Dataset('./data/Y_cell2_students.nc','r')\n",
    "file_Y_cell3 = netCDF4.Dataset('./data/Y_cell3_students.nc','r')\n",
    "file_Y_cell4 = netCDF4.Dataset('./data/Y_cell4_students.nc','r')\n",
    "file_Y_cell5 = netCDF4.Dataset('./data/Y_cell5_students.nc','r')\n",
    "file_Y_cell6 = netCDF4.Dataset('./data/Y_cell6_students.nc','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce835f64-78f2-4404-9f14-2c1fa865784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read out each cells' 3D spatial coordinates\n",
    "cell1_lon = file_Y_cell1['lon'][:]\n",
    "cell2_lon = file_Y_cell2['lon'][:]\n",
    "cell3_lon = file_Y_cell3['lon'][:]\n",
    "cell4_lon = file_Y_cell4['lon'][:]\n",
    "cell5_lon = file_Y_cell5['lon'][:]\n",
    "cell6_lon = file_Y_cell6['lon'][:]\n",
    "#\n",
    "cell1_lat = file_Y_cell1['lat'][:]\n",
    "cell2_lat = file_Y_cell2['lat'][:]\n",
    "cell3_lat = file_Y_cell3['lat'][:]\n",
    "cell4_lat = file_Y_cell4['lat'][:]\n",
    "cell5_lat = file_Y_cell5['lat'][:]\n",
    "cell6_lat = file_Y_cell6['lat'][:]\n",
    "#\n",
    "cell1_z = file_Y_cell1['hybrid_ht'][:]/1000.0\n",
    "cell2_z = file_Y_cell2['hybrid_ht'][:]/1000.0\n",
    "cell3_z = file_Y_cell3['hybrid_ht'][:]/1000.0\n",
    "cell4_z = file_Y_cell4['hybrid_ht'][:]/1000.0\n",
    "cell5_z = file_Y_cell5['hybrid_ht'][:]/1000.0\n",
    "cell6_z = file_Y_cell6['hybrid_ht'][:]/1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0439b80-90c8-41cd-9a2a-12a70e62116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select which variables to include as input features\n",
    "### field2200 = Lumped Cl - grid cell chlorine content in [kg/kg air]\n",
    "### field1079 = Divergence of Eliassen-Palm flux - a quantity measuring the stirring/momentum transfer on the atmospheric flow\n",
    "### q = specific humidity [kg/kg air] \n",
    "### lwhr = Long-wave heating rates [K/s]\n",
    "### p_2 = Pressure [Pa]\n",
    "### swhr = Short-wave heating rates [K/s]\n",
    "### temp = Temperature [K]\n",
    "### u = Westerly wind component [m/s]\n",
    "### v = Southerly wind component [m/s]\n",
    "# columns = ['field2200', 'field1079', 'q', 'lwhr', 'p_2', 'swhr', 'temp', 'u', 'v']\n",
    "### Here, we first only include temperature as predictor\n",
    "columns = ['temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5cb92-a700-41ca-bf94-5593615e0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for reasons in which the regression in an ESM is parallelized on a supercomputer we are only interested in regressions\n",
    "using variable information from the same atmospheric column = up to 85 vertical UKESM model levels. \n",
    "Therefore, we have array dimensions of (number time steps, 85) initially for temperature.\n",
    "Let's read in all the input features we are interested in - therefore the second dimension increases as we stack up more features'''\n",
    "X_cell1 = file_X_cell1.variables[columns[0]][:,:,0,0]\n",
    "X_cell2 = file_X_cell2.variables[columns[0]][:,:,0,0]\n",
    "X_cell3 = file_X_cell3.variables[columns[0]][:,:,0,0]\n",
    "X_cell4 = file_X_cell4.variables[columns[0]][:,:,0,0]\n",
    "X_cell5 = file_X_cell5.variables[columns[0]][:,:,0,0]\n",
    "X_cell6 = file_X_cell6.variables[columns[0]][:,:,0,0]\n",
    "print(X_cell1.shape)\n",
    "for i in columns[1:]:\n",
    "    X_add_cell1 = file_X_cell1.variables[i][:,:,0,0]\n",
    "    X_cell1 = np.hstack((X_cell1,X_add_cell1))\n",
    "    print(X_cell1.shape)\n",
    "    #\n",
    "    X_add_cell2 = file_X_cell2.variables[i][:,:,0,0]\n",
    "    X_cell2 = np.hstack((X_cell2,X_add_cell2))\n",
    "    #\n",
    "    X_add_cell3 = file_X_cell3.variables[i][:,:,0,0]\n",
    "    X_cell3 = np.hstack((X_cell3,X_add_cell3))\n",
    "    #\n",
    "    X_add_cell4 = file_X_cell4.variables[i][:,:,0,0]\n",
    "    X_cell4 = np.hstack((X_cell4,X_add_cell4))\n",
    "    #\n",
    "    X_add_cell5 = file_X_cell5.variables[i][:,:,0,0]\n",
    "    X_cell5 = np.hstack((X_cell5,X_add_cell5))\n",
    "    #\n",
    "    X_add_cell6= file_X_cell6.variables[i][:,:,0,0]\n",
    "    X_cell6 = np.hstack((X_cell6,X_add_cell6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ccbd9-67f8-479a-8671-7df263bded69",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in ozone time series for each grid cell = field2101 [kg of ozone/kg of air]\n",
    "Y_cell1 = file_Y_cell1.variables['field2101'][:,0,0,0]\n",
    "print(Y_cell1.shape)\n",
    "Y_cell2 = file_Y_cell2.variables['field2101'][:,0,0,0]\n",
    "Y_cell3 = file_Y_cell3.variables['field2101'][:,0,0,0]\n",
    "Y_cell4 = file_Y_cell4.variables['field2101'][:,0,0,0]\n",
    "Y_cell5 = file_Y_cell5.variables['field2101'][:,0,0,0]\n",
    "Y_cell6 = file_Y_cell6.variables['field2101'][:,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd685ccc-dcd6-4df4-b012-3012c0d99458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### offset ozone and input features by one day to actually train the predictions on the desired prediction timescale\n",
    "lag=1\n",
    "Y_cell1 = Y_cell1[lag:]\n",
    "Y_cell2 = Y_cell2[lag:]\n",
    "Y_cell3 = Y_cell3[lag:]\n",
    "Y_cell4 = Y_cell4[lag:]\n",
    "Y_cell5 = Y_cell5[lag:]\n",
    "Y_cell6 = Y_cell6[lag:]\n",
    "#\n",
    "X_cell1 = X_cell1[:-lag,:]\n",
    "X_cell2 = X_cell2[:-lag,:]\n",
    "X_cell3 = X_cell3[:-lag,:]\n",
    "X_cell4 = X_cell4[:-lag,:]\n",
    "X_cell5 = X_cell5[:-lag,:]\n",
    "X_cell6 = X_cell6[:-lag,:]\n",
    "# number timesteps in total after lagging = nt\n",
    "nt=len(Y_cell1[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea030b8-da87-4725-bdd0-494f44f48b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot ozone time series\n",
    "### clearly you can see that ozone mass mixing ratios take on very different values in different grid cells\n",
    "# the time behaviour is also very different...\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.arange(0,nt),Y_cell1,label='cell 1')\n",
    "plt.plot(np.arange(0,nt),Y_cell2,label='cell 2')\n",
    "plt.plot(np.arange(0,nt),Y_cell3,label='cell 3')\n",
    "plt.plot(np.arange(0,nt),Y_cell4,label='cell 4')\n",
    "plt.plot(np.arange(0,nt),Y_cell5,label='cell 5')\n",
    "plt.plot(np.arange(0,nt),Y_cell6,label='cell 6')\n",
    "plt.xlabel('Number timesteps (days)',size=16)\n",
    "plt.ylabel('Ozone mmr (kg/kg)',size=16)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ac425-3f00-447d-a04a-ffa42d6beec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### example for a possible split of the data into training and test datasets\n",
    "### once you have found your best parameter settings you might just want to retrain your model on the entire dataset\n",
    "### i.e. set test_size=0.0\n",
    "X_cell1_train, X_cell1_test = train_test_split(X_cell1,test_size=0.2,shuffle=False)\n",
    "X_cell2_train, X_cell2_test = train_test_split(X_cell2,test_size=0.2,shuffle=False)\n",
    "X_cell3_train, X_cell3_test = train_test_split(X_cell3,test_size=0.2,shuffle=False)\n",
    "X_cell4_train, X_cell4_test = train_test_split(X_cell4,test_size=0.2,shuffle=False)\n",
    "X_cell5_train, X_cell5_test = train_test_split(X_cell5,test_size=0.2,shuffle=False)\n",
    "X_cell6_train, X_cell6_test = train_test_split(X_cell6,test_size=0.2,shuffle=False)\n",
    "print(X_cell1_train.shape,X_cell1_test.shape)\n",
    "Y_cell1_train, Y_cell1_test = train_test_split(Y_cell1,test_size=0.2,shuffle=False)\n",
    "Y_cell2_train, Y_cell2_test = train_test_split(Y_cell2,test_size=0.2,shuffle=False)\n",
    "Y_cell3_train, Y_cell3_test = train_test_split(Y_cell3,test_size=0.2,shuffle=False)\n",
    "Y_cell4_train, Y_cell4_test = train_test_split(Y_cell4,test_size=0.2,shuffle=False)\n",
    "Y_cell5_train, Y_cell5_test = train_test_split(Y_cell5,test_size=0.2,shuffle=False)\n",
    "Y_cell6_train, Y_cell6_test = train_test_split(Y_cell6,test_size=0.2,shuffle=False)\n",
    "print(Y_cell1_train.shape,Y_cell1_test.shape)\n",
    "nt_train = len(Y_cell1_train)\n",
    "nt_test = len(Y_cell1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf1146-147b-4be5-a53e-a3de7e912525",
   "metadata": {},
   "outputs": [],
   "source": [
    "### scale the ozone grid cell values to make their errors comparable\n",
    "### each grid cell should be of equal importance for the final score\n",
    "### so make sure you always do this\n",
    "scaler_y_cell1 = StandardScaler().fit(Y_cell1_train[:,np.newaxis])\n",
    "Y_cell1_train_norm = scaler_y_cell1.transform(Y_cell1_train[:,np.newaxis])\n",
    "Y_cell1_test_norm = scaler_y_cell1.transform(Y_cell1_test[:,np.newaxis])\n",
    "#\n",
    "scaler_y_cell2 = StandardScaler().fit(Y_cell2_train[:,np.newaxis])\n",
    "Y_cell2_train_norm = scaler_y_cell2.transform(Y_cell2_train[:,np.newaxis])\n",
    "Y_cell2_test_norm = scaler_y_cell2.transform(Y_cell2_test[:,np.newaxis])\n",
    "#\n",
    "scaler_y_cell3 = StandardScaler().fit(Y_cell3_train[:,np.newaxis])\n",
    "Y_cell3_train_norm = scaler_y_cell3.transform(Y_cell3_train[:,np.newaxis])\n",
    "Y_cell3_test_norm = scaler_y_cell3.transform(Y_cell3_test[:,np.newaxis])\n",
    "#\n",
    "scaler_y_cell4 = StandardScaler().fit(Y_cell4_train[:,np.newaxis])\n",
    "Y_cell4_train_norm = scaler_y_cell4.transform(Y_cell4_train[:,np.newaxis])\n",
    "Y_cell4_test_norm = scaler_y_cell4.transform(Y_cell4_test[:,np.newaxis])\n",
    "#\n",
    "scaler_y_cell5 = StandardScaler().fit(Y_cell5_train[:,np.newaxis])\n",
    "Y_cell5_train_norm = scaler_y_cell5.transform(Y_cell5_train[:,np.newaxis])\n",
    "Y_cell5_test_norm = scaler_y_cell5.transform(Y_cell5_test[:,np.newaxis])\n",
    "#\n",
    "scaler_y_cell6 = StandardScaler().fit(Y_cell6_train[:,np.newaxis])\n",
    "Y_cell6_train_norm = scaler_y_cell6.transform(Y_cell6_train[:,np.newaxis])\n",
    "Y_cell6_test_norm = scaler_y_cell6.transform(Y_cell6_test[:,np.newaxis])\n",
    "\n",
    "del Y_cell1_train, Y_cell1_test, Y_cell2_train, Y_cell2_test, Y_cell3_train, Y_cell3_test\n",
    "del Y_cell4_train, Y_cell4_test, Y_cell5_train, Y_cell5_test, Y_cell6_train, Y_cell6_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b015a62-603e-49e7-9b64-ef587943d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### plot normalized ozone time series after scaling\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(0,nt_train),Y_cell1_train_norm,label='cell 1')\n",
    "plt.plot(np.arange(0,nt_train),Y_cell2_train_norm,label='cell 2')\n",
    "plt.plot(np.arange(0,nt_train),Y_cell3_train_norm,label='cell 3')\n",
    "plt.plot(np.arange(0,nt_train),Y_cell4_train_norm,label='cell 4')\n",
    "plt.plot(np.arange(0,nt_train),Y_cell5_train_norm,label='cell 5')\n",
    "plt.plot(np.arange(0,nt_train),Y_cell6_train_norm,label='cell 6')\n",
    "plt.xlabel('Number timesteps (days)',size=16)\n",
    "plt.ylabel('Ozone mmr (normalized)',size=16)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d4892-e9ce-408b-9356-615d6da0192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### depending on the input features and regression function you choose, you might also want to scale the input features\n",
    "#### this would for example not be necessary for random forests\n",
    "scaler_x_cell1 = StandardScaler().fit(X_cell1_train[:,:])\n",
    "X_cell1_train_norm = scaler_x_cell1.transform(X_cell1_train[:,:])\n",
    "X_cell1_test_norm = scaler_x_cell1.transform(X_cell1_test[:,:])\n",
    "#\n",
    "scaler_x_cell2 = StandardScaler().fit(X_cell2_train[:,:])\n",
    "X_cell2_train_norm = scaler_x_cell2.transform(X_cell2_train[:,:])\n",
    "X_cell2_test_norm = scaler_x_cell2.transform(X_cell2_test[:,:])\n",
    "#\n",
    "scaler_x_cell3 = StandardScaler().fit(X_cell3_train[:,:])\n",
    "X_cell3_train_norm = scaler_x_cell3.transform(X_cell3_train[:,:])\n",
    "X_cell3_test_norm = scaler_x_cell3.transform(X_cell3_test[:,:])\n",
    "#\n",
    "scaler_x_cell4 = StandardScaler().fit(X_cell4_train[:,:])\n",
    "X_cell4_train_norm = scaler_x_cell4.transform(X_cell4_train[:,:])\n",
    "X_cell4_test_norm = scaler_x_cell4.transform(X_cell4_test[:,:])\n",
    "#\n",
    "scaler_x_cell5 = StandardScaler().fit(X_cell5_train[:,:])\n",
    "X_cell5_train_norm = scaler_x_cell5.transform(X_cell5_train[:,:])\n",
    "X_cell5_test_norm = scaler_x_cell5.transform(X_cell5_test[:,:])\n",
    "#\n",
    "scaler_x_cell6 = StandardScaler().fit(X_cell6_train[:,:])\n",
    "X_cell6_train_norm = scaler_x_cell6.transform(X_cell6_train[:,:])\n",
    "X_cell6_test_norm = scaler_x_cell6.transform(X_cell6_test[:,:])\n",
    "### note you still have the non-normalized input features available as X_cell1_train, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138dc2b-8227-4c6e-aea8-42eca89df762",
   "metadata": {},
   "source": [
    "## Define the six regression models\n",
    "Example implementation using a typical `sklearn` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b72eb3-5cbd-449f-b398-bc4f9d3617f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we use the same forward-directed TimeSeriesSplit() CV method here\n",
    "### alternatively you might try KFold() etc\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html\n",
    "cv_obj = TimeSeriesSplit(n_splits=4)\n",
    "### you might want to see how changing the number of splits affects your results\n",
    "\n",
    "### now we will define separate sets of hyperparameters for the GridSearch\n",
    "### define dictionary to collect the best estimators for the six regressions\n",
    "best_estimators = {}\n",
    "\n",
    "### set parameters for a typical Ridge regression\n",
    "alpha_i=[0.03,0.1,0.3,1,10,30,100,300,1000,3000,10000,30000]\n",
    "parameters = {\n",
    "    'alpha': alpha_i,\n",
    "    'fit_intercept': [False,True],\n",
    "    'max_iter':[1000],\n",
    "    'random_state':[100]\n",
    "             }\n",
    "\n",
    "regressor_type = Ridge()\n",
    "\n",
    "### Triple step: 1) Regression object defintion, 2) Fit function, 3) Make prediction\n",
    "### Cell 1\n",
    "regr1 = GridSearchCV(regressor_type,parameters,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr1.fit(X_cell1_train_norm,Y_cell1_train_norm)\n",
    "# might want to check error on training data below, to compare to test error \n",
    "Y_pred1_train = regr1.best_estimator_.predict(X_cell1_train_norm)\n",
    "Y_pred1 = regr1.best_estimator_.predict(X_cell1_test_norm)\n",
    "print(regr1.best_estimator_.alpha)\n",
    "best_estimators['cell1'] = regr1.best_estimator_\n",
    "### Cell 2\n",
    "regr2 = GridSearchCV(regressor_type,parameters,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr2.fit(X_cell2_train_norm,Y_cell2_train_norm)\n",
    "Y_pred2_train = regr2.best_estimator_.predict(X_cell2_train_norm)\n",
    "Y_pred2 = regr2.best_estimator_.predict(X_cell2_test_norm)\n",
    "print(regr2.best_estimator_.alpha)\n",
    "best_estimators['cell2'] = regr2.best_estimator_\n",
    "### Cell 3\n",
    "regr3 = GridSearchCV(regressor_type,parameters,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr3.fit(X_cell3_train_norm,Y_cell3_train_norm)\n",
    "Y_pred3_train = regr3.best_estimator_.predict(X_cell3_train_norm)\n",
    "Y_pred3 = regr3.best_estimator_.predict(X_cell3_test_norm)\n",
    "print(regr3.best_estimator_.alpha)\n",
    "best_estimators['cell3'] = regr3.best_estimator_\n",
    "### Cell 4\n",
    "regr4 = GridSearchCV(regressor_type,parameters,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr4.fit(X_cell4_train_norm,Y_cell4_train_norm)\n",
    "Y_pred4_train = regr4.best_estimator_.predict(X_cell4_train_norm)\n",
    "Y_pred4 = regr4.best_estimator_.predict(X_cell4_test_norm)\n",
    "print(regr4.best_estimator_.alpha)\n",
    "best_estimators['cell4'] = regr4.best_estimator_\n",
    "### Cell 5\n",
    "regr5 = GridSearchCV(regressor_type,parameters,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr5.fit(X_cell5_train_norm,Y_cell5_train_norm)\n",
    "Y_pred5_train = regr5.best_estimator_.predict(X_cell5_train_norm)\n",
    "Y_pred5 = regr5.best_estimator_.predict(X_cell5_test_norm)\n",
    "print(regr5.best_estimator_.alpha)\n",
    "best_estimators['cell5'] = regr5.best_estimator_\n",
    "### Cell 6\n",
    "regr6 = GridSearchCV(regressor_type,parameters,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr6.fit(X_cell6_train_norm,Y_cell6_train_norm)\n",
    "Y_pred6_train = regr6.best_estimator_.predict(X_cell6_train_norm)\n",
    "Y_pred6 = regr6.best_estimator_.predict(X_cell6_test_norm)\n",
    "print(regr6.best_estimator_.alpha)\n",
    "best_estimators['cell6'] = regr6.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f03d9-0b1d-42f7-afe3-15cde6d59e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualize CV the results for cell 1 in a pandas table\n",
    "### you can use this to see which parameters are easiest to tune\n",
    "pd.DataFrame(regr1.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9c99a-1c29-4320-832a-bead38b3f7c6",
   "metadata": {},
   "source": [
    "#### A few visualizations of the results for test data + example of error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23921c68-fe30-4a14-bbc5-1a7898858402",
   "metadata": {},
   "outputs": [],
   "source": [
    "### a few list that make it easier to iterate through plots etc\n",
    "lat_list = [cell1_lat[0], cell2_lat[0], cell3_lat[0], cell4_lat[0], cell5_lat[0], cell6_lat[0]]\n",
    "lon_list = [cell1_lon[0], cell2_lon[0], cell3_lon[0], cell4_lon[0], cell5_lon[0], cell6_lon[0]]\n",
    "z_list = [cell1_z[0], cell2_z[0], cell3_z[0], cell4_z[0], cell5_z[0], cell6_z[0]]\n",
    "ypred_list = [Y_pred1, Y_pred2, Y_pred3, Y_pred4, Y_pred5, Y_pred6]\n",
    "y_norms = [Y_cell1_test_norm, Y_cell2_test_norm, Y_cell3_test_norm, Y_cell4_test_norm, Y_cell5_test_norm, Y_cell6_test_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed517559-e62f-4e83-86ec-82c1482d9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot time series of predictions and the ground truth (i.e. the UKESM simulations) for the test data\n",
    "### good sanity check for the quality of the fits\n",
    "plt.rcParams['figure.figsize'] = [25,50]\n",
    "figs, axs = plt.subplots(6)\n",
    "for i in range(0,6):\n",
    "    axs[i].set_title('Cell '+str(i+1)+ '; lat: '+str(\"%.2f\" % lat_list[i])+', lon: '+str(\"%.2f\" % lon_list[i])+', altitude: '+str(\"%.2f\" % z_list[i])+'km',size=20)\n",
    "    axs[i].plot(np.arange(0,nt_test),y_norms[i],color='k',linewidth=2,label='UKESM data')\n",
    "    axs[i].plot(np.arange(0,nt_test),ypred_list[i],color='r',linewidth=2,label='ML predictions')\n",
    "    axs[i].set_xlabel('Timesteps (days)',size=20)\n",
    "    axs[i].set_ylabel('Ozone mmr (normalized)',size=20)\n",
    "    axs[i].legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b2862-c216-40f6-b5aa-1afbf2c68446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### measure your testset error for each grid cell individually\n",
    "err1 = mean_squared_error(Y_pred1,Y_cell1_test_norm)\n",
    "err1_train = mean_squared_error(Y_pred1_train,Y_cell1_train_norm)\n",
    "print('Test error grid cell 1: ', \"%.5f\" % err1, '; Error on training data: ',\"%.5f\" % err1_train)\n",
    "err2 = mean_squared_error(Y_pred2,Y_cell2_test_norm)\n",
    "err2_train = mean_squared_error(Y_pred2_train,Y_cell2_train_norm)\n",
    "print('Test error grid cell 2: ', \"%.5f\" % err2, '; Error on training data: ',\"%.5f\" % err2_train)\n",
    "err3 = mean_squared_error(Y_pred3,Y_cell3_test_norm)\n",
    "err3_train = mean_squared_error(Y_pred3_train,Y_cell3_train_norm)\n",
    "print('Test error grid cell 3: ', \"%.5f\" % err3, '; Error on training data: ',\"%.5f\" % err3_train)\n",
    "err4 = mean_squared_error(Y_pred4,Y_cell4_test_norm)\n",
    "err4_train = mean_squared_error(Y_pred4_train,Y_cell4_train_norm)\n",
    "print('Test error grid cell 4: ', \"%.5f\" % err4, '; Error on training data: ',\"%.5f\" % err4_train)\n",
    "err5 = mean_squared_error(Y_pred5,Y_cell5_test_norm)\n",
    "err5_train = mean_squared_error(Y_pred5_train,Y_cell5_train_norm)\n",
    "print('Test error grid cell 5: ', \"%.5f\" % err5, '; Error on training data: ',\"%.5f\" % err5_train)\n",
    "err6 = mean_squared_error(Y_pred6,Y_cell6_test_norm)\n",
    "err6_train = mean_squared_error(Y_pred6_train,Y_cell6_train_norm)\n",
    "print('Test error grid cell 6: ', \"%.5f\" % err6, '; Error on training data: ',\"%.5f\" % err6_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b4be7-20b3-4ffc-a635-cae8c41b8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate average error for all grid cells\n",
    "err_average = (err1+err2+err3+err4+err5+err6)/6.0\n",
    "print('This is the final score you are trying to improve (but be aware of overfitting the test data)')\n",
    "print(err_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4e0c6-0fda-4202-942b-5ba8222462b7",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "Implement your own three solutions in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bcd1d-f556-47e7-9818-5229f1ce03c4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f6f322b8b76e15e43c8f2d0799e3678",
     "grade": true,
     "grade_id": "Model_1",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Model 1\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fb4e9-b36f-4361-b216-b36ebefb6df1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6cce37c187efaaf94f37183627a7e4",
     "grade": true,
     "grade_id": "Model_2",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Model 2\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66289c8d-4be2-46a4-94c0-78f913508c48",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1bbd6ed5103ee4feac843f2e2e2df8d",
     "grade": true,
     "grade_id": "Model_3",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Model 3\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233558d-6b59-4c35-8809-a1e9020e2e96",
   "metadata": {},
   "source": [
    "### Save your best model\n",
    "\n",
    "Note: save the model(s) that you think is (are) best only. It should be clear from the dictionary you save which regression belongs to which grid cell.\n",
    "\n",
    "In addition, make sure to add an entry of which predictors you used, as we do below. For transformations, name the additional predictors sensibly and also alert us to this in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44c1e5-e393-42b5-a95f-87106afc5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760be57d-d024-49b0-b155-32d79874b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add predictor names to dictionary\n",
    "best_estimators['predictors'] = columns\n",
    "### specify a sensible group name for your submission\n",
    "### CHANGE\n",
    "group_name = 'your_names_final'\n",
    "### save the model that you think is BEST only.\n",
    "### for most model types you should be able to use joblib\n",
    "### and the dictionary format used in the example above\n",
    "joblib.dump(best_estimators, './model_objects/'+group_name+'_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72543fd-0788-4ba1-b7cf-9e6285cec217",
   "metadata": {},
   "source": [
    "## Three ways to fit a feedforward neural network\n",
    "\n",
    "Three different examples written using `sklearn`, `keras`, and `pytorch`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe52981-8951-468b-a03f-20e690067f67",
   "metadata": {},
   "source": [
    "#### sklearn\n",
    "\n",
    "See also Worksheet 1. You can find the function documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), including an explanation of the many hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad73ba-ad7c-40ef-8bda-85d93bb9d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "#\n",
    "reg_type = MLPRegressor()\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(10,10),(100,100)],\n",
    "    'activation': ['tanh','relu'],\n",
    "    'alpha': [0,1e-3,1e-2],\n",
    "    'shuffle': [False],\n",
    "    'solver': ['adam'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "regr_cv = GridSearchCV(reg_type,parameters,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr_cv.fit(X_cell1_train_norm, Y_cell1_train_norm[:,0])\n",
    "y_pred_NN = regr_cv.best_estimator_.predict(X_cell1_test_norm)\n",
    "mse_NN = mean_squared_error(Y_cell1_test_norm,y_pred_NN)\n",
    "print('MLP regressor sklearn error on test data: ',round(mse_NN,5))\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.plot(np.arange(0,y_pred_NN.shape[0]),Y_cell1_test_norm,color='k',label='Ground truth')\n",
    "plt.plot(np.arange(0,y_pred_NN.shape[0]),y_pred_NN,color='r',label='Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# pd.DataFrame(regr_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785e052-3145-4d03-bd9d-906e66b30d75",
   "metadata": {},
   "source": [
    "#### Keras\n",
    "\n",
    "Keras is a powerful yet easy-to-use Python library for developing and evaluating deep learning models.\n",
    "\n",
    "It is part of the `TensorFlow` library and can also use a `Theano` backend. There are many good online tutorials. If you have never used it before, this [blog post](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/) by Jason Brownlee provides a nice walk-through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb92cb-2cb0-4408-9508-e3521c3f0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define a simple two-layer neural network using keras with the tensorflow backend\n",
    "### models in Keras are defined as a sequence of network layers\n",
    "### So we create a 'Sequential' model https://keras.io/models/sequential/\n",
    "### or so for example tutorials like this one here https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "### concerning how to find the optimal number of layers, see e.g. https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "def create_NN(dropout_rate=0.0,kernel_reg1=0.0,bias_reg1=0.0,kernel_reg2=0.0,bias_reg2=0.0):\n",
    "    # print(kernel_reg1,bias_reg1,kernel_reg2,bias_reg2,'Regularization parameters')\n",
    "    # print(dropout_rate,'dropout rate')\n",
    "    model = Sequential()\n",
    "    ### the first hidden layer receives the inputs. This must allow as many inputs in as we have input features\n",
    "    ### we use the relu activation function. Feel free to try out others, see https://keras.io/activations/\n",
    "    ### for kernel_initializers see https://keras.io/initializers/\n",
    "    ### concerning the role of weight/bias regularizers, see e.g. \n",
    "    ### here: https://stats.stackexchange.com/questions/383310/difference-between-kernel-bias-and-activity-regulizers-in-keras\n",
    "    ### e.g. you could also use an activity regularizer that works on the loss function itself - just as we did for Ridge regression\n",
    "    nodes_layer1 = 8\n",
    "    nodes_layer2 = 8\n",
    "    ### fully connected layers are defined using the Dense class\n",
    "    model.add(Dense(nodes_layer1,kernel_initializer='random_uniform',kernel_regularizer=l2(kernel_reg1),\n",
    "                    bias_regularizer=l2(bias_reg1),input_dim=(X_cell1_train_norm.shape[1]),activation='relu'))\n",
    "    ### we can use dropout regularization; default is none here\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    ### then we add a second hidden layer\n",
    "    model.add(Dense(nodes_layer2,kernel_initializer='random_uniform',kernel_regularizer=l2(kernel_reg2),\n",
    "                    bias_regularizer=l2(bias_reg2),activation='relu'))\n",
    "    ### then we define the connection to the outputs; if you run a classification you would want to make this e.g. sigmoid or softmax\n",
    "    ### function for sure; here you could as well use a linear activation function; we have one output -> each grid cell\n",
    "    ### activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "    model.add(Dense(1,kernel_initializer='random_uniform',activation='linear'))\n",
    "    ### now construct the model. Define a loss function https://keras.io/losses/\n",
    "    ### you can also choose different gradient descent optimizers and metrics to evaluate your models' performance \n",
    "    ### https://keras.io/metrics/\n",
    "    ### metrics are similar to a loss function, except that the results from evaluating a metric \n",
    "    ### not used when training the model. \n",
    "    ### You may use any of the loss functions as a metric function.\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mse'])\n",
    "    return model\n",
    "### define neural network parameters for cross-validation    \n",
    "### for more tuning parameters see for example: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "### https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/\n",
    "parameters_NN = {\n",
    "        'epochs': [2,10],\n",
    "        'batch_size': [10],\n",
    "        'dropout_rate': [0,0.2],\n",
    "        'kernel_reg1': [0.00],\n",
    "        'bias_reg1': [0.00],\n",
    "        'kernel_reg2': [0.00],\n",
    "        'bias_reg2': [0.00]\n",
    "        \n",
    "}\n",
    "model_NN = KerasRegressor(build_fn=create_NN,verbose=10)\n",
    "regr1 = GridSearchCV(model_NN,parameters_NN,cv=cv_obj,n_jobs=-1,refit=True)\n",
    "regr1.fit(X_cell1_train_norm,Y_cell1_train_norm)\n",
    "history = regr1.best_estimator_.model.history\n",
    "Y_pred1 = regr1.best_estimator_.predict(X_cell1_test_norm)\n",
    "### one might also want to test the predictions on the training dataset. Do we overfit in comparison to the test data?\n",
    "Y_pred1_train = regr1.best_estimator_.predict(X_cell1_train_norm)\n",
    "print(regr1.best_estimator_)\n",
    "# best_estimators['cell1'] = regr1.best_estimator_\n",
    "# plot history\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.xlabel('No epochs',size=16)\n",
    "plt.ylabel('Error',size=16)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(np.arange(0,Y_pred1.shape[0]),Y_cell1_test_norm,color='k',label='Ground truth')\n",
    "plt.plot(np.arange(0,Y_pred1.shape[0]),Y_pred1,color='r',label='Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "mse_NN = mean_squared_error(Y_cell1_test_norm,Y_pred1)\n",
    "print('Keras regressor error on test data: ',round(mse_NN,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602171b-86c1-4cb3-96ca-36bb71f5b06a",
   "metadata": {},
   "source": [
    "#### PyTorch\n",
    "\n",
    "PyTorch is a Deep Learning framework developed by Facebook’s AI Research Lab. \n",
    "\n",
    "Once again, there are many great online tutorials, see e.g. its own [quickstart tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html). Below, we provide an example implementation combined with `skorch`, which allows for the use of `sklearn` cross-validation methods. Forr detailed explanations see for example [this blog post](https://machinelearningmastery.com/use-pytorch-deep-learning-models-with-scikit-learn/).\n",
    "\n",
    "Feel free to use the Hackathon to try out your own ideas and network architectures! Data science always involves independent experimentation, and online searches for interesting network designs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133a91c-a83c-4425-a133-08928e081276",
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to convert data to PyTorch tensor\n",
    "X_cell1_train_norm_pyT = torch.tensor(X_cell1_train_norm, dtype=torch.float32)\n",
    "X_cell1_test_norm_pyT = torch.tensor(X_cell1_test_norm, dtype=torch.float32)\n",
    "Y_cell1_train_norm_pyT = torch.tensor(Y_cell1_train_norm, dtype=torch.float32)\n",
    "Y_cell1_test_norm_pyT = torch.tensor(Y_cell1_test_norm, dtype=torch.float32)\n",
    "### training parameters\n",
    "### define the model\n",
    "class OzoneRegressor(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5, weight_constraint=1.0, nodes_layer1 = 8, nodes_layer2 = 8):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(X_cell1_train_norm.shape[1], nodes_layer1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(nodes_layer1, nodes_layer2)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.output = nn.Linear(nodes_layer2, 1)\n",
    "        self.weight_constraint = weight_constraint\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    " \n",
    "# create model with skorch\n",
    "model = NeuralNetRegressor(\n",
    "    OzoneRegressor,\n",
    "    criterion=nn.MSELoss(),\n",
    "    optimizer=optim.Adam,\n",
    "    max_epochs=100,\n",
    "    batch_size=360,\n",
    "    verbose=False\n",
    ")\n",
    " \n",
    "# define the grid search parameters, thanks to skorch we can use GridSearchCV\n",
    "param_grid = {\n",
    "    'module__weight_constraint': [0.0,0.1],\n",
    "    'module__dropout_rate': [0.0,0.1],\n",
    "    'module__nodes_layer1': [8,16],\n",
    "    'module__nodes_layer2': [8,16]\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_cell1_train_norm_pyT, Y_cell1_train_norm_pyT)\n",
    " \n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    " \n",
    "Y_pred1 = grid.best_estimator_.predict(X_cell1_test_norm_pyT)\n",
    "plt.plot(np.arange(0,Y_pred1.shape[0]),Y_cell1_test_norm,color='k',label='Ground truth')\n",
    "plt.plot(np.arange(0,Y_pred1.shape[0]),Y_pred1,color='r',label='Predictions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "mse_NN = mean_squared_error(Y_cell1_test_norm,Y_pred1)\n",
    "print('PyTorch regressor error on test data: ',round(mse_NN,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-climate",
   "language": "python",
   "name": "ml-climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
