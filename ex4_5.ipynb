{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ex4.mdp import MDP\n",
    "from ex4.misc import make_grader\n",
    "from ex4.misc import plot_v_pi\n",
    "from ex4.misc import expected_output_vi_finite_1, expected_output_vi_finite_2, expected_output_vi_infinite, expected_output_vi_infinite_stoch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP\n",
    "\n",
    "The MDP class for the cleaning robot is already implemented and doesn't need to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Actions\n",
    "DOWN = 0\n",
    "RIGHT = 1\n",
    "UP = 2\n",
    "LEFT = 3\n",
    "STAY = 4\n",
    "\n",
    "\n",
    "class RoboMDP(MDP):\n",
    "    O = -1e3  # Obstacles, walls or inaccessible places\n",
    "    D = 1  # Dirt\n",
    "    W = -1  # Water\n",
    "    C = -30  # Cat\n",
    "    T = 10  # Toy\n",
    "    E = 0  # Empty\n",
    "\n",
    "    grid_world = {\n",
    "        \"reward\": [E, E, E, E, E, E, E, W, E, E,\n",
    "                   E, O, E, E, E, E, E, O, D, O,\n",
    "                   E, O, E, D, O, E, O, D, O, T,\n",
    "                   E, E, E, O, O, E, E, C, E, E,\n",
    "                   O, O, E, E, W, E, E, O, O, E,\n",
    "                   E, E, O, E, E, E, E, O, O, E,\n",
    "                   E, E, E, E, E, E, W, E, E, E, \n",
    "                   E, E, E, E, E, O, E, E, D, W,\n",
    "                   O, W, O, O, O, O, E, E, E, E,\n",
    "                   D, E, E, E, D, O, E, O, O, O],\n",
    "        \"map\": [\n",
    "            \"       W  \",\n",
    "            \" O     ODO\",\n",
    "            \" O DO ODOT\",\n",
    "            \"   OO  C  \",\n",
    "            \"OO  W  OO \",\n",
    "            \"  O    OO \",\n",
    "            \"      W   \",\n",
    "            \"     O  DW\",\n",
    "            \"OWOOOO    \",\n",
    "            \"D   DO OOO\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    n_actions = 5\n",
    "    world_dim = (10, 10)\n",
    "\n",
    "    # Transition probabilities. Each row corresponds to one action, i.e. choosing action down has a 70% chance of\n",
    "    # executing correctly, and 10% chance for executing right, left or stay.\n",
    "    # Usage: self.tp[actual_action, noisy_executed_action]\n",
    "\n",
    "    #             down, right, up, left, stay\n",
    "    tp = np.array([[0.7, 0.1, 0.0, 0.1, 0.1],  # down\n",
    "                  [0.1, 0.7, 0.1, 0.0, 0.1],  # right\n",
    "                  [0.0, 0.1, 0.7, 0.1, 0.1],  # up\n",
    "                  [0.1, 0.0, 0.1, 0.7, 0.1],  # left\n",
    "                  [0.0, 0.0, 0.0, 0.0, 1.0]])  # stay\n",
    "\n",
    "    def __init__(self, discount: float, stochastic: bool):\n",
    "        super(RoboMDP, self).__init__(discount, self.world_dim, self.n_actions, desc=self.grid_world['map'])\n",
    "\n",
    "        def inc(row: int, col: int, action: int):\n",
    "            if action == DOWN:  # move down\n",
    "                row = min(row+1, self.world_dim[0] - 1)\n",
    "            elif action == RIGHT:  # move right\n",
    "                col = min(col+1, self.world_dim[1] - 1)\n",
    "            elif action == UP:  # move up\n",
    "                row = max(row - 1, 0)\n",
    "            elif action == LEFT:  # move left\n",
    "                col = max(col - 1, 0)\n",
    "            elif action == STAY:  # stay\n",
    "                pass  # Not moving\n",
    "            return row, col\n",
    "\n",
    "        # We need to set the next state with its corresponding transition probability based on the current state and\n",
    "        # action. Have a look at mdp.py for the implementation of self.stp\n",
    "        for r in range(self.world_dim[0]):\n",
    "            for c in range(self.world_dim[1]):\n",
    "                s = self.to_s(r, c)\n",
    "                self.rewards[s] = self.grid_world[\"reward\"][s]\n",
    "                for a in range(self.n_actions):\n",
    "                    stp_list = self.stp[s, a]\n",
    "                    if stochastic:\n",
    "                        for noisy_a in range(self.n_actions):\n",
    "                            if self.tp[a, noisy_a] > 0:\n",
    "                                next_r, next_c = inc(r, c, noisy_a)\n",
    "                                next_state = self.to_s(next_r, next_c)\n",
    "                                stp_list.append((next_state, self.tp[a, noisy_a]))\n",
    "                    else:\n",
    "                        next_r, next_c = inc(r, c, a)\n",
    "                        next_state = self.to_s(next_r, next_c)\n",
    "                        stp_list.append((next_state, 1.))\n",
    "\n",
    "    # helper functions\n",
    "\n",
    "    def to_s(self, row, col):\n",
    "        \"\"\"given a row and column, returns a linear index corresponding to the world dimension\"\"\"\n",
    "        return row * self.world_dim[1] + col\n",
    "\n",
    "    def to_rc(self, s):\n",
    "        \"\"\"given a linear state, returns a row and column corresponding to the world dimension\"\"\"\n",
    "        return np.unravel_index(s, self.world_dim)\n",
    "    \n",
    "    def plot(self):\n",
    "        n_rows = self.world_dim[0]\n",
    "        n_cols = self.world_dim[1]\n",
    "\n",
    "        plt.figure(figsize=(n_rows, n_cols))\n",
    "        plt.imshow(self.rewards.reshape((n_rows, n_cols)), cmap='gray', interpolation='none')  #, clim=(0, 1))\n",
    "        ax = plt.gca()\n",
    "        ax.set_xticks(np.arange(n_cols) - .5)\n",
    "        ax.set_yticks(np.arange(n_rows) - .5)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        Y, X = np.mgrid[0:n_rows, 0:n_cols]\n",
    "        for x, y in zip(X.flatten(), Y.flatten()):\n",
    "            plt.text(x, y, self.desc[y][x],\n",
    "                     color='g', size=12, verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "        plt.grid(color='b', lw=2, ls='-')\n",
    "        plt.savefig(\"/tmp/robo_mdp.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing value iteration\n",
    "For exercise 4, you'll implement value iteration, which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $V_{(0)}^\\ast(s)=0$, for all $s$\n",
    "\n",
    "For $i=0, 1, 2, \\dots$\n",
    "- $V^\\ast_{(i+1)}(s) = \\max_a  \\Big( r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) \\, V^\\ast_{(i)}(s') \\Big)$, for all $s$\n",
    "- Until convergence or horizon $H$\n",
    "\n",
    "---\n",
    "\n",
    "We additionally define the sequence of greedy policies $\\pi_{(0)}, \\pi_{(1)}, \\dots, \\pi_{(n-1)}$, where\n",
    "$$\\pi^\\ast_{(i)}(s) = \\underset{a}{\\arg \\max} \\Big( r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) V^\\ast_{(i)}(s')\\Big)$$\n",
    "\n",
    "Your code will return two lists: $[V_{(0)}, V_{(1)}, \\dots, V_{(n)}]$ and $[\\pi_{(0)}, \\pi_{(1)}, \\dots, \\pi_{(n-1)}]$\n",
    "\n",
    "To ensure that you get the same policies as the reference solution, choose the lower-index action to break ties in $\\arg \\max_a$. This is done automatically by np.argmax. This will only affect the \"# chg actions\" printout below--it won't affect the values computed.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Warning: make a copy of your value function each iteration and use that copy for the update--don't update your value function in place.\n",
    "Updating in-place is also a valid algorithm, sometimes called Gauss-Seidel value iteration or asynchronous value iteration, but it will cause you to get different results than our reference solution (which in turn will mean that our testing code wonâ€™t be able to help in verifying your code).\n",
    "</div>\n",
    "\n",
    "#### Hints\n",
    "- The reward function can be accessed by $\\texttt{mdp.rewards[s]}$, the next states and their probability of reaching\n",
    " them (given a state $s$ and and action $a$) by $\\texttt{mdp.stp[s, a]}$\n",
    "- Use the functions $\\texttt{to_s}$ and $\\texttt{to_rc}$ provided by the MDP class to convert between linear states\n",
    " $s \\in [0, |\\mathcal{S}|]$ and row/column format, i.e. $\\texttt{mdp.to_rc(0)}$ returns $(0, 0)$, while\n",
    " $\\texttt{mdp.to_s(0, 0)}$ returns $0$\n",
    "- For your implementation, the identities $V^\\ast(s) = \\max_a Q^\\ast(s, a)$ and\n",
    " $\\pi^\\ast(s) = \\underset{a}{\\arg \\max} \\, Q^\\ast(s, a)$ may be helpful\n",
    "- Following numpy functions may be useful: $\\texttt{np.sum}$, $\\texttt{np.max}$, $\\texttt{np.argmax}$\n",
    "- Try using list comprehensions as this will shorten your code\n",
    "- For the infinite horizon case, we request 100 iterations of value iteration. However, the algorithm should converge\n",
    "earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp: MDP, n_iter: int, grade_print=print) -> (list, list):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    @param mdp: an MDP object\n",
    "    @param n_iter: number of VI iterations\n",
    "    @param grade_print:\n",
    "    Outputs:\n",
    "    @return: (value_functions, policies)\n",
    "\n",
    "    len(value_functions) == n_iter+1 and len(policies) == n_iter\n",
    "    \"\"\"\n",
    "    vs = [np.zeros(shape=mdp.world_dim)]  # list of value functions contains the initial value function V_{(0)}, which is zero\n",
    "    pis = []  # list containing sequence of previous greedy policies\n",
    "\n",
    "    grade_print(\"Iteration |  max|V-Vprev|  | # chg actions | V[0]    \")\n",
    "    grade_print(\"----------+----------------+---------------+---------\")\n",
    "    for it in range(0, n_iter):\n",
    "        pi_old = pis[-1] if len(pis) > 0 else None  # \\pi^{(it)} = Greedy[V^{(it-1)}]. Just used for printout\n",
    "        v_old = vs[-1]\n",
    "        \n",
    "        # Exercise 4 a)\n",
    "        # %%%%%%%% add your code here %%%%%%%%%%%\n",
    "\n",
    "        pi = np.zeros(shape=mdp.world_dim)  # contains the actual actions\n",
    "        v = np.zeros(shape=mdp.world_dim)  # new value function\n",
    "        \n",
    "        ...\n",
    "\n",
    "        # %%%%%%%% add your code here %%%%%%%%%%%\n",
    "\n",
    "        max_diff = np.abs(v - v_old).max()\n",
    "        n_chg_actions = 0 if pi_old is None else (pi != pi_old).sum()\n",
    "        grade_print(\"{:4d}      | {:12.5f}   |   {:4d}        | {:8.3f}\".format(it, max_diff, n_chg_actions, v[0, 0]))\n",
    "\n",
    "        vs.append(v)\n",
    "        pis.append(pi)\n",
    "\n",
    "        if max_diff < 1e-3:\n",
    "            break\n",
    "\n",
    "    return vs, pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 b) Finite Horizon Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "robo_mdp_finite = RoboMDP(discount=0.5, stochastic=False)\n",
    "v_finite, pi_finite = value_iteration(robo_mdp_finite, 10, make_grader(expected_output_vi_finite_1))\n",
    "\n",
    "plot_v_pi(v_finite[-1], pi_finite[-1], robo_mdp_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "robo_mdp_finite = RoboMDP(discount=0.9, stochastic=False)\n",
    "v_finite, pi_finite = value_iteration(robo_mdp_finite, 10, make_grader(expected_output_vi_finite_2))\n",
    "\n",
    "plot_v_pi(v_finite[-1], pi_finite[-1], robo_mdp_finite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 c) Infinite Horizon Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "robo_mdp_infinite = RoboMDP(discount=0.8, stochastic=False)\n",
    "v_infinite, pi_infinite = value_iteration(robo_mdp_infinite, 100, make_grader(expected_output_vi_infinite))\n",
    "\n",
    "plot_v_pi(v_infinite[-1], pi_infinite[-1], robo_mdp_infinite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 d) Infinite Horizon Problem with Stochastic Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "robo_mdp_finite_stoch = RoboMDP(discount=0.8, stochastic=True)\n",
    "v_finite_stoch, pi_finite_stoch = value_iteration(robo_mdp_finite_stoch, 100, make_grader(expected_output_vi_infinite_stoch))\n",
    "\n",
    "plot_v_pi(v_finite_stoch[-1], pi_finite_stoch[-1], robo_mdp_finite_stoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
