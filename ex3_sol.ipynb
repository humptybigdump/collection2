{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 3, AlphaTetris, 44P(oints)\n",
    "\n",
    "## Lab Instructions\n",
    "Your answers for **tasks a - f** should be written **in this notebook**.\n",
    "Your answers for **tasks g - j** should be written **in your solution\n",
    " pdf file**.\n",
    "\n",
    "You shouldn't need to write or modify any other files.\n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "This exercise was developed by Ge Li for the KIT Cognitive Systems Lecture,\n",
    "July 2020.\n",
    "\n",
    "## Exercise 3, a-f:\n",
    "This jupyter notebook offers a framework of **Temporal Difference (TD) learning\n",
    " with linear function approximation** in a popular Tetris game. Read the\n",
    " instructions carefully and complete the unfinished functions. Afterwards,\n",
    " you can run the training procedure in Tetris game and thereby verify your\n",
    " implementation.\n",
    "\n",
    "\n",
    "Detailed instructions:\n",
    "0. You may need to install pygame to run this notebook, e.g. open a terminal\n",
    "and type in \"pip3 install -U pygame\".<br>\n",
    "\n",
    "\n",
    "1. In script \"tetris.py\", you can find the definition of the \"Tetris\" and\n",
    "some helper functions regarding to the environment.<br>\n",
    "\n",
    "\n",
    "2. Keywords in the game:\n",
    "    - **board**: The Tetris game board (width: 10, height: 20) in a board where\n",
    "     the game is played. At the beginning of the episode, the board is initialized as\n",
    "    a blank board. When the game is going on, the falling piece will be merged\n",
    "      into the board. When one or more lines in the board are completed, the\n",
    "      lines will disappear and the player will gain some score.<br>\n",
    "    - **piece**: The Tetris game piece, which has 7 different shapes (S, Z, J,\n",
    "    L, I, O, T). Each piece is formed up with 4 boxes, and may show different\n",
    "     orientations by rotating. During the game play, each time a random piece\n",
    "      with random orientation is spawned on the middle top of the board. It\n",
    "      falls down one height level each time and the player can apply an action to\n",
    "       it and put it in a desired place. In the meantime, the player can also\n",
    "       see the next incoming piece. When the current piece\n",
    "       is placed on the bottom line of the board, it will be merged into the board. Subsequently the\n",
    "       next piece will show on the middle top of the board.<br>\n",
    "    - **action**: After a new piece is generated and before it is merged to the\n",
    "    board, a player can select an action to the piece to adjust its\n",
    "    placement. In contrast to the normal keyboard action (up, down, left,\n",
    "    right), the action used in this homework directly selects the final configuration of the piece, which is described as a tuple **[rot, col]**. The rot is\n",
    "    the number of rotations to take, and col indicates the lateral movement\n",
    "    (left or right) to take. Once the action is given, it will be decomposed\n",
    "    into a series of keyboard \"pressing\" to execute the operation like a human\n",
    "    player. <br>\n",
    "    - **terminate** If the space of the middle top on the board is already\n",
    "    occupied and therefore the next new piece cannot be spawned, then the game\n",
    "   terminates.<br>\n",
    "    - **features** In the slides of Reinforcement Learning in Cognitive\n",
    "    Systems, 22 features (height of each column, diff between two columns,\n",
    "    max column height, number of holes, and 1 as constant) were introduced. To\n",
    "    make this homework simpler, however, a simplified feature vector\n",
    "    is used, which are **the average height of all columns; the summation of\n",
    "     absolute height differences of all columns; the number of holes in the board; and\n",
    "      a constant value 1** respectively. In the function \"get_features\" you can\n",
    "     see how the original 22 features are generated from the board, and in the\n",
    "     function \"get_simplified_features\", you can see how the simplified features are generated.<br>\n",
    "\n",
    "\n",
    "3. As we have the deterministic system dynamics function $s' = f(s,a)$ for the Tetris game to predict the next state from the current state and the action available, we will use a combination of TD-Learning for learning the value function and 1-step lookahead prediction for action selection. For TD-Learning, we will use extend the algorithm given in the lecture slides (page 32, slideset RL 1) to the approximate case using linear function approximation. In order to arrive at the approximate TD-Learning algorithm, follow exactly the same steps as for deriving the approximate Q-Learning algorithm. For action selection we can compute the Q-values using the learned V-function and the 1-step lookahead prediction, i.e., $Q(s,a) = r(s,a) + \\gamma V_{\\beta}(f(s,a))$. The Tetris class given below provides all functions to retrieve all possible actions for the current state as well as to compute the next state given the current state and action. Using the computed Q-values, the action should be selected using an epsilon-greedy strategy.\n",
    "\n",
    "\n",
    "4. You can focus on the TD learning and the action selection part, the remaining code is provided.If you are interested in the\n",
    "entire Tetris game with TD learning, please see the pseudo code below:\n",
    "    - Initialize the game (Game UI, Environment etc.)<br>\n",
    "    - While True (game runs forever)<br>\n",
    "        - Initialize an episode (score, level, initial board, initial piece,\n",
    "        timer,\n",
    "     etc.)<br>\n",
    "        - While the episode is not terminated<br>\n",
    "            - If last piece is merged into the board:<br>\n",
    "                - next piece ->> current piece<br>\n",
    "                - Get a new next piece<br>          \n",
    "                - If episode is terminal:<br>\n",
    "                    - print \"Game Over\"\n",
    "                    - break<br>\n",
    "                - Else:<br>\n",
    "                    - Get action (rotations, and column) to place the\n",
    "                    current piece (**use 1-step lookahead**)<br> \n",
    "                    - Update the Value Function (**use TD Learning**)\n",
    "            - Decompose the action and get one movement to take (key = up, down, left,\n",
    "    right)<br>\n",
    "            - Perform the movement to current piece and compute its new\n",
    "            coordinates <br>\n",
    "            - Update the board (check lines completed, merge piece if possible),\n",
    "    score, level etc.<br>\n",
    "            - Plot board, piece, text, score, etc.<br><br>\n",
    "\n",
    "5. Other function specific instructions can be found above the unfinished\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "# Include some python packages and tetris env\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from ex3.tetris import Tetris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "# Initialize some Learning parameters as global parameters\n",
    "\n",
    "alpha = 0.001  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.5  # Exploration rate\n",
    "betas = np.zeros(4, dtype=float)  # Initial parameters vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Value Function Approximation <br>\n",
    "Please finish the function **\"approximate_value\"** below. This function is to\n",
    " perform **\"linear function approximation\"** to compute the state value, i.e.\n",
    "  V(s), given the features of a state. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hint:\n",
    "- You should use the parameters **\"betas\"**, as well as some useful numpy\n",
    "functions like **\"np.matmul\"** or **\"np.dot\"**.<br> Note that the linear function approximation case can be implemented by a scalar product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "\n",
    "def approximate_value(features):\n",
    "    \"\"\"\n",
    "    Given state features, calculate the approximated state value\n",
    "    :param features: features vector\n",
    "    :return: approximated state value\n",
    "    \"\"\"\n",
    "    # Assert that the parameters and features have the same dimension\n",
    "    assert features.shape == betas.shape, \"Shape do not match\"\n",
    "\n",
    "    ########   Your code begins here   ########\n",
    "    approximated_value = np.matmul(features, betas)\n",
    "    ########    Your code ends here    ########\n",
    "\n",
    "    return approximated_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Compute State Action Value Function <br>\n",
    "Please finish the function **\"q_s_a\"** below. This function is to compute the\n",
    "state action value function Q(s,a).<br>\n",
    "By default, $Q(s,a) = r(s,a) + \\gamma  E[V(s')]$. However, in the Tetris game,\n",
    "the expectation can be ignored as the game is deterministic. The function gets the reward for applying action a as well as the next board configuration $s'$ as input. You should compute \n",
    "$Q(s,a) = r(s,a) + \\gamma V(s')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hint:\n",
    "- You can call function **\"Tetris.get_simplified_features\"** to get the\n",
    "simplified features of a Tetris board.<br>\n",
    "- You can call your implemented function **\"\"approximate_value\"** above to\n",
    "approximate the value of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "\n",
    "def q_s_a(reward, next_board):\n",
    "    \"\"\"\n",
    "    Compute the state action value function Q(s,a)\n",
    "    :param reward: immediate reward\n",
    "    :param next_board: next Tetris board after the action has been performed\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "    new_features = Tetris.get_simplified_features(next_board)\n",
    "    next_value = approximate_value(new_features)\n",
    "    qsa = reward + gamma * next_value\n",
    "    ########    Your code ends here    ########\n",
    "\n",
    "    return qsa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### c) Exploration vs. Exploitation <br>\n",
    "Please finish the function **\"sample_action\"** below. This function is to\n",
    "sample an action (get its index) following the **\"$\\epsilon-greedy$\"**\n",
    "exploration strategy.<br>\n",
    "Recall the knowledge in Cognitive Systems, your implementation should contain:\n",
    "\n",
    "- It has a **\"epsilon\"** probability that the action is randomly sampled from\n",
    " all possible actions. This is Exploration.\n",
    "- Otherwise take the action with the best action's value, i.e. Q(s,a). This is\n",
    "Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hints:\n",
    "- You can use **\"max\"** function to get the max value in a list.\n",
    "- You can use **\"index\"** function to get the index of a value in a list.\n",
    "- You can use **\"random.random\"** function to generate a random float value\n",
    "in the range of [0.0, 1.0). Then you can compare this number with the\n",
    "**\"epsilon\"** to decide whether to randomly sample one action from the action\n",
    "list, or just exploit the best action.\n",
    "- You can use **\"random.randint\"** function to generate a random integer as the\n",
    " index of the sampled action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "\n",
    "def sample_action(action_list, action_value_list):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy exploration\n",
    "    :param action_list: all possible actions\n",
    "    :param action_value_list: and their values Q(s,a)\n",
    "    :return: the index of sampled action in the action list\n",
    "    \"\"\"\n",
    "\n",
    "    ########   Your code begins here   ########\n",
    "\n",
    "    # Find best action with best action value\n",
    "    best_action_value = max(action_value_list)\n",
    "    sampled_action_index = action_value_list.index(best_action_value)\n",
    "\n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        sampled_action_index = random.randint(0, len(action_list) - 1)\n",
    "    ########    Your code ends here    ########\n",
    "\n",
    "    return sampled_action_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### d) Compute Temporal Difference Error <br>\n",
    "Please finish the function **\"compute_td_error\"** below. This function will\n",
    "compute the Temporal Difference Error of the Value function, V(s).\n",
    "Depending on whether the next board is a terminal board (state), you should\n",
    "consider to compute the td_error in different ways, as it is shown in the\n",
    "Cognitive Systems lecture.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "- You can use **\"Tetris.get_simplified_features\"** function to get the\n",
    "simplified features of a board.<br>\n",
    "- You can use your implemented function **\"approximate_value\"** above to\n",
    "compute the state value.<br>\n",
    "- You can call **\"Tetris.check_terminal_board\"** to check if the next board\n",
    "(state) is terminated or not.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "\n",
    "def compute_td_error(board, reward, next_board, next_piece):\n",
    "    \"\"\"\n",
    "    Compute the TD error of the state value\n",
    "    :param board: Tetris board\n",
    "    :param reward: immediate reward\n",
    "    :param next_board: next Tetris board\n",
    "    :param next_piece: next Tetris piece\n",
    "    :return: td error\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "    current_state_value = approximate_value(\n",
    "        Tetris.get_simplified_features(board))\n",
    "\n",
    "    is_terminal = Tetris.check_terminal_board(next_board, next_piece)\n",
    "\n",
    "    if is_terminal:\n",
    "        td_error = reward - current_state_value\n",
    "    else:\n",
    "        next_state_value = approximate_value(\n",
    "            Tetris.get_simplified_features(next_board))\n",
    "        td_error = reward + gamma * next_state_value - current_state_value\n",
    "    ########    Your code ends here    ########\n",
    "\n",
    "    return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### e) Gradient Descent <br>\n",
    "Please finish the function **\"gradient_descent\"** below. This function will\n",
    "update the parameters **\"betas\"** used in the value function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hints:\n",
    "- You should compute the gradien in the case of linear function approximation (see your\n",
    " implementation in function **\"approximate_value\"**) by hand first in oder to implement it. Note that this is not standard \n",
    "- Because **\"betas\"** is a global variable (numpy vector), if you want to\n",
    "modify its values in a local function, you should add a declaration **\"global\n",
    "betas\"** before your modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "\n",
    "def gradient_descent(board, td_error):\n",
    "    \"\"\"\n",
    "    Perform the gradient descent\n",
    "    :param board: Tetris board\n",
    "    :param td_error: td_error\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "    # Since we used linear function approximation, here the gradient is\n",
    "    # just the feature function\n",
    "    features = Tetris.get_simplified_features(board)\n",
    "\n",
    "    global betas\n",
    "    betas += alpha * td_error * features\n",
    "    ########    Your code ends here    ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "def update_hyperparameters():\n",
    "    \"\"\"\n",
    "    Update learning and exploration rate\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Update exploration rate\n",
    "    global epsilon\n",
    "    if epsilon > 0.01:\n",
    "        epsilon *= 0.99\n",
    "    else:\n",
    "        epsilon = 0.01\n",
    "\n",
    "    # Update learning rate\n",
    "    global alpha\n",
    "    if alpha > 1e-5:\n",
    "        alpha *= 0.999\n",
    "    else:\n",
    "        alpha = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### f) TD Learning <br>\n",
    "Now, every single module of the TD learning has been prepared. It is time to\n",
    "finish the function **\"td_learning\"** ! This function maintains a\n",
    "proper temporal difference learning procedure and afterwards exports an action\n",
    "(a tuple of [rot, col]) to place the current piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow and Hints:\n",
    "1. You need to call **\"Tetris.get_possible_action\"** function to get all\n",
    "possible actions (given current board, current piece and next piece), together\n",
    "with their immediate rewards and simulated next boards. <br>\n",
    "2. With all the possible actions in hand, compute the action values by\n",
    "calling function **\"q_s_a\"**. <br>\n",
    "3. Sample one action (get its index) from all possible actions, by calling\n",
    "function **\"sample_action\"**. <br>\n",
    "4. Use this index to get this action from the list, as well as its immediate\n",
    "reward and the simulated next board. <br>\n",
    "5. Compute td_error. <br>\n",
    "6. Perform gradient descent to update parameters. <br>\n",
    "7. Call function **\"update_hyperparameters\"** to update the learning rate and\n",
    " exploration rate. <br>\n",
    "8. Return this action. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "\n",
    "def td_learning(board, piece, next_piece):\n",
    "    \"\"\"\n",
    "    Perform td learning, to get an action for the piece\n",
    "    :param board: current Tetris board\n",
    "    :param piece: current Tetris piece\n",
    "    :param next_piece: next Tetris piece\n",
    "    :return: action (rotation and lateral move)\n",
    "    \"\"\"\n",
    "    ########   Your code begins here   ########\n",
    "\n",
    "    # 1. Given board and piece as a state, get all possible actions with\n",
    "    # their values\n",
    "    action_list, reward_list, next_board_list \\\n",
    "        = Tetris.get_possible_action(board, piece, next_piece)\n",
    "\n",
    "    # 2. Get actions' values\n",
    "    action_value_list = list(map(q_s_a, reward_list, next_board_list))\n",
    "\n",
    "    # 3. Sample one action (its index) from possible actions\n",
    "    sampled_action_index = sample_action(action_list, action_value_list)\n",
    "\n",
    "    # 4. Get action and it's immediate reward (r) and next_board (s')\n",
    "    action = action_list[sampled_action_index]\n",
    "    reward = reward_list[sampled_action_index]\n",
    "    next_board = next_board_list[sampled_action_index]\n",
    "\n",
    "    # 5. Compute the td error\n",
    "    td_error = compute_td_error(board, reward, next_board, next_piece)\n",
    "\n",
    "    # 6. Gradient descent\n",
    "    gradient_descent(board, td_error)\n",
    "\n",
    "    # 7. Update hyperparameters, i.e. learning rate, exploration rate\n",
    "    update_hyperparameters()\n",
    "\n",
    "    ########    Your code ends here    ########\n",
    "\n",
    "    # 8. Return action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "def play_tetris():\n",
    "    \"\"\"\n",
    "    Main function to play Tetris\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    Tetris.show_text_screen('Tetris RL')\n",
    "\n",
    "    # Game loop for episodes\n",
    "    while True:\n",
    "        # You may increase the fps (Frame per second) to speed up your training\n",
    "        # But some pygame UI may behave weird, if fps is too high.\n",
    "        Tetris.run_episode(td_learning, fps=30)\n",
    "        Tetris.show_text_screen('Game Over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS BLOCK\n",
    "\n",
    "# Start playing Tetris\n",
    "play_tetris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task g) to task j) should be answered in the solution pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}