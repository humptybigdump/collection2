{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 5 Exercise 1\n",
    "\n",
    "## Lab Instructions\n",
    "All your answers for Exercise 1 should be written in this notebook. You shouldn't need to write or modify any other files.\n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "*This exercise was developed by Maximilian HÃ¼ttenrauch for the KIT Cognitive Systems Lecture, June 2020.\n",
    "  It is adapted from previous Robot Learning Lectures at TU Darmstadt, the Berkeley Deep RL Bootcamp\n",
    "  [Lab1](https://sites.google.com/view/deep-rl-bootcamp/home)\n",
    "  and Berkeley Deep RL Class\n",
    "   [HW2](https://github.com/berkeleydeeprlcourse/homework/blob/c1027d83cd542e67ebed982d44666e0d22a00141/hw2/HW2.ipynb)\n",
    "   [(license)](https://github.com/berkeleydeeprlcourse/homework/blob/master/LICENSE)*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ex1.mdp import MDP\n",
    "from ex1.misc import make_grader\n",
    "from ex1.misc import plot_v_pi\n",
    "from ex1.misc import expected_output_pi_infinite\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MDP\n",
    "\n",
    "The MDP class for the cleaning robot is already implemented and doesn't need to be changed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Actions\n",
    "DOWN = 0\n",
    "RIGHT = 1\n",
    "UP = 2\n",
    "LEFT = 3\n",
    "STAY = 4\n",
    "\n",
    "\n",
    "class RoboMDP(MDP):\n",
    "    O = -1e3  # Obstacles, walls or inaccessible places\n",
    "    D = 2  # Dirt\n",
    "    W = -1  # Water\n",
    "    C = -30  # Cat\n",
    "    T = 30  # Toy\n",
    "    E = 0  # Empty\n",
    "\n",
    "    grid_world = {\n",
    "        \"reward\": [\n",
    "            E, O, O, E, E, O, O, E, E, E,\n",
    "            E, E, E, E, D, O, E, E, D, E,\n",
    "            E, D, E, E, E, O, E, E, O, E,\n",
    "            O, O, O, O, E, O, E, O, O, O,\n",
    "            D, E, E, D, E, O, T, D, E, E,\n",
    "            E, O, D, D, E, O, W, E, E, E,\n",
    "            W, O, E, O, E, O, D, O, O, E,\n",
    "            W, E, E, O, D, E, E, O, D, E,\n",
    "            E, E, E, D, C, O, E, E, D, E\n",
    "        ],\n",
    "        \"map\": [\n",
    "            \" OO  OO   \",\n",
    "            \"    DO  D \",\n",
    "            \" D   O  O \",\n",
    "            \"OOOO O OOO\",\n",
    "            \"D  D OTD  \",\n",
    "            \" ODD OW   \",\n",
    "            \"WO O ODOO \",\n",
    "            \"W  OD  OD \",\n",
    "            \"   DCO  D \"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    n_actions = 5\n",
    "    world_dim = (9, 10)\n",
    "\n",
    "    # Transition probabilities. Each row corresponds to one action, i.e. choosing action down has a 70% chance of\n",
    "    # executing correctly, and 10% chance for executing right, left or stay.\n",
    "    # Usage: self.tp[actual_action, noisy_executed_action]\n",
    "\n",
    "    #             down, right, up, left, stay\n",
    "    tp = np.array([[0.7, 0.1, 0.0, 0.1, 0.1],  # down\n",
    "                  [0.1, 0.7, 0.1, 0.0, 0.1],  # right\n",
    "                  [0.0, 0.1, 0.7, 0.1, 0.1],  # up\n",
    "                  [0.1, 0.0, 0.1, 0.7, 0.1],  # left\n",
    "                  [0.0, 0.0, 0.0, 0.0, 1.0]])  # stay\n",
    "\n",
    "    def __init__(self, discount: float, stochastic: bool):\n",
    "        super(RoboMDP, self).__init__(discount, self.world_dim, self.n_actions, desc=self.grid_world['map'])\n",
    "\n",
    "        def inc(row: int, col: int, action: int):\n",
    "            if action == DOWN:  # move down\n",
    "                row = min(row+1, self.world_dim[0] - 1)\n",
    "            elif action == RIGHT:  # move right\n",
    "                col = min(col+1, self.world_dim[1] - 1)\n",
    "            elif action == UP:  # move up\n",
    "                row = max(row - 1, 0)\n",
    "            elif action == LEFT:  # move left\n",
    "                col = max(col - 1, 0)\n",
    "            elif action == STAY:  # stay\n",
    "                pass  # Not moving\n",
    "            return row, col\n",
    "\n",
    "        # We need to set the next state with its corresponding transition probability based on the current state and\n",
    "        # action. Have a look at mdp.py for the implementation of self.stp\n",
    "        for r in range(self.world_dim[0]):\n",
    "            for c in range(self.world_dim[1]):\n",
    "                s = self.to_s(r, c)\n",
    "                self.rewards[s] = self.grid_world[\"reward\"][s]\n",
    "                for a in range(self.n_actions):\n",
    "                    stp_list = self.stp[s, a]\n",
    "                    if stochastic:\n",
    "                        for noisy_a in range(self.n_actions):\n",
    "                            if self.tp[a, noisy_a] > 0:\n",
    "                                next_r, next_c = inc(r, c, noisy_a)\n",
    "                                next_state = self.to_s(next_r, next_c)\n",
    "                                stp_list.append((next_state, self.tp[a, noisy_a]))\n",
    "                    else:\n",
    "                        next_r, next_c = inc(r, c, a)\n",
    "                        next_state = self.to_s(next_r, next_c)\n",
    "                        stp_list.append((next_state, 1.))\n",
    "\n",
    "    # helper functions\n",
    "\n",
    "    def to_s(self, row, col):\n",
    "        \"\"\"given a row and column, returns a linear index corresponding to the world dimension\"\"\"\n",
    "        return row * self.world_dim[1] + col\n",
    "\n",
    "    def to_rc(self, s):\n",
    "        \"\"\"given a linear state, returns a row and column corresponding to the world dimension\"\"\"\n",
    "        return np.unravel_index(s, self.world_dim)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Iteration\n",
    "\n",
    "For this exercise, you'll implement policy iteration, which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $V_{(0)}^{\\pi_0}(s)=0$ for all $s$, $\\pi_0 \\leftarrow$ uniform, $k = 0$\n",
    "\n",
    "* For $i=1, 2, \\dots$\n",
    "\n",
    "    If $i>0$: Initialize $V_{(0)}^{\\pi_{i}}(s) = V_{(k)}^{\\pi_{i-1}}(s)$ (i.e. initialize the value function of the new policy with the converged value function of the old policy)\n",
    "\n",
    "    * For $k=0, 1, 2, \\dots$\n",
    "\n",
    "    $V_{(k+1)}^{\\pi_{i}}(s) = \\sum_a \\pi(a \\mid s)  \\Big( r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) \\, V_{(k)}^{\\pi_{i}}(s') \\Big)$\n",
    "\n",
    "    Until convergence\n",
    "\n",
    " $$\\pi_{(i+1)}(s) = \\begin{cases}1, & \\underset{a}{\\arg \\max} \\Big( r(s,a) + \\gamma \\sum_{s'} P(s' \\mid s,a) V^{\\pi_i}(s')\\Big)\\\\0 & else\\end{cases}$$\n",
    "\n",
    "Until convergence\n",
    "\n",
    "---\n",
    "\n",
    "Your code will return two lists: $[V_{(0)}, V_{(1)}, \\dots, V_{(n)}]$ and $[\\pi_{(0)}, \\pi_{(1)}, \\dots, \\pi_{(n-1)}]$\n",
    "\n",
    "To ensure that you get the same policies as the reference solution, choose the lower-index action to break ties in $\\arg \\max_a$. This is done automatically by np.argmax. This will only affect the \"# chg actions\" printout below--it won't affect the values computed.\n",
    "\n",
    "### Hints\n",
    "- We separately implement policy evaluation and policy improvement\n",
    "- The reward function can be accessed by $\\texttt{mdp.rewards[s]}$, the next states and their probability of reaching\n",
    " them (given a state $s$ and and action $a$) by $\\texttt{mdp.stp[s, a]}$\n",
    "- Use the functions $\\texttt{to_s}$ and $\\texttt{to_rc}$ provided by the MDP class to convert between linear states\n",
    " $s \\in [0, |\\mathcal{S}|]$ and row/column format, i.e. $\\texttt{mdp.to_rc(0)}$ returns $(0, 0)$, while\n",
    " $\\texttt{mdp.to_s(0, 0)}$ returns $0$\n",
    "- For your implementation, the identities $V^\\ast(s) = \\max_a Q^\\ast(s, a)$ and\n",
    " $\\pi^\\ast(s) = \\underset{a}{\\arg \\max} \\, Q^\\ast(s, a)$ may be helpful\n",
    "- Follwing numpy functions may be useful: $\\texttt{np.sum}$, $\\texttt{np.max}$, $\\texttt{np.argmax}$\n",
    "- Try using list comprehensions as this will shorten your code\n",
    "- We provide a printing function $\\texttt{make_grader}$ which compares the output of your implementation with the output\n",
    "of a correct implementation of value iteration. For each iteration, it prints the largest difference of the value function,\n",
    " the number of actions that have changed from the last iteration, as well as the value of state 0.\n",
    " If your solution is correct, it will print $\\textit{Test succeeded}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1 a) Implementing policy evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp, pi_prob, v=None):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    @param mdp: the mdp object\n",
    "    @param pi_prob: action probabilities\n",
    "    @param v: initial value function (optional)\n",
    "    Outputs:\n",
    "    @return: value function of the provided policy\n",
    "    \"\"\"\n",
    "\n",
    "    if v is None:\n",
    "        # Initialize value function for policy evaluation\n",
    "        v = np.zeros(shape=mdp.world_dim)\n",
    "    for pe_iter in range(10000):\n",
    "        # save current estimate\n",
    "        v_prev = np.copy(v)\n",
    "\n",
    "        # %%%%%%%% add your code here %%%%%%%%%%%\n",
    "\n",
    "        ...\n",
    "\n",
    "        # %%%%%%%% add your code here %%%%%%%%%%%\n",
    "\n",
    "        # run policy evaluation until convergence\n",
    "        if np.allclose(v, v_prev):\n",
    "            break\n",
    "\n",
    "    return v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1 b) Implementing policy improvement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy_improvement(mdp, v):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    @param mdp: the mdp object\n",
    "    @param v: value function of a policy\n",
    "    Outputs:\n",
    "    @return: new policy and distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize policy\n",
    "    pi = np.zeros(shape=mdp.world_dim)\n",
    "    pi_prob = np.ones(shape=mdp.world_dim + (mdp.n_actions,))\n",
    "\n",
    "    # %%%%%%%% add your code here %%%%%%%%%%%\n",
    "\n",
    "    ...\n",
    "\n",
    "    # %%%%%%%% add your code here %%%%%%%%%%%\n",
    "\n",
    "    return pi, pi_prob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, n_iter, grade_print=print):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    @param mdp: an MDP object\n",
    "    @param n_iter: number of PI iterations\n",
    "    @param grade_print: a printing function\n",
    "    Outputs\n",
    "    @return: (value_functions, policies)\n",
    "\n",
    "    len(value_functions) == n_iter+1 and len(policies) == n_iter\n",
    "    \"\"\"\n",
    "    vs = [np.zeros(shape=mdp.world_dim)]\n",
    "    pis = []  # contains the actual actions\n",
    "    pi_prob = np.ones(shape=mdp.world_dim + (mdp.n_actions, )) / mdp.n_actions  # contains the action probabilities for each state\n",
    "\n",
    "    grade_print(\"Iteration |  max|V-Vprev|  | # chg actions | V[0]    \")\n",
    "    grade_print(\"----------+----------------+---------------+---------\")\n",
    "    for pi_iter in range(n_iter):\n",
    "        pi_old = pis[-1] if len(pis) > 0 else None  # \\pi^{(it)} = Greedy[V^{(it-1)}]. Just used for printout\n",
    "        v_old = vs[-1]\n",
    "\n",
    "        # run policy evaluation\n",
    "        v_init = np.copy(v_old)\n",
    "        v = policy_evaluation(mdp, pi_prob, v_init)\n",
    "\n",
    "        # run policy improvement\n",
    "        pi, pi_prob = policy_improvement(mdp, v)\n",
    "\n",
    "        max_diff = np.abs(v - v_old).max()\n",
    "\n",
    "        n_chg_actions = 0 if pi_old is None else (pi != pi_old).sum()\n",
    "        grade_print(\"{:4d}      | {:12.5f}   |   {:4d}        | {:8.3f}\".format(pi_iter, max_diff, n_chg_actions, v[0, 0]))\n",
    "\n",
    "        vs.append(v)\n",
    "        pis.append(pi)\n",
    "\n",
    "        if max_diff < 1e-5:\n",
    "            break\n",
    "\n",
    "    return vs, pis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 c) Infinite Horizon Problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "robo_mdp_infinite = RoboMDP(discount=0.8, stochastic=False)\n",
    "v_infinite, pi_infinite = policy_iteration(robo_mdp_infinite, 100, make_grader(expected_output_pi_infinite))\n",
    "\n",
    "plot_v_pi(v_infinite[-1], pi_infinite[-1], robo_mdp_infinite)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Put your Text answers here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 d) Policy Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Put your Text answers here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}