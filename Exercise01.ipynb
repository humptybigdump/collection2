{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e90be3f-1e56-4de8-91e3-d6b62a5e75a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise worksheet no 1\n",
    "\n",
    "# An introduction to Python using global warming data\n",
    "\n",
    "*Machine learning in climate and environmental sciences, winter semester 2023, Jun.-Prof. Peer Nowack, peer.nowack@kit.edu*\n",
    "\n",
    "*Chair for AI in Climate and Environmental Sciences, https://ki-klima.iti.kit.edu*\n",
    "\n",
    "**Learning goals:** This first notebook will make you familiar with the Jupyter environment, \n",
    "and introduces a few key Python packages such as ``numpy``, ``pandas``, ``netCDF4``, `xarray`, ``matplotlib``, and ``scikit-learn``.\n",
    "\n",
    "For this, you will consider one of the most prominent examples of evidence for climate change: global warming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf871c5-fade-4e6f-a6c9-611d878d90cf",
   "metadata": {},
   "source": [
    "## Python and Jupyter fundamentals\n",
    "If you are not familiar with Python and Jupyter yet, you might want to learn more about Python\n",
    "and its basic syntax. There are a lof of free and well-written tutorials\n",
    " online, including:\n",
    "\n",
    "* http://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "* https://www.learnpython.org/\n",
    "* https://www.w3schools.com/python/\n",
    "* https://automatetheboringstuff.com/\n",
    "* https://codeacademy.com/catalog/language/python\n",
    "\n",
    "## Load Python and the \"ML-climate\" kernel\n",
    "\n",
    "If you are working on your own computer, now select the Tab \"Kernel\" above, and then select from the drop-down menu the entry \"Change Kernel\" and select \"ML-climate\". This option should exist for you if you followed the Anaconda3 and subsequent installation instructions provided on Ilias.\n",
    "\n",
    "You will now still miss a few Python packages needed for this exercise. On your own computer you can install these from within Jupyter using the `conda install` lines below. You won't need these in future executions of this notebook, so you can comment out these lines after their first successful execution. In the future, you can instead immediately select the ML-climate kernel.\n",
    "\n",
    "However, if you work on [Google Colab](https://colab.research.google.com/), comment out the cell below and uncomment the next one, which additionally installs the `netCDF4` package on your Colab account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa7a41-c393-43e7-a398-1f48e87cfb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### you can comment out these lines with \"#\" after you have run this Jupyter notebook for the first time\n",
    "### In later exercises, you might need to install additional packages to the ML-climate kernel/Conda environment, using similar commands\n",
    "### This is generally a method to install Python packages directly from within Jupyter notebooks. The packages will be added to the environment set as kernel.\n",
    "### Executing this cell might take a while and will create a long output. You can easily collapse that output, by clicking with your mouse left of the output cell.\n",
    "!conda install -c conda-forge nbgrader -y;\n",
    "!conda install numpy -y;\n",
    "!conda install pandas -y;\n",
    "!conda install scipy -y;\n",
    "!conda install matplotlib -y;\n",
    "!conda install netcdf4 -y;\n",
    "!conda install xarray -y;\n",
    "!conda install -c conda-forge cartopy -y;\n",
    "!conda install scikit-learn -y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd005960-681b-4950-9049-fe012401205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### on Google Colab\n",
    "# ! pip install netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a24e46a-0af8-4c9c-912a-6d77b7944d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As described in the Python tutorials above, we first need to load the required Python packages, some of which we have just installed\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mpl_dates\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d125c-c370-4eca-8c4a-7219578d2182",
   "metadata": {},
   "source": [
    "# Global warming\n",
    "\n",
    "- In this exercise, you will first load and visualize the historical global warming signal using `matplotlib` (Task 1).\n",
    "- Using historical changes in potential drivers of global temperature fluctuations and trends, you will then try to explain the signal with regression models. You will also explore the potential and limitations of a couple of first basic machine learning models (Task 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58fd9d3-7acb-4672-bd67-99d099d195a0",
   "metadata": {},
   "source": [
    "## Task 1: Download and visualize global warming data\n",
    "\n",
    "#### Background\n",
    "\n",
    "There are a many different data sources that climate scientists use to reconstruct Earth's surface temperature history. Why do we need to specify \"surface\"? That's because scientists are also interested in temperatures higher up in the atmosphere, whereas in this exercise we will focus on temperatures at the ground, where we live.\n",
    "\n",
    "Temperature measurements have been taken in a variety of ways as technology progressed, which has led to an increasing density and quality of data over time. For example, direct and indirect sources of information include [meteorological station measurements](https://cp.copernicus.org/articles/15/1345/2019/), [ice core](https://www.bas.ac.uk/data/our-data/publication/ice-cores-and-climate-change/), [tree ring](https://climate.nasa.gov/news/2540/tree-rings-provide-snapshots-of-earths-past-climate/#:~:text=The%20color%20and%20width%20of,year%20of%20the%20tree's%20life.), and [sediment](https://niwa.co.nz/climate/faq/how-do-we-determine-past-climate) data. Data sources for the modern climate record in particular include measurements by [ground-based remote sensing](https://www.imk-asf.kit.edu/english/201.php), [ships](https://www2.physics.ox.ac.uk/collaborations/osmosis/how-can-we-observe-these-flows-in-the-ocean), [profiling floats](https://www.aoml.noaa.gov/argo/), [weather balloons](https://www.imk-asf.kit.edu/english/ffb.php), and [aircraft measurements](https://www.imk-asf.kit.edu/english/1443.php). Since the late 1970s, [satellite measurements](https://climate.nasa.gov/faq/49/which-measurement-is-more-accurate-taking-earths-surface-temperature-from-the-ground-or-from-space/) have become central, although they do come with their own challenges. In the end, a variety of sources are typically combined to provide the most reliable estimates of Earth's temperature record.\n",
    "\n",
    "In this exercise, we will look at a surface temperature data product provided by [NASA](https://climate.nasa.gov/vital-signs/global-temperature/), which we can download using Python's ``requests`` package. We here define another function ``fetch_data()`` that builds on ``requests``, and we will use this function repeatedly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474a25d0-bcd7-4645-acef-f3fac1317fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url,fname):\n",
    "    file = fname\n",
    "    if not os.path.exists(file):\n",
    "        print(\"Downloading data ...\")\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(requests.get(url).content)\n",
    "        print(\"Download data complete.\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98c13f-a4dc-4325-8bc6-2e19aac02377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### this might take a minute\n",
    "data_url = \"https://data.giss.nasa.gov/gistemp/graphs/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.txt\"\n",
    "data_file = \"nasa.txt\"\n",
    "nasa_file = fetch_data(data_url,data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998ee4d-71c5-4b18-9dfb-06dd97248e90",
   "metadata": {},
   "source": [
    "The file is in text format. To load and later process the data, we can use the data library ``pandas``.\n",
    "\n",
    "Using ``pandas``returns the ``DataFrame`` object, which gives access to many highly optimized data processing, analysis, and visualization routines.\n",
    "\n",
    "We print the first five rows of the ``DataFrame`` with ``.head()``.\n",
    "\n",
    "Feel free to experiment what happens if you change the options in the `.read_csv()` reader function and maybe also have a look at the original txt file, which should have been downloaded to your exercise folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a47f4-1a1c-4683-adad-61cfdc5fc33f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nasa = pd.read_csv(nasa_file, header=3,delim_whitespace=True,names=[\"Year\", \"T\", \"T-smoothed\"])\n",
    "nasa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6905b-68ea-4b6b-b318-7c6d608667a4",
   "metadata": {},
   "source": [
    "The ``DataFrame`` object allows access via index and columns.\n",
    "Basic Python operators ``[]`` and ``.`` are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f58baa-09c5-46ae-83b0-55065fa85614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Index test: ', nasa.index[3],\"\\n\")\n",
    "print('Access by index example:', nasa.iloc[3],sep=\"\\n\")\n",
    "print()\n",
    "print('Access by column names with list:',nasa[['Year','T']].head(),sep=\"\\n\")\n",
    "print()\n",
    "print('Access by column name:',nasa['T-smoothed'].head(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256ec70-898c-478d-9056-f16350398407",
   "metadata": {},
   "source": [
    "### First visualization (matplotlib, pandas)\n",
    "Next we visualize the temperature data from NASA using ``matplotlib``, a [package for data visualization](https://matplotlib.org/stable/tutorials/index) in Python. You can find a quick tutorial [here](https://matplotlib.org/stable/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py).\n",
    "\n",
    "As can be inferred from the first dimension ``'Year'`` in the ``DataFrame`` object, combined with the info on the [NASA website](https://climate.nasa.gov/vital-signs/global-temperature/), \n",
    "the column ``T`` provides annually and globally averaged temperatures across the years 1880 to 2022 at Earth's surface, relative to the 1951-1980 temperature average.\n",
    "\n",
    "To begin with, we will plot both the annual mean and smoothed data (an approx. 5-year rolling average) provided, just as on the NASA website itself. The rolling average is helpful to smooth out \n",
    "short-term variability to make underlying long-term trends more visible (cf. climate change), as you will see after evaluating the next cell.\n",
    "\n",
    "Now it's time for you to answer the first questions. Wherever possible, built-in functionality of libraries such as `numpy` and `pandas` should be preferred over Python statements as these libraries are heavily optimized and run much faster than raw Python code in general.\n",
    "\n",
    "**Task 1.1** Calculate a 20-year centred rolling-mean version of the NASA data and then add this smoothed data as another line to the figure object implemented in the cell below. Show the new line in blue colour.\n",
    "You might want to use the ``.rolling()`` and ``.mean()`` functions of ``pandas``.\n",
    "\n",
    "*Hint*: use Google search and the [`pandas` documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) if you want to find more guidance on these functions.\n",
    "\n",
    "**Task 1.2** What is the resulting 20-year centred rolling-mean value in year 2010? Pass this value to the `rm_value_2010`variable below.\n",
    "\n",
    "*Note*: some of your answers in this notebook will be autograded, others will be manually graded after submission. Some of the `assert()` statements at the very end of this notebook may help you find out if your answer might be wrong or missing, but there will be additional hidden tests that will be carried out on your submitted notebook (after upload to Ilias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e2577-a568-40bc-bc04-06c8abf98d76",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9afab7c8291babfd13fed3307b6bb4b0",
     "grade": true,
     "grade_id": "rolling_mean_plot",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(nasa['Year'],nasa['T'],s=50,color='white',marker=None,edgecolors='k')\n",
    "ax.plot(nasa['Year'],nasa['T'],label='Annual mean',color='k')\n",
    "ax.plot(nasa['Year'],nasa['T-smoothed'],label='5-year smoothed',color='r',linewidth=2)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax.set_ylabel('Temperature relative to 1951-1980 (K)',size=14)\n",
    "ax.set_xlabel('Year',size=14)\n",
    "plt.legend(loc='upper left',fontsize=14)\n",
    "\n",
    "N=20\n",
    "time_series_20year_rm = None\n",
    "rm_value_2010 = None\n",
    "\n",
    "###\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "###\n",
    "\n",
    "plt.legend(loc='upper left',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c30df-5a45-4181-84a6-ccde95637e64",
   "metadata": {},
   "source": [
    "**Task 1.3** Why is no 20-year centred rolling mean value calculated for e.g. the year 2019?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63d239-3944-488e-b8fc-d219fa6f3656",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d9ac2835bd2eecb007d5c43d505e4ad",
     "grade": true,
     "grade_id": "why_no_rolling_mean_2019",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_rolling_mean = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a98f42-7ca4-48d9-8695-dee653fd8cdf",
   "metadata": {},
   "source": [
    "### Adding higher frequency data to the visualization (netCDF4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e54078-ea44-488c-8299-6161635277b4",
   "metadata": {},
   "source": [
    "The data you visualized so far was annual mean data. However, in reality we are often interested in higher-frequency temperature variability, for example on a monthly basis. Typically, higher temporal resolution means that you also see a larger range of values and more weather \"noise\".\n",
    "\n",
    "To illustrate this, we have prepared another version of the NASA dataset, which provides monthly-mean data in the `netCDF` file format. \n",
    "\n",
    "The `NetCDF` format is one of the standard file formats in climate science (file ending `.nc`). The format allows for easy structuring of typical dimensions of climate data (in particular: time, latitude, longitude, altitude), see also this [tutorial](https://colab.research.google.com/github/astg606/py_materials/blob/master/science_data_format/introduction_netcdf4.ipynb). Since we here look at global mean surface temperature data again, this means that only the time dimension is expanded/active (because there is no need to represent longitudes, latitudes, or various altitudes - just one time series stretched out over all months from 1880 onwards). However, you will encounter regional data gridded by latitude and longitude in Worksheet 2.\n",
    "\n",
    "Now, let's first load the monthly data and print the variables contained in the file - these are *time*, *lat*, *lon*, and *tempanomaly*. They are printed  together with other information, such as their shape and data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cf130-c2a3-45ac-91a7-30f6f389729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_mm_file = netCDF4.Dataset('./data/gistemp_monthly_mean.nc')\n",
    "nasa_mm_file.variables\n",
    "### On Google Colab: comment out the two lines above and instead uncomment and execute the following:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload() ### in the pop-up window \"Choose Files\" the gistemp_monthly_mean.nc file from the /data/ directory in your exercise folder\n",
    "# nasa_mm_file = netCDF4.Dataset('gistemp_monthly_mean.nc')\n",
    "# nasa_mm_file.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37793044-4cf3-43b5-a714-99eb1324cbe7",
   "metadata": {},
   "source": [
    "The `netCDF4` package can handle both netCDF4 and the older version netCDF3 data. \n",
    "\n",
    "You can extract data again using the Python operator `[]` combined with variable names in string format.\n",
    "\n",
    "Let's extract the variable `tempanomaly` which is the same as in the global dataset, just now at higher temporal resolution. This is the quantity we want to plot. As implied by the output above, this variable has a shape of (1725, 1, 1) where the latter two dimensions are pseudo-dimensions, which we can remove with the Python indexing [:,0,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad377ca-7760-4f32-a83b-938a832c4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_mm = nasa_mm_file['tempanomaly'][:,0,0]\n",
    "print(nasa_mm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06929205-769f-45d3-9e22-24878e08b756",
   "metadata": {},
   "source": [
    "#### Time dimension\n",
    "\n",
    "Since this monthly dataset is read in from a `netCDF` file, there is not a table-like dataframe format as in `pandas`. Instead, there is an extra variable `time` that we can access.\n",
    "\n",
    "At first, the time dimension can be confusing. Printing the variable info reveals a rather strange range of values [29233, 81706] which can be explained by their unit \"days since 1800-01-01\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6170b05-1ee6-4804-b266-084d92e5969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_nc = nasa_mm_file['time'][:]\n",
    "print(nasa_mm_file['time'].units)\n",
    "print('Time in original format:\\n',time_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf56f4-f16a-4350-a8e1-49906e2a40f9",
   "metadata": {},
   "source": [
    "Therefore, it is always good to know in advance which time range and temporal averaging you expect for a given dataset. You can use the `netCDF4.num2date()` function to convert the time axis into Gregorian calendar format, which is more human-readable. This reveals the monthly frequency of the data, starting in January 1880, with the timestamp centred on the 15th day of each month.\n",
    "\n",
    "It is now further possible to see that this dataset contains even more recent data than for the year 2022, up to September 2023!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef269ec1-85e9-4c87-8576-c86ca2e44653",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_mm = netCDF4.num2date(time_nc,nasa_mm_file.variables['time'].units)\n",
    "print('Time in Gregorian calendar format:\\n',time_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e54b14e-bd3f-4258-ac9a-238c4b7ec1bc",
   "metadata": {},
   "source": [
    "**Task 1.4** Create a new plot - using the code from Tasks 1.1 and 1.2 as the base, but only showing a scatter and a line plot of the annual mean NASA data in black (not smoothed). In addition, show the new monthly data as a *red* line on top, with a label for the figure legend called 'Monthly mean'. For the monthly mean data choose `linewidth=1` and set the line transparency parameter `alpha` of the `.plot()` function to 0.5.\n",
    "\n",
    "For the monthly time dimension, we have prepared a `numpy` (imported as alias `np`) array for you. The `numpy` package provides a standard array format in Python, with many practical functions for creating regular arrays (e.g. `np.arange()`, `np.linspace()`, `np.empty()`, `np.zeros()`, `np.random()`) and for array processing (e.g. `np.mean()`, `np.reshape()`, `np.std()`). For `numpy` array indexing and more, see for example the quickstart tutorial [here](https://numpy.org/doc/stable/user/quickstart.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28d285-55ce-4453-bc1c-72fe07960a93",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "273e5698de45ab45c8621fdb2f63a091",
     "grade": true,
     "grade_id": "plot_monthly_mean_data",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "time_monthly = np.arange(1880+1/24,2023+1/24+8*1/12,1/12)\n",
    "print(time_monthly.shape)\n",
    "print(time_monthly)\n",
    "fig_mm, ax_mm = (None, None)\n",
    "###\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fc57c-f4a1-46a2-866b-f92119a6b490",
   "metadata": {},
   "source": [
    "**Task 1.5** Using `numpy`, calculate the standard deviations of both the annual and monthly mean time series, and hand the values over for evaluation to the already initialized variables `std_am` and `std_mm`, respectively. In addition, use `numpy` to find the value of the maximum monthly temperature anomaly (`T_max_mm`), as well as the corresponding year (`year_max`, as four-digit integer) and month (`month_max`). For the latter select the short-hand string notation from the defined list of months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfbdc0-f696-4cfe-9a99-5dbc98370e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_am = None\n",
    "std_mm = None\n",
    "T_max_mm = None\n",
    "month_max = None\n",
    "year_max = None\n",
    "list_of_months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb8806-2934-437d-beb2-75831106d950",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5211e7636ed49b74564573c2aaf165dc",
     "grade": true,
     "grade_id": "year_max_temp",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694eadc5-9dce-42dd-b95c-24e7d8b8c30f",
   "metadata": {},
   "source": [
    "## Task 2: Predicting the temperature record\n",
    "\n",
    "#### Background\n",
    "\n",
    "Task 1 demonstrated that global surface temperatures since 1880 have seen significant yearly and monthly fluctuations, on top of an underlying positive trend. The positive trend has mainly been [attributed](https://www.ipcc.ch/site/assets/uploads/2018/02/WG1AR5_Chapter10_FINAL.pdf) to increasing atmospheric greenhouse gas concentrations, in particular carbon dioxide (CO$_2$). The shorter-term variability, in turn, is driven by a variety of weather and climate processes. Many of these processes are commonly subsumed in definitions of so-called \"modes of climate variability\", most famously the [El Niño Southern Oscillation (ENSO)](https://www.climate.gov/news-features/blogs/enso/what-el-ni%C3%B1o%E2%80%93southern-oscillation-enso-nutshell). The [North Atlantic Oscillation (NAO)](https://www.ncei.noaa.gov/access/monitoring/nao/), another climate mode, is especially relevant for European weather. These modes of climate variability are typically characterized in the form of indices, which are based on, e.g., sea surface temperatures across a certain region, or surface pressure gradients reflective of the state of the atmospheric circulation.\n",
    "\n",
    "You will hear more about these mechanisms, including ENSO, in future lectures and exercises. Here, we will instead conduct a few first regression analyses to infer how well the historical changes in CO$_2$, the ENSO, and the NAO, but also in solar variability (measured through a [sunspot index](https://publi2-as.oma.be/record/247/files/paper_revised.pdf)), can explain and predict the historical temperature signal. Please be aware that such a regression analyses are at best indicative of the true causal drivers of change, which have been established with much more complex process-based models (\"correlation does not imply causation\").\n",
    "\n",
    "### Loading and preparing the data\n",
    "\n",
    "We have prepared the download and processing of the 1880-2022 time series of the CO$_2$, ENSO, NAO, and sunspot index data for you. Feel free to explore if you can follow those steps; note for example that the function `.values()` gives access to a `numpy` array representation of the columns of a ``pd.DataFrame()``. We also for the first time use the package `xarray`, which works on top of `numpy` and provides additional functionality for e.g. time-dependent data and for working with `netCDF` files. You will learn more about `xarray` in Worksheet 2.\n",
    "\n",
    "After downloading and pre-processing the data, we also have a first look at those time series using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372bb9b5-bd7b-4419-92c6-cd9077b16aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download and process CO2 data. This is from Input4MIPs, which is also used as input for the 6th international Climate Model Intercomparison Project (CMIP6). \n",
    "### We need to combine two scenarios.\n",
    "co2_am_file_1850_2014 = fetch_data(\"https://files.isimip.org/ISIMIP3b/InputData/climate/atmosphere_composition/co2/historical/co2_historical_annual_1850_2014.txt\",\"co2_historical_annual_1850_2014.txt\")\n",
    "co2_am_1850_2014 = pd.read_csv(co2_am_file_1850_2014,header=None,delim_whitespace=True,names=['Year','CO2 (ppmv)'])\n",
    "co2_am_file_2015_2022 = fetch_data(\"https://files.isimip.org/ISIMIP3b/InputData/climate/atmosphere_composition/co2/ssp126/co2_ssp126_annual_2015_2100.txt\",\"co2_ssp126_annual_2015_2100.txt\")\n",
    "co2_am_2015_2022 = pd.read_csv(co2_am_file_2015_2022,header=None,delim_whitespace=True,names=['Year','CO2 (ppmv)'])\n",
    "co2_am_1880_2022 = pd.concat([co2_am_1850_2014[co2_am_1850_2014['Year'] >= 1880],co2_am_2015_2022[co2_am_2015_2022['Year'] < 2024]]).reset_index(drop=True)\n",
    "co2_am_1880_2022.Year = pd.to_datetime(co2_am_1880_2022.Year,format='%Y')\n",
    "co2_am_1880_2022 = co2_am_1880_2022.to_xarray()\n",
    "co2_am_1880_2022 = co2_am_1880_2022.set_coords('Year').swap_dims({'index': 'Year'})\n",
    "### the CO2 data is only available at yearly temporal resolution, so we upsample the data according to the nearest timestamp to monthly frequency\n",
    "co2_mm_1880_2022 = co2_am_1880_2022.resample(Year='1M').nearest().to_dataframe()['CO2 (ppmv)'][:-1]\n",
    "co2_mm_1880_2022 = np.array(co2_mm_1880_2022.values)[:,np.newaxis]\n",
    "### Download the monthly NINO34 index (one of the indices representing ENSO).\n",
    "### The data first comes in monthly columns for each year, and it orginally starts 10 years earlier\n",
    "nino34_raw_file = fetch_data(\"https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/nino34.long.anom.data\",\"nino34.long.anom.data\")\n",
    "nino34_raw_mm = pd.read_csv(nino34_raw_file,header=1,delim_whitespace=True,skipfooter=8,engine='python',names=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "nino34_raw_mm_1880_2022 = nino34_raw_mm.iloc[9:]\n",
    "nino34_mm_1880_2022 = nino34_raw_mm_1880_2022.values.flatten()[:,np.newaxis]\n",
    "### Now download and process the NAO index, using the definition following https://crudata.uea.ac.uk/cru/data/nao/\n",
    "### More on the North Atlantic Oscillation (NAO) at https://crudata.uea.ac.uk/cru/data/nao/; again there is more data than we actually need here\n",
    "nao_raw_file = fetch_data(\"https://crudata.uea.ac.uk/cru/data/nao/nao.dat\",\"nao.dat\")\n",
    "nao_raw_mm = pd.read_csv(nao_raw_file,header=1,delim_whitespace=True,names=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec','Annual mean'])\n",
    "nao_raw_mm = nao_raw_mm.drop(columns='Annual mean')\n",
    "nao_raw_mm_1880_2022 = nao_raw_mm.iloc[57:-1]\n",
    "nao_mm_1880_2022 = nao_raw_mm_1880_2022.values.flatten()[:,np.newaxis]\n",
    "### and, finally, the sunspot index\n",
    "sunspot_raw_file = fetch_data(\"https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/sunspot.long.data\",\"sunspot.long.data\")\n",
    "sunspot_raw_mm = pd.read_csv(sunspot_raw_file,header=1,delim_whitespace=True,skipfooter=9,engine='python',names=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "sunspot_raw_mm_1880_2022 = sunspot_raw_mm.iloc[130:]\n",
    "sunspot_mm_1880_2022 = sunspot_raw_mm_1880_2022.values.flatten()[:,np.newaxis]\n",
    "### we won't look into 2023 data here, because not all indices were available for 2023 yet...\n",
    "### we can use Python indexing to cut off the last nine months (January to September 2023) from the time array\n",
    "time_monthly_1880_2022 = time_monthly[:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b61294-9dbc-497a-bce5-9a0daa706ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now visualize the four time series using matplotlib\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "plt.tight_layout(w_pad=1.7)\n",
    "axs[0,0].plot(time_monthly_1880_2022,co2_mm_1880_2022,linewidth=2)\n",
    "axs[0,0].set_ylabel('[CO$_2$] (ppmv)',fontweight='bold')\n",
    "axs[0,1].plot(time_monthly_1880_2022,sunspot_mm_1880_2022,linewidth=1)\n",
    "axs[0,1].set_ylabel('Sunspot index',fontweight='bold')\n",
    "axs[1,0].plot(time_monthly_1880_2022,nino34_mm_1880_2022,linewidth=1)\n",
    "axs[1,0].set_ylabel('Nino3.4 index',fontweight='bold')\n",
    "axs[1,0].set_xlabel('Year',fontweight='bold')\n",
    "axs[1,1].plot(time_monthly_1880_2022,nao_mm_1880_2022,linewidth=1)\n",
    "axs[1,1].set_ylabel('NAO index',fontweight='bold')\n",
    "axs[1,1].set_xlabel('Year',fontweight='bold')\n",
    "axs[1,1].set_yticks(np.arange(-6,8,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27255788-692b-4801-9272-225491950fe2",
   "metadata": {},
   "source": [
    "### First machine learning regressions\n",
    "\n",
    "Clearly, the four variables/indices show different trends and periodicities, which might help to explain different timescales of variability in the NASA monthly temperature data.\n",
    "\n",
    "So, let's formulate a regression problem in which we try to predict global mean surface temperature, *T*, from the evolution of the concentration of atmospheric carbon dioxide *[CO$_2$]*, the sunspot index *S*, the ENSO index *Nino3.4*, and the *NAO* index. As will often be the case in machine learning, we thus aim to learn a predictive function *f*\n",
    "over time *t*\n",
    "$$\n",
    "T (t) = f\\bigl([\\text{CO}_2], \\text{S}, \\text{Nino3.4}, \\text{NAO}\\bigr)(t) + \\epsilon\n",
    "$$\n",
    "of the general form\n",
    "$$\n",
    "y(t) = f(X(t)) + \\epsilon\n",
    "$$\n",
    "expressing the dependency of the predictand *y* on multiple predictors *X*.\n",
    "\n",
    "To fit this function, we will next compare one-dimensional linear regression (based on CO$_2$ as the only predictor), multiple linear regression, random forest regression, and a small neural network. This will be your first encounter with the `scikit-learn` package in Python, which provides access to a large number of easy-to-use and efficient machine learning functions, see also its documentation [here](https://scikit-learn.org/stable/).\n",
    "\n",
    "So, first let's concatenate all predictors into a predictor `numpy` array `X_all` of shape (number months, number predictor dimensions) and re-write the NASA monthly temperature data into an array *y*, removing the months for the year 2023. \n",
    "\n",
    "Let's also make sure the dimensions match in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fc15d-16b0-42cd-a502-d2e816be9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.concatenate((co2_mm_1880_2022[:,:],sunspot_mm_1880_2022[:,:],nino34_mm_1880_2022[:,:],nao_mm_1880_2022[:,:]),axis=1)\n",
    "print(X_all.shape)\n",
    "y = nasa_mm[:-9]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb63c9d-b217-449e-aac6-1fc9d559c15e",
   "metadata": {},
   "source": [
    "Now we fit multiple linear regression, random forest regression, and a small neural network function on this *X* and *y* data. The corresponding regressor functions from `scikit-learn` were imported at the beginning of this notebook. For their documentation see [LinearRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and [MLPRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html). The random forests method is a non-linear regression algorithm that thanks to `scikit-learn` is easy to implement yet typically a powerful predictive tool. You will learn much more about random forests when we discuss ensemble methods. The MLP in the neural network regressor object stands for Multi-Layer Perceptron and allows for the straightforward creation and training of feedforward neural networks. For now, \n",
    "there is no need for you to understand the details behind this code yet. \n",
    "\n",
    "However, from the next cell, you will already be able to deduce the general logic of fitting regression functions with `scikit-learn`:\n",
    "\n",
    "1. A regression object is `.fit()` on a pair of `X` and `y` data. Note that the predictor array `X` must have a second dimension, even if it is just a single dummy dimension which would not be required for a single time series, e.g. (1000,1) or (3000,1). If `X` for example had shape (1000,) you could use the `np.newaxis` command to add a dummy dimension, i.e. `X[:,np.newaxis]`. We also used `np.newaxis` in the preparation of the predictor data above. \n",
    "2. Once the regression object has been fit, you can use it to `.predict()` new y-values.\n",
    "\n",
    "There are many useful functions to evaluate the prediction skill. Here we will measure prediction error with the already imported `mean_squared_error()` function from `sklearn.metrics`.\n",
    "\n",
    "**Task 2.1** Using the same type of `LinearRegression()` object as for the Multiple Linear Regression (MLR), fit a fourth regression function that only includes the CO$_2$ evolution as the linear predictor. Just as for the other three regression functions below calculate and print the mean squared error on its own training data.\n",
    "\n",
    "*Hint*: CO$_2$ is the first predictor in `X_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8201e-53b0-423b-bd38-18a52e4d7778",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6699603dfc57da6bf590ad4ddcb9ef1a",
     "grade": true,
     "grade_id": "implement_linear_regression",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Multiple linear regression\n",
    "MLR = LinearRegression()\n",
    "MLR.fit(X_all,y)\n",
    "y_pred_MLR = MLR.predict(X_all)\n",
    "mse_MLR = mean_squared_error(y,y_pred_MLR)\n",
    "print('MLR error on training data: ',round(mse_MLR,5))\n",
    "\n",
    "### Random forest regression\n",
    "RFR = RandomForestRegressor(random_state=10)\n",
    "RFR.fit(X_all,y)\n",
    "y_pred_RFR = RFR.predict(X_all)\n",
    "mse_RFR = mean_squared_error(y,y_pred_RFR)\n",
    "print('RFR error on training data: ',round(mse_RFR,5))\n",
    "lr_co2 = None\n",
    "\n",
    "### Neural network\n",
    "NN = MLPRegressor(hidden_layer_sizes=(100,100), random_state=1, max_iter=2000,activation='tanh',shuffle=False,alpha=0.0,solver='lbfgs')\n",
    "NN.fit(X_all, y)\n",
    "y_pred_NN = NN.predict(X_all)\n",
    "mse_NN = mean_squared_error(y,y_pred_NN)\n",
    "print('NN error on training data: ',round(mse_NN,5))\n",
    "\n",
    "### One-dimensional linear regression\n",
    "LR = LinearRegression()\n",
    "mse_LR = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6203b-521e-448b-8af9-4924794a9b05",
   "metadata": {},
   "source": [
    "**Task 2.2** Which of the four regression models fits the training data best, which one worst? Select the fitting strings from the list `models` and assign them to the two variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0784e-8e0c-423e-8314-0fee5fc73eba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9b86b8e5a4e758f96390ee7143f2716",
     "grade": true,
     "grade_id": "best_worst_model_training",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "models = ['MLR', 'RFR', 'NN', 'LR']\n",
    "best_fitting_model_training = None\n",
    "worst_fitting_model_training = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697f109-6002-4c48-9a5a-bbc9df42e929",
   "metadata": {},
   "source": [
    "**Task 2.3** Now create a `matplotlib` time series plot showing the true NASA 1880-2022 monthly temperature data, \n",
    "as well as the four predicted time series by the regression functions (free style, your design). However, make sure to add a figure legend with informative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07826a89-4c5e-4a51-9387-ef93ab69d6db",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ac38d2c0612238d5f2a39f219b9cbb7",
     "grade": true,
     "grade_id": "Training_predictions_vis",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ef21c-72e9-48ac-a825-271c2d3084c0",
   "metadata": {},
   "source": [
    "**Task 2.4** What is specific to the predictions of the one-dimensional linear regression you carried out, compared to the predictions by the other three regression models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c62cd-6865-4129-a5b4-4dbcea260774",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb2aa3954796260cb15e434323def26b",
     "grade": true,
     "grade_id": "peculiar_curve_CO2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_LR = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e762a31-8b84-4970-bc01-b8b84327b39a",
   "metadata": {},
   "source": [
    "### Training vs. Test data\n",
    "\n",
    "Intuitively, you might have wondered if predicting on the same data as used for fitting is such a sensible thing to do!\n",
    "\n",
    "Indeed, predicting on what statisticians refer to as *training data* is of little value. Especially highly flexible machine learning functions can always fit the training data very well. The key question is if the function learned has ***generalizable*** predictive skill on y, i.e. if confronted with new inputs 'X' that were not part of the training data.\n",
    "\n",
    "To test generalization skill, machine learners usually split the available data into representative subsets of *training* and *test* data. The algorithm, in various configurations, is then trained first, but its true predictive skill can only be measured on the held-out test data. You will learn more about this idea in the lectures and in future worksheets.\n",
    "\n",
    "Here, let's run a few practical tests first to see if the functions that fitted the data above best, would also perform best on held-out test data.\n",
    "\n",
    "Since we cannot produce a second observational record, we need to start again from scratch. However, this time around we will split the predictor and predictand data into training and test sets, i.e. `X_train` and `X_test`, as well as `y_train` and `y_test`. Specifically, we will hold out test data for the 1980-2000 period and use all remaining years for training. Afterwards, we will again compare the mean squared error, but this time for predictions on the 1980-2000 data not seen during training.\n",
    "\n",
    "Let's see what this will bring! Maybe a surprise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170f0de-09e7-48aa-b90e-fca104e187c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train on 1880-1979, plus 2001-2022\n",
    "### then predict on 1980-2000 period\n",
    "i=100\n",
    "j=121\n",
    "idx_test = np.arange(i*12,j*12)\n",
    "print(time_monthly_1880_2022[idx_test][0:3])\n",
    "print(time_monthly_1880_2022[idx_test][-3:])\n",
    "### define training and test data using these indices\n",
    "X_train = np.delete(X_all[:,:],idx_test,axis=0)\n",
    "y_train = np.delete(y,idx_test)\n",
    "X_test = X_all[i*12:j*12,:]\n",
    "y_test = y[i*12:j*12]\n",
    "### Multiple linear regression\n",
    "MLR_surprise = LinearRegression()\n",
    "MLR_surprise.fit(X_train,y_train)\n",
    "y_hat_MLR_surprise = MLR_surprise.predict(X_test)\n",
    "mse_MLR_surprise = mean_squared_error(y_test, y_hat_MLR_surprise)\n",
    "print('MLR error on test data: ',round(mse_MLR_surprise,5))\n",
    "      \n",
    "### Random forest regression\n",
    "RFR_surprise = RandomForestRegressor(random_state=10)\n",
    "RFR_surprise.fit(X_train,y_train)\n",
    "y_hat_RFR_surprise = RFR_surprise.predict(X_test)\n",
    "mse_RFR_surprise = mean_squared_error(y_test,y_hat_RFR_surprise)\n",
    "print('RFR error on test data: ',round(mse_RFR_surprise,5))\n",
    "\n",
    "### Neural network\n",
    "NN_surprise = MLPRegressor(hidden_layer_sizes=(100,100), random_state=1, max_iter=2000, activation='tanh',shuffle=False,alpha=0.0,solver='lbfgs')\n",
    "NN_surprise.fit(X_train, y_train)\n",
    "y_hat_NN_surprise = NN_surprise.predict(X_test)\n",
    "mse_NN_surprise = mean_squared_error(y_test,y_hat_NN_surprise)\n",
    "print('NN error on test data: ',round(mse_NN_surprise,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661abc5b-1d28-4ea9-ab57-7c27cc6626ba",
   "metadata": {},
   "source": [
    "**Task 2.5** What is the best-performing model on the 1980-2000 test data, which one performs worst? Select the fitting string from the list `models` and write it to the two variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f7fcb-4bd3-453e-9c90-19283c2088bb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df945f83b7d510babe3d700ca829322e",
     "grade": true,
     "grade_id": "test_case",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "models = ['MLR', 'RFR', 'NN']\n",
    "best_fitting_model_test = None\n",
    "worst_fitting_model_test = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3a6cc-27c6-4c34-8171-13535b59b301",
   "metadata": {},
   "source": [
    "**Task 2.6** How do you explain this result? What would be your strategy, across all three models, to maximize your predictive skill on the test data? How could you achieve this goal?\n",
    "\n",
    "*Hint*: see what happens if you change some of the parameters in the machine learning regression functions. If necessary look up additional parameters in their Python documentations. Does changing the parameters affect the results a lot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b1dad-a823-43e4-a02e-0af946a095c8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31621136e35ddcff44068dbb04061389",
     "grade": true,
     "grade_id": "task2p6_test_explanation",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_testdata= \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370fd09-f997-4b6b-bd8b-8c33786ac743",
   "metadata": {},
   "source": [
    "### Extrapolation\n",
    "\n",
    "Finally, let's consider another peculiarity about using machine learning in climate science: the system we study is non-stationary.\n",
    "\n",
    "**Task 2.7** Write your own code (of course, your are allowed to also re-use code from above) to train MLR, RFR, and NN models on data for\n",
    "the years 1880-2000 and then predict the temperature anomalies for the remaining years 2001-2022. \n",
    "\n",
    "**Task 2.8** Visualize the three predicted time series for the years 2001-2022, similar to the figure produced in Task 2.3, next to the actual NASA temperature data for that period.\n",
    "\n",
    "**Task 2.9** What do you observe? How do you explain these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71cbb7c-181f-4b12-b2ed-aa9468540116",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e12e8a4d6f8ec5284c16bc12397ca572",
     "grade": true,
     "grade_id": "extrapolation_regressions",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Task 2.7 - write your own code\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89ad71-96a6-40ff-a0d7-d1c41f4cb226",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ec0fa625669a5a2f2cbc11de56fe9e3",
     "grade": true,
     "grade_id": "extrapolation_visualization",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Task 2.8 - write your own matplotlib code\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8050ca-3725-4b35-8eae-21018ea732b9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6f960d175fcac152374d1d5b3a0e06c",
     "grade": true,
     "grade_id": "cell-12b6df3c8e822fce",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Task 2.9 - what do you observe and how do you explain it\n",
    "answer_extrapolation = \"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442e369-974f-4d32-870c-56e90c9fe8a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b01584649ebd423f7c3c13c3a0bb5443",
     "grade": true,
     "grade_id": "assert_rolling_mean_2010",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### In the following, the public part of the automized tests\n",
    "assert time_series_20year_rm is not None\n",
    "assert rm_value_2010 is not None\n",
    "assert len(answer_rolling_mean) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a3f95-a1ee-4a42-bded-2f916035928b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c3e57b0cc533db9907bb808f4fc1d42",
     "grade": true,
     "grade_id": "task_1p5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert std_am is not None\n",
    "assert std_mm is not None\n",
    "assert T_max_mm is not None\n",
    "assert month_max is not None\n",
    "assert year_max is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb71428-1fd5-49a5-b493-a94f81bc54ce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db7576193e5d78351952f7e322210bbb",
     "grade": true,
     "grade_id": "Tasks_2p1_2p2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert mse_LR is not None\n",
    "assert best_fitting_model_training is not None\n",
    "assert worst_fitting_model_training is not None\n",
    "assert len(answer_LR) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0936b-3473-4c5e-a7d0-6ae8586a082e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "006031b95927bbda258a45a2d6650138",
     "grade": true,
     "grade_id": "autograding_test_data_question",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert best_fitting_model_test is not None\n",
    "assert worst_fitting_model_test is not None\n",
    "assert len(answer_testdata) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d85f2-ac1a-4fa1-adcf-5ff45d6649dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answer_extrapolation) > 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-climate",
   "language": "python",
   "name": "ml-climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
