{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2  - ML - Grundlagen und Algorithmen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Multiclass Classification\n",
    "\n",
    "The Iris Dataset is a very classical machine learning and statistics benchmark for classification, developed in the 1930's. The goal is to classify 3 types of flowers (more specifically, 3 types of flowers form the Iris species) based on 4 features: petal length, petal width, sepal length and sepal width.\n",
    "\n",
    "As we have $K=3$ different types of flowers we are dealing with a multi-class classification problem and need to extend our sigmoid-based classifier from the previous exercise / recap session. \n",
    "\n",
    "We will reuse our \"minimize\" and \"affine feature\" functions. Those are exactly as before. The affine features are sufficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def minimize(f: Callable , df: Callable, x0: np.ndarray, lr: float, num_iters: int) -> \\\n",
    "        Tuple[np.ndarray, float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    :param f: objective function\n",
    "    :param df: gradient of objective function\n",
    "    :param x0: start point, shape [dimension]\n",
    "    :param lr: learning rate\n",
    "    :param num_iters: maximum number of iterations\n",
    "    :return argmin, min, values of x for all interations, value of f(x) for all iterations\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    x = np.zeros([num_iters + 1] + list(x0.shape))\n",
    "    f_x = np.zeros(num_iters + 1)\n",
    "    x[0] = x0\n",
    "    f_x[0] = f(x0)\n",
    "    for i in range(num_iters):\n",
    "        # update using gradient descent rule\n",
    "        grad = df(x[i])\n",
    "        x[i + 1] = x[i] - lr * grad\n",
    "        f_x[i + 1] = f(x[i + 1])\n",
    "    return x[i+1], f_x[i+1], x[:i+1], f_x[:i+1] # logging info for visualization\n",
    "\n",
    "\n",
    "def affine_features(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    implements affine feature function\n",
    "    :param x: inputs\n",
    "    :return inputs with additional bias dimension\n",
    "    \"\"\"\n",
    "    return np.concatenate([x, np.ones((x.shape[0], 1))], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "In the original dataset the different types of flowers are labeled with $0, 1$ and $2$. The output of our classifier will be a vector with $K=3$ entries, $\\begin{pmatrix}p(c=0 | \\boldsymbol x) & p(c=1 | \\boldsymbol x) & p(c=2 | \\boldsymbol x) \\end{pmatrix}$, i.e. the probability for each class that a given sample is an instance of that class, given a datapoint $\\boldsymbol x$. As presented in the lecture, working with categorical (=multinomial) distributions is easiest when we represent the labels in a different form, a so called one-hot encoding. This is a vector of the length of number of classes, in this case 3, with zeros everywhere except for the entry corresponding to the class number, which is one. For the train and test data we know to which class it belongs, so the probability for that class is one and the probability for all other classes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(\"iris_data.npz\")\n",
    "train_samples = data[\"train_features\"]\n",
    "train_labels = data[\"train_labels\"]\n",
    "test_samples = data[\"test_features\"]\n",
    "test_labels = data[\"test_labels\"]\n",
    "\n",
    "train_features = affine_features(train_samples)\n",
    "test_features = affine_features(test_samples)\n",
    "\n",
    "def generate_one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param y: vector containing classes as numbers, shape: [N]\n",
    "    :param num_classes: number of classes\n",
    "    :return a matrix containing the labels in an one-hot encoding, shape: [N x K]\n",
    "    \"\"\"\n",
    "    y_oh = np.zeros([y.shape[0], num_classes])\n",
    "\n",
    "    # can be done more efficiently using numpy with\n",
    "    # y_oh[np.arange(y.size), y] = 1.0\n",
    "    # we use the for loop for clarity\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        y_oh[i, y[i]] = 1.0\n",
    "\n",
    "    return y_oh\n",
    "\n",
    "\n",
    "oh_train_labels = generate_one_hot_encoding(train_labels, 3)\n",
    "oh_test_labels = generate_one_hot_encoding(test_labels, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using Gradient Descent\n",
    "\n",
    "The multi-class generalization of the sigmoid is the softmax function. It takes an vector of length $K$ and outputs another vector of length $K$ where the $k$-th entry is given by\n",
    "$$ \\textrm{softmax}(\\boldsymbol{x})_k = \\dfrac{\\exp(x_k)}{\\sum_{j=1}^K \\exp(x_j)}.$$\n",
    "This vector contains positive elements which sum to $1$ and thus can be interpreted as parameters of a categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"softmax function\n",
    "    :param x: inputs, shape: [N x K]\n",
    "    :return softmax(x), shape [N x K]\n",
    "    \"\"\"\n",
    "    a = np.max(x, axis=-1, keepdims=True)\n",
    "    log_normalizer = a + np.log(np.sum(np.exp(x - a), axis=-1, keepdims=True))\n",
    "    return np.exp(x - log_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Aspect:** In the above implementation of the softmax we stayed in the log-domain until the very last command.\n",
    "We also used the log-sum-exp-trick (https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations).\n",
    "Staying in the log domain and applying the log-sum-exp-trick whenever possible is a simple way to make the implementation\n",
    "numerically more robust. It does not change anything with regards to the underlying theory.\n",
    "\n",
    "We also need to extend our loss function. Instead of the log-likelihood of a Bernoulli distribution, we now maximize the log-likelihood of a categorical distribution which, for a single sample $\\boldsymbol{x}_i$, is given by\n",
    "$$\\log p(c_i | \\boldsymbol x_i) = \\sum_{k=1}^K h_{i, k} \\log(p_{i,k})$$\n",
    "where $\\boldsymbol h_i$ denotes the one-hot encoded true label and $p_{i,k} \\equiv p(c_i = k | \\boldsymbol x_i)$ the class probabilities predicted by the classifier. In multiclass classification, we learn one weight vector $\\boldsymbol w_k$ per class s.t. those probabilities are given by $p(c_i = k | \\boldsymbol x_i) = \\mathrm{softmax}(\\boldsymbol w_k^T \\boldsymbol \\phi (\\boldsymbol x_i)) $.\n",
    "We can now implement the (negative) log-likelihood of a categorical distribution (we use the negative log-likelihood as we will minimize the loss later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_nll(predictions: np.ndarray, labels: np.ndarray, epsilon: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    cross entropy loss function\n",
    "    :param predictions: class labels predicted by the classifier, shape: [N x K]\n",
    "    :param labels: true class labels, shape: [N x K]\n",
    "    :param epsilon: small offset to avoid numerical instabilities (i.e log(0))\n",
    "    :return negative log-likelihood of the labels given the predictions, shape: [N]\n",
    "    \"\"\"\n",
    "    return - np.sum(labels * np.log(predictions + epsilon), -1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the loss for a single sample. To get the loss for all samples we will need to sum over loss for a single sample\n",
    "\n",
    "\\begin{align} \n",
    "\\mathcal L_{\\mathrm{cat-NLL}} \n",
    "=& - \\sum_{i=1}^N \\log p(c_i | \\boldsymbol x_i) \\\\ \n",
    "=& - \\sum_{i=1}^N \\sum_{k=1}^K h_{i, k} \\log(p_{i,k}) \\\\ \n",
    "=& - \\sum_{i=1}^N  \\sum_{k=1}^K h_{i, k} \\log(\\textrm{softmax}(\\boldsymbol{w}_k^T \\boldsymbol \\phi(\\boldsymbol{x}_i))_k)\\\\ \n",
    "=& - \\sum_{i=1}^N \\left(\\sum_{k=1}^K h_{i,k}\\boldsymbol{w}^T_k \\boldsymbol \\phi(\\boldsymbol{x}_i) - \\log \\sum_{j=1}^K \\exp(\\boldsymbol{w}_j^T \\boldsymbol \\phi(\\boldsymbol{x}_i))\\right).\n",
    "\\end{align}\n",
    "\n",
    "In order to use gradient based optimization for this, we of course also need to derive the gradient.\n",
    "\n",
    "### 1.1) Derivation (4 Points)\n",
    "Derive the gradient $\\dfrac{\\partial \\mathcal L_{\\mathrm{cat-NLL}}}{\\partial \\boldsymbol{w}}$ of the loss function w.r.t. the full weight vector $\\boldsymbol w \\equiv \\begin{pmatrix} \\boldsymbol w_1^T & \\dots & \\boldsymbol w_K^T \\end{pmatrix}^T$, which is obtained by stacking the class-specific weight vectors $\\boldsymbol w_k$.\n",
    "\n",
    "**Hint 1:** Follow the steps in the derivation of the gradient of the loss for the binary classification in the lecture.\n",
    "\n",
    "**Hint 2:** Derive the gradient not for the whole vector $\\boldsymbol w$ but only for $\\boldsymbol w_k$ i.e., $\\dfrac{\\partial \\mathcal L_{\\mathrm{cat-NLL}}}{\\partial \\boldsymbol{w}_k}$. The gradients for the individual\n",
    "$\\boldsymbol w_k$ can then be stacked to obtain the full gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Implementation (3 Points)\n",
    "Now that we have the formulas for the loss and its gradient, we can implement them. Fill in the function skeletons below so that they implement the loss and its gradient. Again, in praxis, it is advisable to work with the mean nll instead of the sum, as this simplifies setting the learning rate.\n",
    "\n",
    "Hint: The optimizer works with vectors only. So the function get the weights as vectors in the flat_weights parameter. Make sure you use efficient vectorized computations (no for-loops!). Thus, we reshape the weights appropriately before using them for the computations. For the gradients make sure to return again a vector by flattening the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# objective\n",
    "def objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N x K]\n",
    "    :return cross entropy loss of the classifier given the samples \n",
    "    \"\"\"\n",
    "    num_features = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "    weights = np.reshape(flat_weights, [num_features, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "\n",
    "# derivative\n",
    "def d_objective_cat(flat_weights: np.ndarray, features: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param flat_weights: weights of the classifier (as flattened vector), shape: [feature_dim * K]\n",
    "    :param features: samples to evaluate objective on, shape: [N x feature_dim]\n",
    "    :param labels: labels corresponding to samples, shape: [N x K]\n",
    "    :return gradient of cross entropy loss of the classifier given the samples, shape: [feature_dim * K]\n",
    "    \"\"\"\n",
    "    feature_dim = features.shape[-1]\n",
    "    num_classes = labels.shape[-1]\n",
    "    weights = np.reshape(flat_weights, [feature_dim, num_classes])\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO, do not forget to flatten the gradient before returning!\n",
    "    #---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tie everything together again. Both train and test accuracy should be at least 0.9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# optimization\n",
    "\n",
    "w0_flat = np.zeros(5 * 3) # 4 features + bias, 3 classes\n",
    "w_opt_flat, loss_opt, x_history, f_x_history = \\\n",
    "   minimize(lambda w: objective_cat(w, train_features, oh_train_labels),\n",
    "            lambda w: d_objective_cat(w, train_features, oh_train_labels),\n",
    "            w0_flat, 1e-2, 1000)\n",
    "\n",
    "w_opt = np.reshape(w_opt_flat, [5, 3])\n",
    "\n",
    "# plotting and evaluation\n",
    "print(\"Final Loss:\", loss_opt)\n",
    "plt.figure()\n",
    "plt.plot(f_x_history)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"negative categorical log-likelihood\")\n",
    "\n",
    "train_pred = softmax(train_features @ w_opt)\n",
    "train_acc = np.count_nonzero(np.argmax(train_pred, axis=-1) == np.argmax(oh_train_labels, axis=-1))\n",
    "train_acc /= train_labels.shape[0]\n",
    "test_pred = softmax(test_features @ w_opt)\n",
    "test_acc = np.count_nonzero(np.argmax(test_pred, axis=-1) == np.argmax(oh_test_labels, axis=-1))\n",
    "test_acc /= test_labels.shape[0]\n",
    "print(\"Train Accuracy:\", train_acc, \"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) k-NN (3 Points)\n",
    "Here we implement a simple k-NN approach. As we want to use it for classification now and later for regression, we choose a modular approach. Firstly we implement a function that returns the $k$ nearest neighbour points' x-values and (target) y-values, given a querry point. Then we implement a function to do a majority vote for classification, given the (target) y-values of the k nearest points. Note that we use the \"real\" labels, not the one-hot encoding for the k-NN classifier.\n",
    "\n",
    "Work flow and hints (get_k_nearest):\n",
    "- Compute the distance (e.g. Euclidean) between the query point to all data points.\n",
    "- Sort the data points according to their distance to the query point. Sort indices can be more efficient.\n",
    "- Get the K nearest points, return their x and y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest(k: int, query_point: np.ndarray, x_data: np.ndarray, y_data: np.ndarray) \\\n",
    "    -> Tuple[np.ndarray, np.ndarray]:                                                                                        \n",
    "    \"\"\"\n",
    "    :param k: number of nearest neigbours to return \n",
    "    :param query_point: point to evaluate, shape [dimension]\n",
    "    :param x_data: x values of the data [N x input_dimension]\n",
    "    :param y_data: y values of the data [N x target_dimension]\n",
    "    :return k-nearest x values [k x input_dimension], k-nearest y values [k x target_dimension]\n",
    "    \"\"\"\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    #---------------------------------------------------------------\n",
    "    \n",
    "def majority_vote(y: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    :param y: k nearest targets [K]\n",
    "    :return the number x which occours most often in y. \n",
    "    \"\"\"\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    #---------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the classifier and measure the accuracy. For $k=5$ it should be $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "predictions = np.zeros(test_features.shape[0])\n",
    "for i in range(test_features.shape[0]):\n",
    "    _, nearest_y = get_k_nearest(k, test_features[i], train_features, train_labels)\n",
    "    predictions[i] = majority_vote(nearest_y)\n",
    "\n",
    "print(\"Accuracy: \", np.count_nonzero(predictions == test_labels) / test_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Hold-out and Cross Validation\n",
    "In this part of the exercise we will have a closer look on the hold-out and cross validation methods for model selection. We will apply these methods to do model selection for different regression algorithms below.\n",
    "\n",
    "Let's first have a look at the data. Note that the data is given as a tensor of shape [20 x 50 x 1], corresponding to 20 different data sets (drawn from the same ground truth function) with 50 data points each. The data is 1-dimensional. \n",
    "\n",
    "**Note:** \n",
    "In practice we typically have only one dataset available. We evaluate hold-out and cross validation for 20 different datasets here only to get a feeling for the robustness of these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(33)\n",
    "\n",
    "# Load 20 training sets with 50 samples in each set\n",
    "x_samples = np.load('x_samples.npy')    # shape: [20, 50, 1]\n",
    "y_samples = np.load('y_samples.npy')    # shape: [20, 50, 1]\n",
    "\n",
    "# Load the ground truth data\n",
    "x_plt = np.load('x_plt.npy')\n",
    "y_plt = np.load('y_plt.npy')\n",
    "\n",
    "# Plot the data (for the training data we just use the first training set)\n",
    "plt.plot(x_plt, y_plt, c=\"blue\", label=\"Ground truth polynomial\")\n",
    "plt.scatter(x_samples[0, :, :], y_samples[0, :, :], c=\"orange\", label=\"Samples of first training set\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions for Plotting\n",
    "Before we start, we define some helper functions which we will make use of later on. You do not need to implement anything yourself here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(mse_val:np.ndarray, mse_train:np.ndarray, x_axis, m_star_idx:int, x_plt:np.ndarray, y_plt: np.ndarray,\n",
    "         x_samples:np.ndarray, y_samples:np.ndarray, model_best, model_predict_func:callable):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(121)\n",
    "    plot_error_curves(mse_val, mse_train, x_axis, m_star_idx)\n",
    "    plt.subplot(122)\n",
    "    plot_best_model(x_plt, y_plt, x_samples, y_samples, model_best, model_predict_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_curves(MSE_val: np.ndarray, MSE_train:np.ndarray, x_axis, m_star_idx: int):\n",
    "    plt.yscale('log')\n",
    "    plt.plot(x_axis, np.mean(MSE_val, axis=0), color='blue', alpha=1, label=\"mean MSE validation\")\n",
    "    plt.plot(x_axis, np.mean(MSE_train, axis=0), color='orange', alpha=1, label=\"mean MSE train\")\n",
    "    plt.plot(x_axis[m_star_idx], np.min(np.mean(MSE_val, axis=0)), \"x\", label='best model')\n",
    "    plt.xticks(x_axis)\n",
    "    plt.xlabel(\"Model complexity\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_best_model(x_plt: np.ndarray, y_plt: np.ndarray, x_samples: np.ndarray, y_samples:np.ndarray, \n",
    "                    model_best, model_predict_func: callable):\n",
    "    plt.plot(x_plt, y_plt, color='g', label=\"Ground truth\")\n",
    "    plt.scatter(x_samples, y_samples, label=\"Noisy data\", color=\"orange\")\n",
    "    f_hat = model_predict_func(model_best, x_plt)\n",
    "    plt.plot(x_plt, f_hat, label=\"Best model\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    \n",
    "def plot_bars(M, std_mse_val_ho, std_mse_val_cv):\n",
    "    models = np.arange(1, M+1)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.bar(models, std_mse_val_ho, yerr=np.zeros(std_mse_val_ho.shape), align='center', alpha=0.5, ecolor='black', \n",
    "               color='red', capsize=None)\n",
    "    ax1.bar(models, std_mse_val_cv, yerr=np.zeros(std_mse_val_cv.shape), align='center', alpha=0.5, ecolor='black', \n",
    "               color='blue', capsize=None)\n",
    "    ax1.set_xticks(models)\n",
    "    ax1.set_xlabel('Model complexity')\n",
    "    ax1.set_ylabel('Standard deviation')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.set_title('Standard Deviations of MSEs')\n",
    "    ax1.yaxis.grid(True)\n",
    "    plt.legend(['HO', 'CV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Hold-Out Method (4 Points)\n",
    "We will implement the hold-out method for model selection in this section. First, we require a function to split a dataset into a training set and a validation set. Please fill in the missing code snippets. Make sure that you follow the instructions written in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_in: np.ndarray, data_out: np.ndarray, split_coeff: float)->Tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Splits the data into a training data set and a validation data set. \n",
    "    :param data_in: The input data which we want to split, shape: [n_data x indim_data] \n",
    "    :param data_out: The output data which we want to split, shape: [n_data x outdim_data]\n",
    "        Note: each pair of data points with index i in data_in and data_out is considered as a \n",
    "          training/validation sample: (x_i, y_i)\n",
    "    :param split_coeff: A value between [0, 1], which determines the index to split data into test and validation set\n",
    "                        according to: split_idx = int(n_data*split_coeff)\n",
    "    :return: A tuple of 2 dictionaries: the first element in the tuple is the training data set dictionary\n",
    "             containing the input data marked with key 'x' and the output data marked with key 'y'.\n",
    "             The second element in the tuple is the validation data set dictionary containing the input data \n",
    "             marked with key 'x' and the output data marked with key 'y'.\n",
    "    \"\"\"\n",
    "    n_data = data_in.shape[0]\n",
    "    # We use a dictionary to store the training and validation data. \n",
    "    # Please use 'x' as a key for the input data and 'y' as a key for the output data in the dictionaries\n",
    "    # for the training data and validation data.\n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    #---------------------------------------------------------------\n",
    "    # TODO\n",
    "    #---------------------------------------------------------------\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function implements the hold-out method. We split the dataset into a training and a validation data set (using the split_data function you have implemented above). Then, we train a model for a range of complexity values on the training set and choose the model complexity with the best MSE on the validation set. \n",
    "\n",
    "The function expects a callable `fit_func` and a callable `predict_func`. We will pass different functions to this argument depending on the regression algorithm we consider. The `fit_func` function returns model parameters obtained by training a given model with a given complexity on a given training data set. The `predict_func` function computes predictions using a given model with a given complexity. For more information, have a look at the comments.\n",
    "\n",
    "As noted above, we do hold-out for 20 different datasets to get a feeling for the robustness of this method. To this end, we compute the standard deviation of the resulting MSEs over the 20 datasets.\n",
    "\n",
    "You do not need to implement anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_hold_out(M: int, split_coeff: float, fit_func: callable, predict_func: callable) -> float:\n",
    "    \"\"\"\n",
    "    :param M: Determines the range of model complexity parameters. \n",
    "              We perform the hold_out method for model complexities (1, ..., M).\n",
    "    :param split_coeff: A value between [0, 1] determines the index to split data (cf. split_data function).\n",
    "    :param fit_func: Callable which fits the model: \n",
    "                     (x_train: np.ndarray, y_train: np.ndarray, complexity_parameter: int) -> model_parameters: np.ndarray\n",
    "    :param predict_func: Callable which computes predictions with the model: \n",
    "                         (model_parameters: np.ndarray, x_val: np.ndarray) -> y_pred_val: np.ndarray\n",
    "    \"\"\"\n",
    "    n_datasets = 20\n",
    "    mse_train_ho = np.zeros((n_datasets, M))\n",
    "    mse_val_ho = np.zeros((n_datasets, M))\n",
    "    \n",
    "    for d in range(n_datasets):\n",
    "        # Extract current data set and split it into train and validation data\n",
    "        c_x_samples = x_samples[d, :, :]  \n",
    "        c_y_samples = y_samples[d, :, :]\n",
    "        train_data, val_data = split_data(c_x_samples, c_y_samples, split_coeff)\n",
    "        \n",
    "        for m in range(M):\n",
    "            # Train model with complexity m on training set\n",
    "            p = fit_func(train_data['x'], train_data['y'], m + 1)\n",
    "            \n",
    "            # Compute MSE on validation set\n",
    "            y_pred_val = predict_func(p, val_data['x'])\n",
    "            mse_val_ho[d, m] = np.mean((y_pred_val - val_data['y'])**2)\n",
    "            \n",
    "            # For comparison, compute the MSE of the trained model on current training set\n",
    "            y_pred_train = predict_func(p, train_data['x'])\n",
    "            mse_train_ho[d, m] = np.mean((y_pred_train - train_data['y'])**2)\n",
    "                    \n",
    "    # Compute mean and std-deviation of MSE over all datasets\n",
    "    mean_mse_train_ho = np.mean(mse_train_ho, axis=0)\n",
    "    mean_mse_val_ho = np.mean(mse_val_ho, axis=0)\n",
    "    std_mse_train_ho = np.std(mse_train_ho, axis=0)\n",
    "    std_mse_val_ho = np.std(mse_val_ho, axis=0)\n",
    "    \n",
    "    # Pick model with best mean validation loss\n",
    "    m_star_ho = np.argmin(mean_mse_val_ho) \n",
    "    print(\"Best model complexity determined with hold-out method: {}\".format(m_star_ho + 1))\n",
    "    \n",
    "    # Plot predictions with best model (use only the first data set for better readability)\n",
    "    train_data, val_data = split_data(x_samples[0, :, :], y_samples[0, :, :], split_coeff)\n",
    "    p_best_ho = fit_func(train_data['x'], train_data['y'], m_star_ho + 1)\n",
    "    plot(mse_val_ho, mse_train_ho, np.arange(1, M+1), m_star_ho, x_plt, y_plt,\n",
    "         x_samples[0, :, :], y_samples[0, :, :], p_best_ho, predict_func)\n",
    "    \n",
    "    return std_mse_val_ho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) k-Fold-Cross Validation Method (4 Points)\n",
    "We will now implement the $k$-fold cross validation method for model selection in this section. In contrast to the hold-out method, we do not use a single split of a given dataset into a training and validation sets, but rather $k$ different splits. Refer to the lecture slide 21 for our convention on how to define the $i$-th split.\n",
    "\n",
    "Please fill in the missing code snippets. Make sure that you follow the instructions written in the comments. You can refer to the `eval_hold_out`-function above for inspiration (note that for clarity we split the logic into two separate functions `k_fold_cross_validation` and `eval_k_fold_cross_validation` here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data_in: np.ndarray, data_out: np.ndarray, m: int, k: int, fit_func: callable, \n",
    "                            predict_func: callable) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation for a model with complexity m on data (data_in, data_out). \n",
    "    Return the mean squared error incurred on the training and the validation data sets.\n",
    "    :param data_in: The input data, shape: [N x indim_data] \n",
    "    :param data_out: The output data, shape: [N x outdim_data]\n",
    "    :param m: Model complexity parameter. \n",
    "    :param k: Number of partitions of the data set (not to be confused with k in kNN).\n",
    "    :param fit_func: Callable which fits the model: \n",
    "                     (x_train: np.ndarray, y_train: np.ndarray, complexity_parameter: int) -> model_parameters: np.ndarray\n",
    "    :param predict_func: Callable which computes predictions with the model: \n",
    "                         (model_parameters: np.ndarray, x_val: np.ndarray) -> y_pred_val: np.ndarray\n",
    "    :return mse_train: np.ndarray containg the mean squarred errors incurred on the training set for each split k, shape: [k]\n",
    "    :return mse_val: np.ndarray containing the mean squarred errors incurred on the validation set for each split, shape: [k]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check consistency of inputs and prepare some constants\n",
    "    n_data = data_in.shape[0]  # total number of datapoints\n",
    "    assert k <= n_data  # number of partitions has to be smaller than number of data points\n",
    "    assert n_data % k == 0  # we assume that we can split the data into k equally sized partitions here\n",
    "    n_val_data = n_data // k  # number of datapoints in each validation set\n",
    "    \n",
    "    # Prepare return values\n",
    "    mse_train = np.zeros(k)\n",
    "    mse_val = np.zeros(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        # 1: Prepare i-th partition into training and validation data sets (cf. lecture slide 21)\n",
    "        #---------------------------------------------------------------\n",
    "        # TODO   \n",
    "        #---------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # 2: Fit model on training set\n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        #---------------------------------------------------------------\n",
    "\n",
    "        \n",
    "        # 3: Compute predictions on training set and validation set\n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        #---------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # 4: Compute the mean squarred error for the training and validation sets\n",
    "        #---------------------------------------------------------------\n",
    "        # TODO\n",
    "        # mse_val[i] =\n",
    "        # mse_train[i] = \n",
    "        #---------------------------------------------------------------\n",
    "        \n",
    "    return mse_train, mse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will uses the functions you have implemented to evaluate the robustness of the k-fold cross validation method. Similar to the `eval_held_out` function above, it will perform cross validation on the 20 different data sets we have loaded at the beginning and return the standard deviation of the mean squarred errors over the 20 data sets of each different model it is tested on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_k_fold_cross_validation(M: int, k: int, fit_func:callable, predict_func: callable) -> float:\n",
    "    \"\"\"\n",
    "    :param M: Determines the range of model complexity parameters. \n",
    "              We perform the cross-validation method for model complexities (1, ..., M).\n",
    "    :param k: Number of partitions of the data set (not to be confused with k in kNN).\n",
    "    :param fit_func: Callable which fits the model: \n",
    "                     (x_train: np.ndarray, y_train: np.ndarray, complexity_parameter: int) -> model_parameters: np.ndarray\n",
    "    :param predict_func: Callable which computes predictions with the model: \n",
    "                         (model_parameters: np.ndarray, x_val: np.ndarray) -> y_pred_val: np.ndarray    \n",
    "    \"\"\"\n",
    "    n_datasets = 20\n",
    "    mse_train_cv = np.zeros((n_datasets, M))\n",
    "    mse_val_cv = np.zeros((n_datasets, M))\n",
    "    \n",
    "    for d in range(n_datasets):\n",
    "        # Extract current data set and split it into train and validation data\n",
    "        c_x_samples = x_samples[d, :, :]  \n",
    "        c_y_samples = y_samples[d, :, :]\n",
    "        \n",
    "        for m in range(M):        \n",
    "            mse_train_k_cv, mse_val_k_cv = k_fold_cross_validation(c_x_samples, c_y_samples, m + 1, k, fit_func, predict_func)\n",
    "            # Average MSEs over splits\n",
    "            mse_train_cv[d, m] = np.mean(mse_train_k_cv)\n",
    "            mse_val_cv[d, m] = np.mean(mse_val_k_cv)\n",
    "                    \n",
    "    # Compute mean and std-deviation of MSE over all datasets\n",
    "    mean_mse_train_cv = np.mean(mse_train_cv, axis=0)\n",
    "    mean_mse_val_cv = np.mean(mse_val_cv, axis=0)\n",
    "    std_mse_train_cv = np.std(mse_train_cv, axis=0)\n",
    "    std_mse_val_cv = np.std(mse_val_cv, axis=0)\n",
    "    \n",
    "    # Pick model with best mean validation loss\n",
    "    m_star_cv = np.argmin(mean_mse_val_cv) \n",
    "    print(\"Best model complexity determined with cross-validation method: {}\".format(m_star_cv + 1))\n",
    "    \n",
    "    # Plot predictions with best model (use only the first data set for better readability)\n",
    "    train_data, val_data = split_data(x_samples[0, :, :], y_samples[0, :, :], split_coeff)\n",
    "    p_best_cv = fit_func(train_data['x'], train_data['y'], m_star_cv + 1)\n",
    "    plot(mse_val_cv, mse_train_cv, np.arange(1, M+1), m_star_cv, x_plt, y_plt,\n",
    "         x_samples[0, :, :], y_samples[0, :, :], p_best_cv, predict_func)\n",
    "    \n",
    "    return std_mse_val_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) kNN Regression\n",
    "We will now apply hold-out and k-fold cross validation on the regression problem using kNN Regression. In the following we provide a fit and an evaluate function for kNN, which we will use as the callables `fit_func` and `eval_func` for hold-out and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_knn_regressor(train_in: np.ndarray, train_out:np.ndarray, k: int) -> dict:\n",
    "    \"\"\"\n",
    "    Fit a k-nearest neighbors model to the data. This function just returns a compact representation of the input data\n",
    "    data provided, i.e. it stores the training in- and output data together with the number of k neighbors in a dictionary.\n",
    "    :param train_in: The training input data, shape: [N x input dim]\n",
    "    :param train_out: The training output data, shape: [N x output dim]\n",
    "    :param k: The parameter determining how many nearest neighbors to consider\n",
    "    :return: A dictionary containing the training data and the parameter k for k-nearest-neighbors.\n",
    "             Key 'x': the training input data, shape: [N x input dim]\n",
    "             Key 'y': the training output data, shape: [N x output dimension]\n",
    "             Key 'k': the parameter determining how many nearest neighbors to consider\n",
    "    \"\"\"\n",
    "    \n",
    "    model = {'x': train_in, 'y': train_out, 'k': k}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn_regressor(model, data_in: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function will perform predictions using a k-nearest-neighbor regression model given the input data. \n",
    "    Note that knn is a lazy model and requires to store all the training data (see dictionary 'model').\n",
    "    :param model: A dictionary containing the training data and the parameter k for k-nearest-neighbors.\n",
    "                  Key 'x': the training input data, shape: [N x input dim]\n",
    "                  Key 'y': the training output data, shape: [N x output dimension]\n",
    "                  Key 'k': the parameter determining how many nearest neighbors to consider\n",
    "    :param data_in: The data we want to perform predictions on, shape: [N x input dimension]\n",
    "    :return prediction based on k nearest neighbors (mean of the k - neares neighbors) (shape[N x output dimension])\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    if len(data_in.shape) == 1:\n",
    "        data_in = np.reshape(data_in, (-1, 1))\n",
    "    train_data_in = model['x']\n",
    "    train_data_out = model['y']\n",
    "    k = model['k']\n",
    "    if len(train_data_in.shape) == 1:\n",
    "        train_data_in = np.reshape(train_data_in, (-1, 1))\n",
    "    \n",
    "    # Perform predictions\n",
    "    predictions = np.zeros((data_in.shape[0], train_data_out.shape[1]))\n",
    "    for i in range(data_in.shape[0]):\n",
    "        _, nearest_y = get_k_nearest(k, data_in[i, :], train_data_in, train_data_out)   \n",
    "        # we take the mean of the nearest samples to perform predictions\n",
    "        predictions[i, :] = np.mean(nearest_y, axis=0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1) Apply Hold-Out and Cross-Validation to kNN Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply $k$-nearest neighbor regression on our data set and use the hold-out and cross-validation methods to determine the complexity parameter of this model, i.e., the number $k$ of nearest neighbors to consider.\n",
    "As described above, we furthermore plot and compare the standard deviations of the mean squared errors for each model based on the 20 data sets to get a feeling of the robustness of hold-out and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_knn = 20        # Maximum number k of nearest neighbors \n",
    "split_coeff = 0.8 # Split coefficient for the hold-out method\n",
    "k = 10            # Number of splits for the cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hold-out method\n",
    "std_mse_val_ho_knn = eval_hold_out(M=M_knn, split_coeff=split_coeff, fit_func=fit_knn_regressor, \n",
    "                     predict_func=predict_knn_regressor)\n",
    "\n",
    "# Evaluate cross validation method\n",
    "std_mse_val_cv_knn = eval_k_fold_cross_validation(M=M_knn, k=k, fit_func=fit_knn_regressor, \n",
    "                                                  predict_func=predict_knn_regressor)\n",
    "\n",
    "# Plot the standard deviations\n",
    "plot_bars(M_knn, std_mse_val_ho_knn, std_mse_val_cv_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two rows in the cell above show the errorplots and the best model's prediction for hold out (first row) and cross validation (second row), respectively. The last row shows the standard deviation of the mean squarred error over the 20 different data sets incurred by each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Forests\n",
    "We will now apply hold-out and $k$-fold-cross validation on regression with forests. As for k-nearest neighbor regression above, we provide a fit and an evaluate function for forests. Note that we have two different functions for fitting a forest model. In `fit_forest_fixed_n_trees` we investigate the behavior of the algorithm when fixing the number of trees to $1$ and varying the number of samples per leaf. In `fit_forest_fixed_n_samples_leaf` we fix the number of samples per leaf to $1$ and investigate the behavior of the algorithm when varying the number of trees. The evaluation function can be used for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_forest_fixed_n_trees(train_in: np.ndarray, train_out:np.ndarray, min_samples_leaf: int):\n",
    "    \"\"\"\n",
    "    This function will fit a forest model based on a fixed number of trees (can not be change when using this \n",
    "    function, is set globally)\n",
    "    :param train_in: the training input data, shape [N x input dim]\n",
    "    :param train_out: the training output data, shape [N x output dim]\n",
    "    :param min_samples_leaf: the number of samples per leaf to be used \n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=1, min_samples_leaf=min_samples_leaf)\n",
    "    model.fit(train_in, train_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_forest_fixed_n_samples_leaf(train_in: np.ndarray, train_out:np.ndarray, n_trees: int):\n",
    "    \"\"\"\n",
    "    This function will fit a forest model based on a fixed number of sample per leaf (can not be change when \n",
    "    using this function, is set globally)\n",
    "    :param train_in: the training input data, shape [N x input dim]\n",
    "    :param train_out: the training output data, shape [N x output dim]\n",
    "    :param n_trees: the number of trees in the forest \n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=n_trees, min_samples_leaf=1)\n",
    "    model.fit(train_in, train_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forest(model, data_in: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function will perform predictions using a forest regression model on the input data. \n",
    "    :param model: the forest model from scikit learn (fitted before)\n",
    "    :param data_in: :param data_in: the data we want to perform predictions (shape [N x input dimension])\n",
    "    :return prediction based on chosen minimum samples per leaf (shape[N x output dimension]\n",
    "    \"\"\"\n",
    "    y = model.predict(data_in)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape((-1, 1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1) Apply Hold-out and Cross-validation to Forests (Fixed Number of Trees)\n",
    "\n",
    "We apply forest regression with a fixed number of trees of $1$ and use hold-out and cross-validation to determine the complexity parameter of this model, i.e., the number of samples per leaf. As described above, we furthermore plot and compare the standard deviations of the mean squared errors for each model based on the 20 data sets to get a feeling of the robustness of hold-out and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_n_samples_leaf = 10       # Maximum number of samples per leaf\n",
    "split_coeff = 0.8           # Split coefficient for the hold-out method\n",
    "k = 10                      # Number of splits for the cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out method\n",
    "std_mse_val_ho_forest_fixed_n_trees = eval_hold_out(M=M_n_samples_leaf, split_coeff=split_coeff, \n",
    "                                                    fit_func=fit_forest_fixed_n_trees, \n",
    "                                                    predict_func=predict_forest)\n",
    "# Cross validation method\n",
    "std_mse_val_cv_forest_fixed_n_trees = eval_k_fold_cross_validation(M=M_n_samples_leaf, k=k, fit_func=fit_forest_fixed_n_trees, \n",
    "                                                                   predict_func=predict_forest)\n",
    "\n",
    "# Plot the standard deviations\n",
    "plot_bars(M_n_samples_leaf, std_mse_val_ho_forest_fixed_n_trees, std_mse_val_cv_forest_fixed_n_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two rows in the cell above show the errorplots and the best model's prediction for hold out (first row) and cross validation (second row), respectively. The last row shows the standard deviation of the mean squarred error over the 20 different data sets incurred by each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2) Apply Hold-out and Cross-validation to Forests (Fixed Number of Samples per Leaf) \n",
    "We apply forest regression with a fixed number of samples per leaf of $1$ and use hold-out and cross-validation to determine the complexity parameter of this model, i.e., the number of trees. As described above, we furthermore plot and compare the standard deviations of the mean squared errors for each model based on the 20 data sets to get a feeling of the robustness of hold-out and cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_n_trees = 20              # Maximum number of trees\n",
    "split_coeff = 0.8           # Split coefficient for the hold-out method\n",
    "k = 10                      # Number of splits for the cross validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out method\n",
    "std_mse_val_ho_forest_fixed_n_samples = eval_hold_out(M=M_n_trees, split_coeff=split_coeff, \n",
    "                                                      fit_func=fit_forest_fixed_n_samples_leaf,  \n",
    "                                                      predict_func=predict_forest)\n",
    "# Cross-validation method\n",
    "std_mse_val_cv_forest_fixed_n_samples = eval_k_fold_cross_validation(M=M_n_trees, k=k, \n",
    "                                                                     fit_func=fit_forest_fixed_n_samples_leaf, \n",
    "                                                                     predict_func=predict_forest)\n",
    "\n",
    "\n",
    "# Plot the standard deviations\n",
    "plot_bars(M_n_trees, std_mse_val_ho_forest_fixed_n_samples, std_mse_val_cv_forest_fixed_n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two rows in the cell above show the errorplots and the best model's prediction for hold out (first row) and cross validation (second row), respectively. The last row shows the standard deviation of the mean squarred error over the 20 different data sets incurred by each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5) Comparisons\n",
    "\n",
    "#### 3.5.1) (1 Point)\n",
    "Comparing the error plots from section 3.4.2) to the error plots from 3.3.1) and 3.4.1) we observe that the validation error does not increase with the number of trees. Give an intution for this observation.\n",
    "\n",
    "#### 3.5.2) (1 Point)\n",
    "Compare the standard deviation plots from the last three sections. What is the main difference between the hold-out and cross validation methods? Explain the reason for the observed behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
