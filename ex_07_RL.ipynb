{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe8b8a873aaa60c0a467a0aa9f444709",
     "grade": false,
     "grade_id": "cell-e68c7959e70f1332",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Grundlagen der Künstlichen Intelligenz - Wintersemester 2024/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b17c8d50cff7cdfa055a25f7d863b09c",
     "grade": false,
     "grade_id": "cell-a30bb0cac01749e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Übung 7: Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "> 'Grundlagen der künstlichen Intelligenz' im Wintersemester 2024/2025\n",
    ">\n",
    "> - T.T.-Prof. Benjamin Schäfer, benjamin.schaefer@kit.edu\n",
    "> - Prof. Gerhard Neumann, gerhard.neumann@kit.edu\n",
    "\n",
    "---\n",
    "\n",
    "In dieser Übung werden wir ein approximatives Q-Lernen mit linearer Funktionsannäherung. Dafür implementieren wir die Berechnung von Q-Werten, Softmax-Policy zur Aktionswahl, den Temporal Difference (TD)-Fehler sowie die Parameteraktualisierung mittels Gradientenabstieg.\n",
    "\n",
    "### Übungsteam\n",
    "\n",
    "- Philipp Dahlinger, philipp.dahlinger@kit.edu\n",
    "- Nicolas Schreiber, nicolas.schreiber@kit.edu\n",
    "- Sebastian Pütz, sebastian.puetz@kit.edu\n",
    "- Ulrich Oberhofer, ulrich.oberhofer@kit.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63eba3f92a4d5778d791a76e85073bf3",
     "grade": false,
     "grade_id": "cell-2e2ab1e90e963081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Gruppenabgabe\n",
    "\n",
    "Die Übungsblätter können in Gruppen von bis zu **3 Studierenden** abgegeben werden. **Jede Person aus der Gruppe muss die finale Version der Abgabe über Ilias hochladen**, es genügt nicht, dass nur eine Person aus der Gruppe dies tut. Es ist prinzipiell möglich, im Laufe des Semesters sich einer neuen Gruppe anzuschließen, sollte sich die eigene Gruppe vorzeitig auflösen. Generell muss jede Gruppe ihre eigene Lösung hochladen, wir werden die Abgaben auf Duplikate überprüfen.\n",
    "\n",
    "Die Gruppen werden automatisch erfasst, **gebt deshalb die u-Kürzel eurer Gruppenmitglieder in die folgende Zelle ein.** Falls eure Gruppe nur aus 2 Studierenden besteht, oder ihr alleine abgibt, lasst die verbleibenden Felder frei. Hier ein Beispiel für eine Gruppe bestehend aus uabcd und uefgh:\n",
    "\n",
    "_U-Kürzel der Gruppenmitglieder:_\n",
    "\n",
    "_Mitglied 1: uabcd_\n",
    "\n",
    "_Mitglied 2: uefgh_\n",
    "\n",
    "_Mitglied 3:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Kürzel der Gruppenmitglieder:\n",
    "\n",
    "Mitglied 1:\n",
    "\n",
    "Mitglied 2:\n",
    "\n",
    "Mitglied 3:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a056823f27d68b8baed31d453fbd1b3",
     "grade": false,
     "grade_id": "cell-1aeddd52354875b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Auto-grading\n",
    "\n",
    "Wir nutzen ein auto-grading System, welches eure abgegebenen Jupyter Notebooks automatisch analysiert und über\n",
    "hidden Tests auf Richtigkeit prüft. Über diese Tests werden die Punkte bestimmt, die ihr für das Übungsblatt erhaltet.\n",
    "\n",
    "Damit das auto-grading reibungslos funktioniert bitte folgende Dinge beachten:\n",
    "\n",
    "- Vor dem Abgeben eines Notebooks bitte testen, dass alles von vorne bis hinten ohne Fehler durchläuft.\n",
    "- Zellen, welche mit \"### DO NOT CHANGE ###\" markiert sind dürfen weder gelöscht noch bearbeitet werden\n",
    "- Eure Lösung muss in die richtige Zelle (markiert mit \"# YOUR CODE HERE\") eingetragen werden.\n",
    "    - (dabei natürlich den NotImplementedError löschen!)\n",
    "- Es gibt potentiell scheinbar leere Zellen, die auch mit \"### DO NOT CHANGE ###\" markiert sind. Auch diese dürfen nicht bearbeitet oder gelöscht werden.\n",
    "    - Falls dies doch gemacht wird, dann wird das automatische Grading nicht funktionieren und ihr erhaltet keine Punkte.\n",
    "    - Wir werden hier strikt handeln und keine Ausnahmen machen, falls jemand doch Zellen verändert, die eindeutig als readonly markiert sind!\n",
    "- Die Jupyter Notebooks haben inline Tests (für euch sichtbar), welche euer Ergebnis auf grobe Richtigkeit überprüfen.\n",
    "    - Diese sind primär für euch, um Fehler zu erkennen und zu korrigieren.\n",
    "    - Die inline Tests, die ihr im Notebook sehen könnt, sind allerdings nicht die Tests welche für das Grading verwendet werden!\n",
    "    - Die inline Tests sind eine notwendige Bedingung, um beim Grading der Aufgabe Punkte zu erhalten!\n",
    "\n",
    "# **WICHTIG** Abgabe des Notebooks\n",
    "- Bitte das Jupyter Notebook mit dem ursprünglichen Dateinamen ins Ilias hochladen (\"ex_06_hmm_mdp.ipynb\")\n",
    "- Bitte Jupyter Notebook und handgeschriebene PDF einzeln hochladen, nicht als ZIP.\n",
    "- Bitte darauf achten, dass die Jupyter Notebook Zell-Metadaten erhalten bleiben. Das ist eigentlich immer der Fall,\n",
    "in wenigen Fällen gab es hier jedoch Probleme. Um auf Nummer Sicher zu gehen bitte das Notebook vor der Abgabe ein Mal\n",
    "in einem normalen Texteditor öffnen und nach \"nbgrader\" suchen. Wenn hier dann keine entsprechenden JSON-Einträge auftauchen\n",
    "dann sind leider die Metadaten verloren gegangen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05b29da95bfafc00929fbdd070bc9a67",
     "grade": false,
     "grade_id": "cell-378830a033377f4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Aufgabe 2: SnaQe\n",
    "Diese Übung wurde von Philipp Dahlinger für die KIT-Vorlesung Grundlagen der Künstlicher Intelligenz. Die pygame-Implementierung der Snake-Umgebung wurde aus dieser Quelle adaptiert: [GitHub](https://github.com/jl4r1991/SnakeQlearning)\n",
    "\n",
    "## Anweisungen\n",
    "\n",
    "**Führe jeden Codeblock aus, um keine Dependency zu verpassen.**\n",
    "\n",
    "Dieses Jupyter-Notebook bietet ein Framework für Approximate Q-Learning mit linearer Funktionsapproximation im beliebten [Snake-Spiel](https://en.wikipedia.org/wiki/Snake_(video_game_genre)). Lies die Anweisungen sorgfältig durch und vervollständige die unvollständigen Funktionen. Anschließend kannst du das Trainingsverfahren ausführen und deine Implementierung überprüfen.\n",
    "\n",
    "Detaillierte Anweisungen:\n",
    "\n",
    "1. Im Skript *\"snake.py\"* findest du die Definition der *\"Snake\"*-Klasse.\n",
    "2. **Falls du Google Colab verwendest: Lade die Datei *snake.py* hoch, indem du sie per Drag & Drop links in die Ordnerstruktur ziehst.**\n",
    "3. Wichtige Begriffe und Spielregeln:\n",
    "    - **state (Zustand)**: Ein Zustand wird durch die Positionen der Snake-Elemente auf dem Bildschirm (Breite: 30, Höhe: 20) und die Position des Essens bestimmt. Zu Beginn einer Episode wird das Spiel mit einer Schlange der Länge 1 und einer zufälligen Position für das Essen initialisiert. Jedes Mal, wenn sich der Kopf der Schlange an der gleichen Position wie das Essen befindet, wird die Länge der Schlange um 1 erhöht, der Punktestand steigt um 1, und ein neues Essen erscheint an einer zufälligen Position. <br>\n",
    "    - **action (Aktion)**: In jedem Zustand gibt es 4 mögliche Aktionen, nämlich die Richtung, in die sich der Schlangenkopf um 1 Feld bewegen kann (oben, rechts, unten, links).  <br>\n",
    "    - **termination (Beendigung)**: Wenn sich die Schlange in ihren eigenen Körper bewegt oder den Bildschirm verlässt, endet das Spiel. <br>\n",
    "    - **features (Merkmale)**: Ähnlich zu den Tetris-Merkmalen aus den Vorlesungsfolien (Folie 28) extrahieren wir handgefertigte Merkmale des aktuellen Zustands, auf deren Basis die Q-Werte berechnet werden können. Die insgesamt 6 Merkmale sind:\n",
    "        - `pos_x`: Befindet sich das Essen rechts von der Schlange, ist dieser Wert 1. Befindet es sich links von der Schlange, ist der Wert -1. Hat es die gleiche x-Koordinate wie die Schlange, ist der Wert 0.\n",
    "        - `pos_y`: Befindet sich das Essen unterhalb der Schlange, ist dieser Wert 1. Befindet es sich oberhalb der Schlange, ist der Wert -1. Hat es die gleiche y-Koordinate wie die Schlange, ist der Wert 0.\n",
    "        - `surrounding`: Enthält 4 Einträge. Jeder Eintrag repräsentiert das Feld direkt über, rechts, unter oder links vom Schlangenkopf. Ist dieses Feld belegt (entweder durch die Schlange oder eine Wand), beträgt dieser Eintrag 1, ansonsten 0.\n",
    "\n",
    "    <br>\n",
    "\n",
    "4. **Darstellung der Q-Werte**:\n",
    "    Für einen gegebenen Zustand $s$ haben wir immer 4 Q-Werte $Q(s, a_i) \\in \\mathbb{R}$, einen für jede der vier Aktionen $a_1, a_2, a_3, a_4$. Wir stellen sie als vierdimensionalen Vektor dar und berechnen sie mit der Formel:\n",
    "\n",
    "    $$ Q(s) = (Q(s, a_1), ..., Q(s,a_4)) = \\phi(s)^T \\beta \\in \\mathbb{R}^4.$$\n",
    "\n",
    "Hierbei ist $\\phi(s) \\in \\mathbb{R}^6$ die Merkmalsrepräsentation des Zustands. Unsere lernbaren Gewichte $\\beta \\in \\mathbb{R}^{6 \\times 4}$ sind eine Matrix, wobei jede Zeile für die Berechnung des Q-Werts einer der 4 Aktionen verantwortlich ist.\n",
    "\n",
    "4. Weitere funktionsspezifische Anweisungen befinden sich über den unvollständigen Funktionen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6b82ceb6a575f3acad8ef46a8cbea64",
     "grade": false,
     "grade_id": "cell-6e5d4cf5acfc06bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Infer if we are on google colab\n",
    "try:\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    colab_mode = True\n",
    "except ModuleNotFoundError:\n",
    "    colab_mode = False\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "k7FKt0qrbjb1",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56e331ecc45d303bc8f50eb7d707a811",
     "grade": false,
     "grade_id": "cell-bea5b21d66603962",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Include some python packages and snake env\n",
    "import os\n",
    "import pygame\n",
    "import random\n",
    "from snake import Snake\n",
    "import time\n",
    "import numpy as np\n",
    "if colab_mode:\n",
    "    import cv2\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    from google.colab import output as colab_output\n",
    "\n",
    "grading_env_var = os.getenv('NBGRADER_EXECUTION', None)\n",
    "is_grading = (grading_env_var == \"autograde\" or grading_env_var == \"validate\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ybCMIE_fbjb2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a10bfe93649f3d9de7d57b4d95483b0a",
     "grade": false,
     "grade_id": "cell-1a35bb99c26756ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# initialization\n",
    "\n",
    "# fixed seed for deterministic behavior:\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "\n",
    "# dimensions\n",
    "feature_size = 6\n",
    "action_size = 4\n",
    "\n",
    "\n",
    "# learnable weight initialization\n",
    "# these are the global weights we update during the learning\n",
    "betas = np.random.rand(feature_size, action_size)\n",
    "\n",
    "# number of transitions the agent performs Q learning\n",
    "len_epoch = 20000\n",
    "# hyperparameters\n",
    "temp = 100.0\n",
    "lr = 0.2\n",
    "gamma = 0.5\n",
    "\n",
    "# action translator. The snake environment expects string keywords, but for the RL agent it is simpler to output\n",
    "# indices between 0 and 3.\n",
    "actions = {\n",
    "    0: \"left\",\n",
    "    1: \"up\",\n",
    "    2: \"right\",\n",
    "    3: \"down\"\n",
    "}\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9c19837936b6a983e76bd139372ca95",
     "grade": false,
     "grade_id": "cell-64a082c0963a4fd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2 a) Value Function Approximation (1 Punkt) <br>\n",
    "Bitte vervollständige die Funktion **\"approximate_q_values\"** unten. Diese Funktion soll eine **\"lineare Funktionsapproximation\"** durchführen, um die 4 Q-Werte $Q(s, a)$ für einen Zustand zu berechnen.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c268583dc95e9ea514724a6015718e1",
     "grade": false,
     "grade_id": "cell-62f0d9220ebf8251",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hinweis:\n",
    "- Du solltest die Parameter **\"betas\"** sowie einige nützliche numpy-Funktionen wie **\"np.matmul\"** oder **\"np.dot\"** verwenden.<br> Beachte, dass die lineare Funktionsapproximation durch ein Skalarprodukt implementiert werden kann.\n",
    "- Die korrekte Formel findest du in den oberen Anweisungen unter bulletpoint 4.\n",
    "- Du kannst die Korrektheit deiner Funktion mit der Testzelle unten überprüfen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "a9tpbQBfbjb4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "805e1661596d4526c4c984e1f5285136",
     "grade": false,
     "grade_id": "cell-52c0d168df750e08",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def approximate_q_values(phi_s):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector with shape (feature_size,). Extracted features from a state s.\n",
    "\n",
    "    return: Vector containing the 4 Q-values Q(s,a) for every possible action a.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return approximated_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "Hq6BjPNebjb4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c65dfd4ce7d2f55fe79cae957b517713",
     "grade": true,
     "grade_id": "Ex2a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d2f36595-8a7f-4c88-9320-337a1a3d2d09"
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex2a - possible points: 1\n",
    "\n",
    "### Testing your code (If your implementation is right, the following assertion\n",
    "### on randomly generated inputs should hold.)\n",
    "\n",
    "phi_s_example = np.array([0.7, 1.4, 1.1, 0.5, 0.2, 0.9])\n",
    "assert np.allclose(approximate_q_values(phi_s_example), np.array([1.18090161, 1.93226149, 3.22564332, 2.82428688]))\n",
    "print(\"Test Passed!\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bbf48e3c1538060f768b0795e9cea7a",
     "grade": false,
     "grade_id": "cell-7a5cee88723a81ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2 b) Exploration vs. Exploitation (3 Punkte) <br>\n",
    "Bitte vervollständige die Funktionen **\"select_action\"** und **\"softmax\"** unten. Die Funktion **\"select_action\"** wählt eine Aktion (gibt ihren Index zurück) gemäß der **\"Soft-Max-Policy\"**-Explorationsstrategie aus (siehe Folie 18 im Kapitel Reinforcement Learning). Erinnere dich daran, dass diese Explorationsstrategie einen **\"Temperatur\"**-Wert besitzt (in unserem Notebook `temp` genannt), der die Q-Werte skaliert. Eine höhere Temperatur führt zu ähnlicheren Eingaben für die Soft-Max-Funktion, was eine gleichmäßigere Verteilung der Auswahlwahrscheinlichkeiten zur Folge hat. Eine Temperatur nahe 0 erhöht die Schärfe der Wahrscheinlichkeitsverteilung, sodass fast immer die Aktion mit dem höchsten Q-Wert gewählt wird. <br>\n",
    "\n",
    "- Beginne mit der Implementierung der **\"softmax\"**-Funktion:\n",
    "$$\n",
    "f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\n",
    "$$\n",
    "$$\n",
    "f(x)_i = \\frac {\\exp(x_i)}{\\sum_j \\exp(x_j)}.\n",
    "$$\n",
    "Beachte, dass sowohl die Eingabe als auch die Ausgabe Vektoren mit der gleichen Form sind. In dieser Funktion gibt es keine Temperatur, stattdessen skalieren wir die Eingaben zu dieser Funktion.\n",
    "\n",
    "- Implementiere anschließend die **\"select_action\"**-Funktion. Sie hat zwei Argumente: Die aktuelle Merkmalsrepräsentation `phi_s` sowie einen booleschen Wert `sample`. Falls `sample` auf `True` gesetzt ist, soll die Aktion basierend auf der Soft-Max-Policy ausgewählt werden (denke daran, hier die Werte mit der Temperatur zu skalieren). Andernfalls soll die deterministische Aktion mit dem höchsten Q-Wert zurückgegeben werden (wobei die Temperatur ignoriert wird)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baa679e7801520d4b56d8dc2026d4d80",
     "grade": false,
     "grade_id": "cell-c51437b7886d05b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hinweise:\n",
    "- Die Softmax-Funktion kann für eine bessere numerische Stabilität anders implementiert werden als in der oben angegebenen Formel. In dieser Übung kannst du jedoch die einfachere Implementierung gemäß der Formel verwenden.\n",
    "- Du kannst die Funktion **\"np.argmax\"** nutzen, um den Index des maximalen Wertes in einem `np.array` zu erhalten.\n",
    "- Verwende die Softmax-Funktion, um die Auswahlwahrscheinlichkeiten zu berechnen. Denke daran, die Q-Werte mit der Temperatur zu skalieren. Diese ist als globale Variable `temp` definiert.\n",
    "- Sobald du die Auswahlwahrscheinlichkeiten hast, kannst du die Funktion `np.random.multinomial(...)` für das Sampling nutzen. Sieh dir die Dokumentation hier an: [np.random.multinomial](https://numpy.org/doc/stable/reference/random/generated/numpy.random.multinomial.html).\n",
    "- Stelle sicher, dass der Rückgabewert ein Integer ist und kein `np.array` mit Länge 4!\n",
    "- Du kannst die Korrektheit deiner Softmax-Implementierung mit dem Test in der Zelle unten überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "P5yzvJjXbjb5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea1d94992de87bd18e465d0d115b1e82",
     "grade": false,
     "grade_id": "cell-f34d8b77fe0a975a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: 1-dimensional numpy array.\n",
    "    returns: softmax(x), 1-dimensional numpy.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return softmax_value\n",
    "\n",
    "def select_action(phi_s, sample=True):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector with shape (feature_size,). Extracted features from a state s.\n",
    "    sample: Boolean flag. If true, sample an action based on the Soft-Max policy, otherwise return the action with\n",
    "            the highest Q-value.\n",
    "\n",
    "    return: Integer action index a_idx (in [0,1,2,3]) of the selected action.\n",
    "    \"\"\"\n",
    "    qs = approximate_q_values(phi_s)\n",
    "    if sample:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    else:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return a_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "IHBKtJx9bjb6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79312658dac9e369b88275e8cbbcb205",
     "grade": true,
     "grade_id": "Ex2b_softmax",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e615e541-259d-40c4-dc32-3f97a12161dd"
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex2b_softmax - possible points: 1\n",
    "\n",
    "### Testing your code (If your implementation is right, the following assertion\n",
    "### on randomly generated inputs should hold!!)\n",
    "# example input\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "output = softmax(x)\n",
    "assert np.allclose(output, np.array([0.0320586,  0.08714432, 0.23688282, 0.64391426]))\n",
    "print(\"Test passed!\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "l6SrB6BYbjb7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94e0fe55ab8038f7096b8fc3a0fa7bf3",
     "grade": true,
     "grade_id": "ex2b_select_action",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c7fe83be-9748-4aee-f2ca-3f14f9b8c7b7"
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: ex2b_select_action - possible points: 2\n",
    "\n",
    "# Testing your code (If your implementation is right, the following assertion should hold!!)\n",
    "np.random.seed(2201)\n",
    "random.seed(2201)\n",
    "betas = np.random.rand(feature_size, action_size)\n",
    "\n",
    "phi_s_example = np.array([0.5, 1.0, 1.1, 0.2, 0.3, 0.4])\n",
    "phi_s_example_2 = np.array([0.7, 1.4, 0.8, 0.3, 0.1, 0.7])\n",
    "assert select_action(phi_s_example) == 0\n",
    "print(\"Test Passed!\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f7ef12460c7fb038b34047ac47ebe5e",
     "grade": false,
     "grade_id": "cell-902a771122348bfd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2 c) Berechnung des Temporal Difference Fehlers (2 Punkte) <br>\n",
    "Bitte vervollständige die Funktion **\"compute_delta\"** unten. Diese Funktion berechnet den Temporal Difference Error $\\delta$, wie er im Algorithmus auf Folie 31 des RL-Kapitels angegeben ist. Allerdings müssen wir auch terminale Zustände berücksichtigen, was auf der Folie nicht dargestellt ist. Falls der Zustand $s$ terminal ist, berechnen wir $\\delta$ mit der Formel:\n",
    "\n",
    "$$\n",
    "\\delta = r(s,a) - Q(s,a).\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7e92cffd9cc9b6f8aae68eaad2b49a6",
     "grade": false,
     "grade_id": "cell-bbe216d9c8e2f252",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hinweise:\n",
    "- Verwende deine implementierte **\"approximate_q_values()\"**-Funktion.\n",
    "- Du kannst die Funktion **\"np.max\"** nutzen, um den maximalen Wert eines `np.array` zu erhalten.\n",
    "- Verwende die global initialisierte Variable **\"gamma\"** als Diskontfaktor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "rTMcx0nVbjb8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dc88bf68fb7817f1da54595c51932b7",
     "grade": false,
     "grade_id": "cell-8832f0e67cac7236",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def compute_delta(phi_s, a_idx, r, phi_new_s, is_terminal):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector of state s with shape (feature_size,).\n",
    "    a_idx: action index a_idx (in [0,1,2,3]) of the action selected in state s.\n",
    "    r: reward r(s,a) of the action with index a_idx in state s (float).\n",
    "    phi_new_s: Feature vector of state new_s with shape (feature_size,).\n",
    "               new_s is the state after selection action a_idx in state s.\n",
    "    is_terminal: boolean which indicates if s is a terminal state.\n",
    "\n",
    "    return: td_error delta of type float\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "bPzV_643bjcD",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c44d310f8d24c886910cbf58aaae4f0",
     "grade": true,
     "grade_id": "ex_2c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3dec641a-7839-4fae-c133-37d17da5668b"
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: ex_2c - possible points: 2\n",
    "\n",
    "np.random.seed(60)\n",
    "random.seed(60)\n",
    "betas = np.random.rand(feature_size, action_size)\n",
    "phi_s = np.array([0.5, 1.0, 1.1, 0.2, 0.3, 0.4])\n",
    "a_idx = 2\n",
    "r = 4\n",
    "phi_new_s = np.array([0.7, 1.4, 0.8, 0.3, 0.1, 0.7])\n",
    "is_terminal = False\n",
    "assert np.allclose(compute_delta(phi_s, a_idx, r, phi_new_s, is_terminal), 3.2786106153158)\n",
    "is_terminal = True\n",
    "assert np.allclose(compute_delta(phi_s, a_idx, r, phi_new_s, is_terminal), 2.287845163104087)\n",
    "print(\"Test Passed!\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7873427d023fb9c1151b61c717f6c491",
     "grade": false,
     "grade_id": "cell-013b45738154919f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2 d) Berechnung der Ableitung des Q-Werts bezüglich Beta (2 Punkte) <br>\n",
    "Bitte vervollständige die Funktion **\"compute_d_qsa_d_beta\"** unten. Diese Funktion berechnet die Ableitung des Q-Werts bezüglich des globalen Parameters **\"betas\"**. Daher musst du zunächst verstehen, was die Ableitung $ \\frac {\\text{d}Q(s,a)}{\\text{d} \\beta} $ genau ist, bevor du sie implementierst. Einige allgemeine Hinweise:\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45ab8466a6fe5214a7b2c663793903de",
     "grade": false,
     "grade_id": "cell-2830444abab2d140",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wenn wir die Spalten von $\\beta$ definieren als\n",
    "$$\n",
    "\\beta  = (\\beta_1, \\beta_2, \\beta_3, \\beta_4)\n",
    "$$\n",
    "erhalten wir 4 Vektoren $\\beta_i \\in \\mathbb{R}^6$.\n",
    "\n",
    "Falls die Aktion $a$ den Index $i_a$ hat, ergibt sich:\n",
    "$$\n",
    "Q(s, a) = \\phi(s)^T\\beta_{i_a}.\n",
    "$$\n",
    "\n",
    "Die Ableitung einer Matrix kann als die Ableitung bezüglich jeder Spaltenvektor betrachtet werden, horizontal gestapelt:\n",
    "\n",
    "$$\n",
    "\\frac {\\text{d}Q(s,a)}{\\text{d} \\beta} = \\left(\\frac {\\text{d}Q(s,a)}{\\text{d} \\beta_{i,j}}\\right)_{i,j} = \\left(\\frac {\\text{d}Q(s,a)}{\\text{d} \\beta_{1}}, ... \\frac {\\text{d}Q(s,a)}{\\text{d} \\beta_{4}} \\right)\n",
    "$$\n",
    "\n",
    "Für $j \\ne i_a$ hängt der Q-Wert nicht von $\\beta_j$ ab, daher ist die Ableitung $0 \\in \\mathbb{R}^6$. Für $i_a$ ist die Ableitung $\\phi(s) \\in \\mathbb{R}^6$.\n",
    "\n",
    "**Daher ist die vollständige Ableitung\n",
    "$$\n",
    "\\frac {\\text{d}Q(s,a)}{\\text{d} \\beta}\n",
    "$$\n",
    "eine Matrix, die überall Nullen enthält, außer in der Spalte $i_a$, wo die Werte von $\\phi(s)$ stehen.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "l-Rv1jtKbjcF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09b08d4dc512a330d84a8db3f13f510b",
     "grade": false,
     "grade_id": "cell-4d5d1d8a40d6854d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def compute_d_qsa_d_beta(phi_s, a_idx):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector of state s with shape (feature_size,).\n",
    "    a_idx: action index a_idx (in [0,1,2,3]) of the action selected in state s.\n",
    "\n",
    "    return: Derivative of the q_value Q(s, a) wrt. betas. It has shape (feature_size, action_size)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return d_qsa_d_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "HjTKZ6dBbjcF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1922a48404b40aca95d19346601501f6",
     "grade": true,
     "grade_id": "ex_2d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "6740dff0-fb65-4c69-c1b0-351c9895be17"
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: ex_2d - possible points: 2\n",
    "\n",
    "phi_s = np.array([0.5, 1.0, 1.1, 0.2, 0.3, 0.4])\n",
    "a_idx = 0\n",
    "\n",
    "d_qsa = compute_d_qsa_d_beta(phi_s, a_idx)\n",
    "assert np.allclose(d_qsa, np.array([[0.5, 0. , 0. , 0. ],\n",
    " [1.  , 0. , 0.,  0. ],\n",
    " [1.1 ,0.  ,0.  ,0. ],\n",
    " [0.2 ,0.  ,0.  ,0. ],\n",
    " [0.3 ,0.  ,0.  ,0. ],\n",
    " [0.4 ,0.  ,0.  ,0. ]]))\n",
    "print(\"Test Passed!\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad7df300d401eaefbe3b665f90635f57",
     "grade": false,
     "grade_id": "cell-12a88c29ef9afc66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2 e) Gradientabstieg (2 Punkte) <br>\n",
    "Bitte vervollständige die Funktion **\"update_betas\"** unten. Diese Funktion aktualisiert die Parameter **\"betas\"**, die in der Q-Wert-Funktionsapproximation verwendet werden. Die Formel dazu findest du auf Folie 31 (Update des Parametervektors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5468ac84f6879dba987c9e39860965b1",
     "grade": false,
     "grade_id": "cell-3568355df49f099b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hinweise:\n",
    "- Verwende deine Implementierung des TD-Fehlers $\\delta$ sowie der Ableitung der Q-Wert-Funktion $\\frac {\\text{d}Q(s,a)}{\\text{d} \\beta}$.\n",
    "- Gib den neuen Wert von **betas** zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "xRtPUHeTbjcG",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d84b02afab96229fb1e2cbaed57f7bc2",
     "grade": false,
     "grade_id": "cell-4e07fac9f25ea6c3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: PLEASE FINISH THE FUNCTION IN THIS BLOCK\n",
    "def update_betas(phi_s, a_idx, r, phi_new_s, is_terminal):\n",
    "    \"\"\"\n",
    "    phi_s: Feature vector of state s with shape (feature_size,).\n",
    "    a_idx: action index a_idx (in [0,1,2,3]) of the action selected in state s.\n",
    "    r: reward r(s,a) of the action with index a_idx in state s (float).\n",
    "    phi_new_s: Feature vector of state new_s with shape (feature_size,).\n",
    "               new_s is the state after selection action a_idx in state s.\n",
    "    is_terminal: boolean which indicates if s is a terminal state.\n",
    "\n",
    "    return: new value of betas (same shape as betas)\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return new_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2GIln8fHbjcH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c48c140a9cc8a5776cafc0856a53c26f",
     "grade": true,
     "grade_id": "ex2e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: ex2e - possible points: 2\n",
    "\n",
    "# Hidden tests..\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2DYLXZ2ubjcH",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aaeb502c39281af64766cabe873de36d",
     "grade": false,
     "grade_id": "cell-43bea5f19865b507",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Helper Functions for evaluating and playing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "easxt_9_bjcI",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f117965a7dcc4e918a15bb038a47ffca",
     "grade": false,
     "grade_id": "cell-7b7a604de8397ee5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# helper function to test the current policy\n",
    "def test_policy(num_games=5):\n",
    "    av_score = 0\n",
    "    g = 0\n",
    "    snake = Snake(FRAMESPEED=50000)\n",
    "    while g < num_games:\n",
    "        s = snake.get_feature_representation()\n",
    "        a_idx = select_action(s, sample=False)\n",
    "        a = actions[a_idx]\n",
    "        is_terminal = snake.step(a)\n",
    "        if is_terminal:\n",
    "            av_score += snake.last_score\n",
    "            g += 1\n",
    "    pygame.quit()\n",
    "\n",
    "    av_score /= num_games\n",
    "    print(f\"Average score: {av_score}\")\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-hiQ_AmobjcJ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa9886249861083f67d2e5a298a90bda",
     "grade": false,
     "grade_id": "cell-fe2e66deaf71b6c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# helper function to simulate one game with normal speed\n",
    "def play_single_game(framespeed=20):\n",
    "    screen = pygame.display.set_mode((400, 300))\n",
    "    snake = Snake(FRAMESPEED=framespeed)\n",
    "    while True:\n",
    "        s = snake.get_feature_representation()\n",
    "        a_idx = select_action(s, sample=False)\n",
    "        is_terminal = snake.step(actions[a_idx], init_new_game_after_terminal=False)\n",
    "        if is_terminal:\n",
    "            print(f\"Total Score: {snake.last_score}\")\n",
    "            break\n",
    "        if colab_mode:\n",
    "            # Do a sketchy external visualization\n",
    "            os.environ[\"SDL_VIDEODRIVER\"]=\"dummy\"  \n",
    "            #convert image so it can be displayed in OpenCV\n",
    "            view = pygame.surfarray.array3d(screen)\n",
    "            #  convert from (width, height, channel) to (height, width, channel)\n",
    "            view = view.transpose([1, 0, 2])\n",
    "\n",
    "            #  convert from rgb to bgr\n",
    "            img_bgr = cv2.cvtColor(view, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            #Display image, clear cell every 0.1 seconds\n",
    "            cv2_imshow(img_bgr)\n",
    "            time.sleep(0.1)\n",
    "            colab_output.clear()\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62f860bde5d4fefa0e0194df671e9d7f",
     "grade": false,
     "grade_id": "cell-c57e9f9b96299760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Der vollständige Q-Learning-Algorithmus <br>\n",
    "Nun sind alle einzelnen Module des Q-Learning vorbereitet. In der Zelle unten findest du den vollständigen Q-Learning-Algorithmus. Du solltest eine durchschnittliche Punktzahl von über 35 erreichen!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DXeum9objcK",
    "outputId": "98e97336-6e17-4b6f-cfcd-6b8de6f07e6f"
   },
   "outputs": [],
   "source": [
    "# import the global beta values\n",
    "global betas\n",
    "import os\n",
    "\n",
    "if not is_grading:\n",
    "\n",
    "    snake = Snake(FRAMESPEED=50000)\n",
    "\n",
    "    for i in range(len_epoch):\n",
    "        # 1. get the feature representation of the current state by calling snake.get_feature_representation()\n",
    "        phi_s = snake.get_feature_representation()\n",
    "        # 2. Select the action using the epsilon-greedy exploration strategy\n",
    "        a_idx = select_action(phi_s, sample=True)\n",
    "        # 3. Get the action string which is needed for the snake environment. (already implemented)\n",
    "        a = actions[a_idx]\n",
    "        # 4. Ask for the current reward (already impelmented)\n",
    "        r = snake.get_reward(a)\n",
    "        # 5. Perform one step. This returns the boolean is_terminal if the snake died during this step.\n",
    "        # (already implemented)\n",
    "        is_terminal = snake.step(a)\n",
    "        # 6. Get the feature representation of the updated state by calling snake.get_feature_representation()\n",
    "        phi_new_s = snake.get_feature_representation()\n",
    "        # 7. Update betas by calling the update_betas(...) method (already implemented)\n",
    "        betas = update_betas(phi_s, a_idx, r, phi_new_s, is_terminal)\n",
    "\n",
    "        # To see how well the current policy works we test it every 5000 updates\n",
    "        if i % 5000 == 0:\n",
    "            pygame.quit()\n",
    "            test_policy()\n",
    "            snake = Snake(FRAMESPEED=50000)\n",
    "\n",
    "        # update hyperparameters\n",
    "        temp *= 0.999\n",
    "        temp = max(temp, 0.1)\n",
    "        #print(epsilon)\n",
    "        lr *= 0.9999\n",
    "\n",
    "    test_policy()\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "2K_RQcNQbjcK",
    "outputId": "a42feaa8-e25a-46a2-f7ed-877721240b4d"
   },
   "outputs": [],
   "source": [
    "# Test your learned strategy. You can adjust the speed of the animation with the framespeed argument. We used a cheesy way to make pygame work inside google colab, for a more smooth and faster animation, please run the code using your own jupyter notebook.\n",
    "if not is_grading:\n",
    "    play_single_game(framespeed=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIIekFLibjcL"
   },
   "outputs": [],
   "source": [
    "# if you want to close the pygame window (only relevant if you are outside google colab)\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1df0e440e28b544c73ce4ef2f6237e41",
     "grade": false,
     "grade_id": "cell-c1131b193794c3b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Zusammenfassung\n",
    "Das war die letzte Übung der Vorlesung GKI. Die hier vorgestellte Reinforcement Learning Methode wird nicht gut skalieren zu größeren Problemen, da Target Networks und Replay Buffers nicht implementiert wurden. Auch die Features sind nicht aussagekräftig genug, um globale Problemkonstellationen zu erkennen. Hoffentlich habt ihr aber einen Eindruck bekommen, wie ein RL-Agent aussehen kann.\n",
    "Viel Erfolg bei der Klausur!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
