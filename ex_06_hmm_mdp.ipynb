{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ece6e4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6100ccdf7264975de5a69120a01b753",
     "grade": false,
     "grade_id": "cell-fbf3f215da39bace",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Grundlagen der Künstlichen Intelligenz - Wintersemester 2024/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55cd18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d764ebf962c90bb44ca930382b58bfa4",
     "grade": false,
     "grade_id": "cell-d6c92a41157c4935",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Übung 6: Hidden Markov Models und Markov Decision Processes\n",
    "\n",
    "---\n",
    "\n",
    "> 'Grundlagen der künstlichen Intelligenz' im Wintersemester 2024/2025\n",
    ">\n",
    "> - T.T.-Prof. Benjamin Schäfer, benjamin.schaefer@kit.edu\n",
    "> - Prof. Gerhard Neumann, gerhard.neumann@kit.edu\n",
    "\n",
    "---\n",
    "\n",
    "In dieser Übung werden wir ein Hidden Markov Model definieren und den Forward/Backward Algorithmus darauf anwenden. Außerdem wollen wir für ein gegebenes MDP die Value Iteration durchführen und eine Policy extrahieren.\n",
    "\n",
    "### Übungsteam\n",
    "\n",
    "- Philipp Dahlinger, philipp.dahlinger@kit.edu\n",
    "- Nicolas Schreiber, nicolas.schreiber@kit.edu\n",
    "- Sebastian Pütz, sebastian.puetz@kit.edu\n",
    "- Ulrich Oberhofer, ulrich.oberhofer@kit.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50493118",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63eba3f92a4d5778d791a76e85073bf3",
     "grade": false,
     "grade_id": "cell-2e2ab1e90e963081",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Gruppenabgabe\n",
    "\n",
    "Die Übungsblätter können in Gruppen von bis zu **3 Studierenden** abgegeben werden. **Jede Person aus der Gruppe muss die finale Version der Abgabe über Ilias hochladen**, es genügt nicht, dass nur eine Person aus der Gruppe dies tut. Es ist prinzipiell möglich, im Laufe des Semesters sich einer neuen Gruppe anzuschließen, sollte sich die eigene Gruppe vorzeitig auflösen. Generell muss jede Gruppe ihre eigene Lösung hochladen, wir werden die Abgaben auf Duplikate überprüfen.\n",
    "\n",
    "Die Gruppen werden automatisch erfasst, **gebt deshalb die u-Kürzel eurer Gruppenmitglieder in die folgende Zelle ein.** Falls eure Gruppe nur aus 2 Studierenden besteht, oder ihr alleine abgibt, lasst die verbleibenden Felder frei. Hier ein Beispiel für eine Gruppe bestehend aus uabcd und uefgh:\n",
    "\n",
    "_U-Kürzel der Gruppenmitglieder:_\n",
    "\n",
    "_Mitglied 1: uabcd_\n",
    "\n",
    "_Mitglied 2: uefgh_\n",
    "\n",
    "_Mitglied 3:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29208ecb",
   "metadata": {},
   "source": [
    "U-Kürzel der Gruppenmitglieder:\n",
    "\n",
    "Mitglied 1:\n",
    "\n",
    "Mitglied 2:\n",
    "\n",
    "Mitglied 3:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b5258",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a056823f27d68b8baed31d453fbd1b3",
     "grade": false,
     "grade_id": "cell-1aeddd52354875b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Auto-grading\n",
    "\n",
    "Wir nutzen ein auto-grading System, welches eure abgegebenen Jupyter Notebooks automatisch analysiert und über\n",
    "hidden Tests auf Richtigkeit prüft. Über diese Tests werden die Punkte bestimmt, die ihr für das Übungsblatt erhaltet.\n",
    "\n",
    "Damit das auto-grading reibungslos funktioniert bitte folgende Dinge beachten:\n",
    "\n",
    "- Vor dem Abgeben eines Notebooks bitte testen, dass alles von vorne bis hinten ohne Fehler durchläuft.\n",
    "- Zellen, welche mit \"### DO NOT CHANGE ###\" markiert sind dürfen weder gelöscht noch bearbeitet werden\n",
    "- Eure Lösung muss in die richtige Zelle (markiert mit \"# YOUR CODE HERE\") eingetragen werden.\n",
    "    - (dabei natürlich den NotImplementedError löschen!)\n",
    "- Es gibt potentiell scheinbar leere Zellen, die auch mit \"### DO NOT CHANGE ###\" markiert sind. Auch diese dürfen nicht bearbeitet oder gelöscht werden.\n",
    "    - Falls dies doch gemacht wird, dann wird das automatische Grading nicht funktionieren und ihr erhaltet keine Punkte.\n",
    "    - Wir werden hier strikt handeln und keine Ausnahmen machen, falls jemand doch Zellen verändert, die eindeutig als readonly markiert sind!\n",
    "- Die Jupyter Notebooks haben inline Tests (für euch sichtbar), welche euer Ergebnis auf grobe Richtigkeit überprüfen.\n",
    "    - Diese sind primär für euch, um Fehler zu erkennen und zu korrigieren.\n",
    "    - Die inline Tests, die ihr im Notebook sehen könnt, sind allerdings nicht die Tests welche für das Grading verwendet werden!\n",
    "    - Die inline Tests sind eine notwendige Bedingung, um beim Grading der Aufgabe Punkte zu erhalten!\n",
    "\n",
    "# **WICHTIG** Abgabe des Notebooks\n",
    "- Bitte das Jupyter Notebook mit dem ursprünglichen Dateinamen ins Ilias hochladen (\"ex_06_hmm_mdp.ipynb\")\n",
    "- Bitte Jupyter Notebook und handgeschriebene PDF einzeln hochladen, nicht als ZIP.\n",
    "- Bitte darauf achten, dass die Jupyter Notebook Zell-Metadaten erhalten bleiben. Das ist eigentlich immer der Fall,\n",
    "in wenigen Fällen gab es hier jedoch Probleme. Um auf Nummer Sicher zu gehen bitte das Notebook vor der Abgabe ein Mal\n",
    "in einem normalen Texteditor öffnen und nach \"nbgrader\" suchen. Wenn hier dann keine entsprechenden JSON-Einträge auftauchen\n",
    "dann sind leider die Metadaten verloren gegangen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a0d3b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68b28c8d2b2454de32d64d53869bea02",
     "grade": false,
     "grade_id": "cell-1ee8248a4c0f624e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Hidden Markov Models und der Forward / Backward Algorithmus\n",
    "In dieser Aufgabe werden wir ein Hidden Markov Model (HMM) am Beispiel einer Roboter Lokalisierung definieren und anschließend den Belief über Zeit berechnen. \n",
    "\n",
    "Wir nehmen an, dass unsere Welt aus 5 vertikal benachbarten Feldern besteht, in einem dieser Felder befindet sich der Roboter. Wir kodieren die aktuelle Position $z_t$ des Roboters als Ganzzahl. Der Zustand (also die Position des Roboters) zum Zeitpunkt $t$ liegt also zwischen $z_t=0$ und $z_t=4$.\n",
    "Unser Sensor zum Messen des aktuellen Zustands gibt ebenfalls eine Zahl zwischen $y_t=0$ und $y_t=4$ aus. Allerdings ist diese Messung fehlerhaft, wie wir gleich sehen werden.\n",
    "\n",
    "Wir wissen aus der Vorlesung, dass wir für die Definition eines HMMs drei Größen benötigen:\n",
    "- Ausgangsverteilung $p(z_0)$: Diese können wir als Vektor der Größe 5 mit Einträgen zwischen 0 und 1 angeben. Die Summe muss 1 ergeben, um eine wohldefinierte Wahrscheinlichkeitsverteilung zu erhalten.\n",
    "- Transitionsmodell $p(z_{t+1} | z_t)$: Übegangswahrscheinlichkeit von Zustand $z_t$ zu $z_{t+1}$. Dies können wir als $5x5$-Matrix realisieren.\n",
    "- Beobachtungsmodell $p(y_t| z_t)$: Wahrscheinlichkeit für eine Beobachtung gegebn der aktuellen Position. Diese Modell Können wir ebenfalls als $5x5$-Matrix modellieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643fd01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db3eafecee177ef21c095019f60d89e3",
     "grade": false,
     "grade_id": "cell-8b71cbbff221602d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646bc7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b305b0df167405e98bcaecb6b6c0700e",
     "grade": false,
     "grade_id": "cell-6e400335a41b58eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wir nehmen an, dass wir nichts über den initialen Zustand wissen, dementsprechend ist $p(z_0=i) = \\frac1 5$ für alle $i=0,...,4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f28d1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99076b2dbca590d9df3f183bd673c555",
     "grade": false,
     "grade_id": "cell-b145247f2c8a7581",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# Initial state distribution\n",
    "p_z0 = np.ones(5) * 0.2\n",
    "print(\"p(z_0):\", p_z0)\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d05af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db018bb4bae1d594af2d7d3afc33786c",
     "grade": false,
     "grade_id": "cell-6324b4c3c3005927",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Transitionsmodell (1 Punkt)\n",
    "Der Roboter bewegt sich probabilistisch: entweder bleibt er auf seiner aktuellen Position stehen (90% der Fälle), \n",
    "oder er bewegt sich von Feld $i$ auf Feld $i+1$ (10% der Fälle). Zurück gehen kann er also nicht. Falls der Roboter sich schon auf dem letzten Feld $i=4$ befindet, bewegt er sich für die zukünfiten Zeitpunkte nicht mehr.\n",
    "\n",
    "Definiere dieses Transitionsmodell als $5x5$ Matrix $T$, so dass gilt:\n",
    "$$\n",
    "p(z_{t+1}=j|z_t=i) = T[i,j]\n",
    "$$\n",
    "\n",
    "*Hinweis: Beachte, dass die Matrix richtig transponiert ist, siehe dazu die Testwerte in der Zelle darunter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f669819",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78ca44d7459cdee6862f86d370e1983b",
     "grade": false,
     "grade_id": "cell-24c4d6385f363905",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_transition_model():\n",
    "    \"\"\"\n",
    "    returns: np.array T of shape [5, 5], such that p(z_t+1 = j | z_t = i) = T[i, j]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa26b18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4f3dca3bc1bef35da783cdd1c73450e",
     "grade": true,
     "grade_id": "Ex1_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex1_1 - possible points: 1\n",
    "\n",
    "# Tests for 1.1:\n",
    "transition_model = get_transition_model()\n",
    "assert isinstance(transition_model, np.ndarray)\n",
    "assert transition_model.shape == (5, 5)\n",
    "print(\"Prob. of State z_t+1 = 3 given z_t = 2:\", transition_model[2, 3])\n",
    "print(\"Prob. of State z_t+1 = 2 given z_t = 3:\", transition_model[3, 2])\n",
    "print(\"Prob. of State z_t+1 = 1 given z_t = 1:\", transition_model[1, 1])\n",
    "assert np.isclose(transition_model[2, 3], 0.1)\n",
    "assert np.isclose(transition_model[4, 2], 0.0)\n",
    "assert np.isclose(transition_model[1, 1], 0.9)\n",
    "\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0a9cd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e5a3f6c8ddf9de1f3e5e90fba55a83a",
     "grade": false,
     "grade_id": "cell-bb167f26245254e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Beobachtungsmodell (1 Punkt)\n",
    "Der Sensor ist leider ungenau, um die Position des Roboters zu messen. Ist der Roboter in Zustand $i$, so sagt der Sensor alle Positionen von $0$ bis $i$ mit gleicher Wahrscheinlichkeit voraus. Außerdem misst der Sensor niemals eine Beobachtung $j$, die größer als der aktuelle Zustand $i$ des Roboters ist. Als Formel geschrieben:\n",
    "$$\n",
    "p(y_t=j | z_t=i) = \\left\\{ \\begin{array}{ll}\n",
    "                    \\frac 1 {i +1} & j <= i &  \\\\\n",
    "                    0 & \\, \\textrm{sonst} \\\\\n",
    "                    \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Implementiere das Beobachtungsmodell als $5x5$ Matrix H, sodass gilt:\n",
    "$$\n",
    "p(y_t=j | z_t=i) = H[i, j]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aea1cb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a06d74794ebb6710d391ce6f86e4f9c6",
     "grade": false,
     "grade_id": "cell-61d24ca164b4f321",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_observation_model():\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803afc43",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30ea436e0efac7732a6a0e30ddfca96d",
     "grade": true,
     "grade_id": "Ex1_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex1_2 - possible points: 1\n",
    "\n",
    "# Tests for 1.2:\n",
    "observation_model = get_observation_model()\n",
    "assert isinstance(observation_model, np.ndarray)\n",
    "assert observation_model.shape == (5, 5)\n",
    "print(\"Prob. of Observation y_t = 2 given z_t = 1:\", observation_model[1, 2])\n",
    "print(\"Prob. of Observation y_t = 1 given z_t = 2:\", observation_model[2, 1])\n",
    "assert np.isclose(observation_model[1, 2], 0.0)\n",
    "assert np.isclose(observation_model[2, 1], 1/3)\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce222c36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "429574d9706c519955aefc37bce94eab",
     "grade": false,
     "grade_id": "cell-b96d196b84742400",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Um sich das HMM besser vorzustellen, ist hier eine Visualisierung. Die x-Achse läuft über Zeit, die y-Achse sind die verschiedenen Zustände. Ein \"X\" bedeutet, dass sich hier der Roboter zu diesem Zeitpunkt befindet. ein blauer Rahmen eines Kästchen zeigt an, dass diese Position vom Sensor beobachtet wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2c362",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14bd7fa6d138754b936ac83f73f85ec1",
     "grade": false,
     "grade_id": "cell-7504aecc8f77f893",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "def get_states_obs(transition_model, observation_model, seed=42, length=50):\n",
    "    \"\"\"\n",
    "    Creates a sequence of states and observations according to the given HMM.\n",
    "    \"\"\"\n",
    "    # In the visualization, we always start in state 0\n",
    "    current_state = 0\n",
    "    gth_states = [current_state]\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for step in range(length):\n",
    "        transition_probs = transition_model[current_state]\n",
    "        current_state = np.random.choice(len(transition_probs), p=transition_probs)\n",
    "        gth_states.append(current_state)\n",
    "\n",
    "\n",
    "    observations = []\n",
    "    for state in gth_states:\n",
    "        obs_probs = observation_model[state]\n",
    "        observations.append(np.random.choice(len(obs_probs), p=obs_probs))\n",
    "        \n",
    "    return gth_states, observations\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49317de8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9208dabced90e6185702354a9d2fa9ea",
     "grade": false,
     "grade_id": "cell-73395add3999b4e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "def visualize_hmm(belief=None, gth_states=None, observations=None):\n",
    "    T = len(gth_states)\n",
    "    num_states = 5\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(T, num_states ))\n",
    "\n",
    "    # Create the grid of red blocks\n",
    "    for i in range(num_states):\n",
    "        for j in range(T):\n",
    "            if belief is not None:\n",
    "                facecolor =  np.array([1, 0, 0]) *(belief[j, i]) + np.array([1, 1, 1]) * (1 -belief[j, i])  # fill color\n",
    "            else:\n",
    "                facecolor = \"none\"\n",
    "            rect = plt.Rectangle(\n",
    "                (j, num_states - i - 1),  # x, y position (invert y for better readability)\n",
    "                1, 1,  # width, height\n",
    "                facecolor=facecolor,\n",
    "                alpha=1,  # transparency level\n",
    "                edgecolor=None,  # border color   \n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            if observations is not None:\n",
    "                if observations[j] == i:\n",
    "                    linewidth = 2\n",
    "                    edgecolor = \"blue\" \n",
    "                    rect = plt.Rectangle(\n",
    "                        (j, num_states - i - 1),  # x, y position (invert y for better readability)\n",
    "                        1, 1,  # width, height\n",
    "                        facecolor=\"none\",\n",
    "                        edgecolor=edgecolor,  # border color   \n",
    "                        linewidth=linewidth,\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "            if gth_states is not None:\n",
    "                if gth_states[j] == i:\n",
    "                    # Draw an \"X\" if it the real position\n",
    "                    x1, y1 = j, num_states - i - 1  # Bottom-left corner of the rectangle\n",
    "                    x2, y2 = x1 + 1, y1 + 1  # Top-right corner of the rectangle\n",
    "                    line1 = plt.Line2D([x1, x2], [y1, y2], color='black', linewidth=0.5)  # First diagonal\n",
    "                    line2 = plt.Line2D([x1, x2], [y2, y1], color='black', linewidth=0.5)  # Second diagonal\n",
    "                    ax.add_line(line1)\n",
    "                    ax.add_line(line2)\n",
    "                    \n",
    "\n",
    "    # Set axis limits and labels\n",
    "    ax.set_xlim(0, T)\n",
    "    ax.set_ylim(0, num_states)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks(np.arange(T) + 0.5)  # Center labels in grid cells\n",
    "    ax.set_yticks(np.arange(num_states-1, -1, -1) + 0.5)\n",
    "    ax.set_xticklabels([f\"t={t}\" for t in range(T)])\n",
    "    ax.set_yticklabels([f\"State {s}\" for s in range(num_states)])\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "    ax.set_ylabel(\"State Index\")\n",
    "\n",
    "    # Optional: Add gridlines for clarity\n",
    "    # ax.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.5, which='both')\n",
    "\n",
    "    # Turn off the axis frame\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=42, length=15)\n",
    "visualize_hmm(belief=None, gth_states=gth_states, observations=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=42, length=50)\n",
    "visualize_hmm(belief=None, gth_states=gth_states, observations=observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908f032",
   "metadata": {},
   "source": [
    "## 1.3 Forward Algorithmus (3 Punkte)\n",
    "Nun wollen wir die Beobachtungen filtern und einen Belief über den aktuellen State gewinnen. Der Algorithmus ist auf Slide 52 der Vorlesung 12 gegeben. Wir fassen alle Alpha-Werte als ein großes Array der Größe `[len(observations), 5]` auf. Es gilt also\n",
    "$$\n",
    "\\alpha_t(z_t=i) = \\text{alpha}[t, i]\n",
    "$$\n",
    "\n",
    "Implementiere den Forward Algorithmus in der nächsten Funktion! Gebe sowohl die (unnormalisierten) Alpha-Werte, als auch die normalisierten Belief Werte (auf Slide 52 als $b_t$ definiert) zurück. Fange mit der Initialisierung an ($\\alpha_0(z_0) = p(y_0|z_0)p(z_0)$ für alle $z_0 = 0, ..., 4$).\n",
    "\n",
    "*Hinweis: Du kannst for-loops verwenden, eine komplette Vektorisierung ist nicht verlangt und für dieses Beispiel nicht notwendig.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846d14b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daa8be957d2f7bdeba01787b51a82b18",
     "grade": false,
     "grade_id": "cell-66a772d15ac42156",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_algo(p_z0, transition_model, observation_model, observations):\n",
    "    \"\"\"\n",
    "    Forward Algorithm.\n",
    "    params:\n",
    "        p_z0: Prior Belief of the initial state distribution z_0\n",
    "        transition_model: Output of get_transition_model(), shape [5, 5]\n",
    "        observation_model: Output of get_observation_model(), shape [5, 5]\n",
    "        observations: List of observations, length: num_timesteps, values: between 0 and 4. Output of the sensor\n",
    "    returns:\n",
    "        Tuple (alpha, b). alpha: np.array of shape [len(observations), 5],\n",
    "                              b: np.array of shape [len(observations), 5].\n",
    "    \"\"\"\n",
    "    alpha = np.zeros((len(observations), 5))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return alpha, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93bb00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53da9854351ea29f2d3a829fe61fb89e",
     "grade": true,
     "grade_id": "Ex1_3a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex1_3a - possible points: 2\n",
    "\n",
    "# Tests for 1.3\n",
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=42, length=15)\n",
    "alpha, b = forward_algo(p_z0, transition_model, observation_model, observations)\n",
    "assert isinstance(alpha, np.ndarray)\n",
    "assert isinstance(b, np.ndarray)\n",
    "assert alpha.shape == (len(observations), 5)\n",
    "assert b.shape == alpha.shape\n",
    "assert np.all(alpha >= 0)\n",
    "assert np.all(b >= 0)\n",
    "assert np.all(b <= 1)\n",
    "\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a35a7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bec751328c0d8ea97835f50710d65f55",
     "grade": true,
     "grade_id": "Ex1_3b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex1_3b - possible points: 1\n",
    "\n",
    "# Hidden Tests for 1.3\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2bb56",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93956ea38f1230a02fb538c72d89ed02",
     "grade": false,
     "grade_id": "cell-3f576cc5a4849adc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hier ein paar Visualisierung des gefilterten Belief States: Die Intensität der Färbung gibt die Wahrscheinlichkeit an, dass hier der Roboter sich befindet. Da wir als Prior Verteilung des Zustands keine Annahmen gemacht haben, ist die initiale Verteilung noch sehr uniform. Über die ersten Zeitschritte wird sich der Algorithmus dann zunehmend sicherer, dass sich der Roboter noch im ersten Zustand befindet. \n",
    "\n",
    "Beachte außerdem, dass z.B. in der ersten Visualisierung von $t=11$ bis $t=17$ der Zustand 2 einen vergleichsweise hohen Belief hat, allerdings Zustand 0 einen Belief von 0 hat. Dies folgt aus der Definition des HMM, da sobald eine Beobachtung von State 1 vorhanden ist, eine Position des Roboters im Zustand 0 ausgeschlossen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=0, length=20)\n",
    "alpha, b = forward_algo(p_z0, transition_model, observation_model, observations)\n",
    "visualize_hmm(belief=b, gth_states=gth_states, observations=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d335d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=42, length=50)\n",
    "alpha, b = forward_algo(p_z0, transition_model, observation_model, observations)\n",
    "visualize_hmm(belief=b, gth_states=gth_states, observations=observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b460c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a6e39cdf4de771b34f73060bd8129b1",
     "grade": false,
     "grade_id": "cell-b28fe6b45470d01b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.4 Backward Algorithmus (2 Punkte)\n",
    "Um eine Glättung durchzuführen, müssen wir den Backward Algorithmus implementieren. Hier die wichtigen Formeln:\n",
    "\n",
    "Initialisierung: für alle $z_T = 0,...,4$:\n",
    "    $$\\beta_T(z_T) = 1$$\n",
    "    \n",
    "Iteration: für $t= T-1, ..., 0$ und $z_t = 0, ..., 4$:\n",
    "    $$\\beta_t(z_t) = \\sum_{z_{t+1}=0}^4 p(z_{t+1}| z_t)p(y_t | z_t) \\beta_{t+1}(z_{t+1})$$\n",
    "\n",
    "Implementiere den Backward Algorithmus in der nächste Funktion. Anders als im Forward Algorithmus ist hier keine Normalisierung von $\\beta$ verlangt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b86da",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a243b5b48f3ebd543b5975e09c25fb88",
     "grade": false,
     "grade_id": "cell-52ba168fe12541e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def backward_algo(transition_model, obs_model, observations):\n",
    "    \"\"\"\n",
    "    Backward Algorithm.\n",
    "    params:\n",
    "        transition_model: Output of get_transition_model(), shape [5, 5]\n",
    "        observation_model: Output of get_observation_model(), shape [5, 5]\n",
    "        observations: List of observations, length: num_timesteps, values: between 0 and 4. Output of the sensor\n",
    "    returns:\n",
    "        beta: np.array of shape [len(observations), 5].\n",
    "                              \n",
    "    \"\"\"\n",
    "    beta = np.zeros((len(observations), 5))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28217fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4a4bd185b798ce9bcaed36a050a0500",
     "grade": true,
     "grade_id": "Ex1_4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex1_4 - possible points: 2\n",
    "\n",
    "# Tests for 1.4\n",
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=42, length=15)\n",
    "beta = backward_algo(transition_model, observation_model, observations)\n",
    "assert isinstance(beta, np.ndarray)\n",
    "assert beta.shape == (len(observations), 5)\n",
    "assert np.all(beta >= 0)\n",
    "\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b90d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "239c21a94cc88bb6ffcd22eaa9dd9415",
     "grade": false,
     "grade_id": "cell-c8c0eb1bf82a6677",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.5 Forward-Backward Algorithmus / Glättung (2 Punkte)\n",
    "Nun können wir beide Schritte zusammen führen und eine Glättung der gemessenen Daten bestimmen, welche zu jedem Zeitpunkt sowohl vergangene als auch zukünftige Zeitpunkte mit einbezieht. Wie auf Folie 56 hergeleitet, ist der finale Belief gegeben als das normalisierte Produkt der Alpha und Beta Werte:\n",
    "$$\n",
    "b_t(z_t) = \\frac {\\alpha_t(z_t)\\beta_t(z_t)}{\\sum_{z_t=0}^4 \\alpha_t(z_t)\\beta_t(z_t)}\n",
    "$$\n",
    "\n",
    "Implementiere dies in der nächsten Funktion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d7bb1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a75487bcfa91d4f7c3705f6cb8e99568",
     "grade": false,
     "grade_id": "cell-ac0dee1e6353f55e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_backward_algo(p_z0, transition_model, obs_model, observations):\n",
    "    \"\"\"\n",
    "    Forward-Backward Algorithm. The filtered result (forward_b) will also be returned, but we don't need it \n",
    "    as an intermediate value.\n",
    "    params:\n",
    "        p_z0: Prior Belief of the initial state distribution z_0\n",
    "        transition_model: Output of get_transition_model(), shape [5, 5]\n",
    "        observation_model: Output of get_observation_model(), shape [5, 5]\n",
    "        observations: List of observations, length: num_timesteps, values: between 0 and 4. Output of the sensor\n",
    "    returns:\n",
    "        Tuple (belief, forward_b). belief: np.array of shape [len(observations), 5], \n",
    "                                           Ouptut of the Forward-Backward Algo. (b_t(z_t) in the above equation.)\n",
    "                                   forward_b: np.array of shape [len(observations), 5], \n",
    "                                           Normalized output of the Forward Algo.\n",
    "    \"\"\"\n",
    "    alpha, forward_b = forward_algo(p_z0, transition_model, obs_model, observations)\n",
    "    beta = backward_algo(transition_model, obs_model, observations)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return belief, forward_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53205f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "403e5989fd7975328a61666b312bb96f",
     "grade": true,
     "grade_id": "Ex1_5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex1_5 - possible points: 2\n",
    "\n",
    "# Tests for 1.5\n",
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=42, length=15)\n",
    "belief, forward_b = forward_backward_algo(p_z0, transition_model, observation_model, observations)\n",
    "assert isinstance(belief, np.ndarray)\n",
    "assert isinstance(forward_b, np.ndarray)\n",
    "assert np.shape(belief) == (len(observations), 5)\n",
    "assert np.shape(forward_b) == (len(observations), 5)\n",
    "assert np.all(belief >= 0)\n",
    "assert np.all(belief <= 1)\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af499e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb32da579f2a6a585b5cdb6afe1af829",
     "grade": false,
     "grade_id": "cell-85ea4bf0d7565330",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In der finalen Visualisierung können wir den Filter mit der Glättung vergleichen. Wir sollten sehen, dass die  uniforme Prior Verteilung in der Glättung komplett verdrängt wurde, da aufgrund der zukünftigen Daten nur $z_t=0$ als Startzustand plausibel ist. Außerdem ist die geglättete Verteileung schärfer, der Algorithmus ist sich weniger unsicher, was der aktuelle Zustand ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=0, length=20)\n",
    "belief, forward_b = forward_backward_algo(p_z0, transition_model, observation_model, observations)\n",
    "print(\"Filtering:\")\n",
    "visualize_hmm(belief=forward_b, gth_states=gth_states, observations=observations)\n",
    "print(\"Smoothing:\")\n",
    "visualize_hmm(belief=belief, gth_states=gth_states, observations=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "gth_states, observations = get_states_obs(transition_model, observation_model, seed=42, length=50)\n",
    "belief, forward_b = forward_backward_algo(p_z0, transition_model, observation_model, observations)\n",
    "print(\"Filtering:\")\n",
    "visualize_hmm(belief=forward_b, gth_states=gth_states, observations=observations)\n",
    "print(\"Smoothing:\")\n",
    "visualize_hmm(belief=belief, gth_states=gth_states, observations=observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41e581",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d955ca9502fb65c66a370d90062bad8",
     "grade": false,
     "grade_id": "cell-a25da1ef7c2cd742",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2 MDPs\n",
    "(Für Context, zunächst Aufgabe 2 auf Übungsblatt durcharbeiten)\n",
    "\n",
    "Nach einer Weile findet ihr in einer weiteren Kiste im Lagerraum einige andere Tamagotchi Modelle, der Geschäftsmann scheint ein Sammler gewesen zu sein. Jetzt willst du nachdem du für Fillibert schon mehrere Iterationen der Value-Iteration händisch gerechnet hast, das ganze etwas automatisieren und diese implementieren. Die anderen Tamagotchi funktionieren zum Glück gleich wie Fillibert, haben die gleichen States, und Aktionen, nur die Wahrscheinlichkeiten und Rewards sind anders.\n",
    "\n",
    "Perfekt, dann können wir das ganze erstmal mit den Daten von Fillibert testen!\n",
    "\n",
    "Hinweis:\n",
    "- Wir stellen die State-, Action-, Zustandsübergangs- und Reward-Definitionen bereit. Du musst nur die Value Iteration implementieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3bcfb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d9bf9aefac184bc9c3adc0e18ad6673",
     "grade": false,
     "grade_id": "cell-b8f02137aea4fd19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fb642",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7d529b19bd7805d6816f5454e76c4f8",
     "grade": false,
     "grade_id": "cell-7e1cd8d7a13faec6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# 1) Definition: Zustände, Aktionen\n",
    "states = [\"happy\", \"tired\", \"dead\"]\n",
    "actions = [\"play\", \"sleep\"]\n",
    "state_idx = {s: i for i, s in enumerate(states)}\n",
    "action_idx = {a: i for i, a in enumerate(actions)}\n",
    "\n",
    "# Übergangswahrscheinlichkeiten P(s' | s, a)\n",
    "# p[s, a, s'] = Wahrscheinlichkeit, von Zustand s durch Aktion a nach Zustand s' zu gelangen\n",
    "p = np.zeros((len(states), len(actions), len(states)))\n",
    "\n",
    "# Rewards R(s, a) (Zustands-/Aktions-abhängig, unabhängig von s')\n",
    "r = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# 2) Modellieren der MDP-Übergänge und Rewards:\n",
    "\n",
    "# Zustand happy (index 0)\n",
    "p[state_idx[\"happy\"], action_idx[\"play\"], state_idx[\"happy\"]] = 0.9\n",
    "p[state_idx[\"happy\"], action_idx[\"play\"], state_idx[\"tired\"]] = 0.1\n",
    "r[state_idx[\"happy\"], action_idx[\"play\"]] = 16\n",
    "\n",
    "p[state_idx[\"happy\"], action_idx[\"sleep\"], state_idx[\"happy\"]] = 1.0\n",
    "r[state_idx[\"happy\"], action_idx[\"sleep\"]] = 0\n",
    "\n",
    "# Zustand tired (index 1)\n",
    "p[state_idx[\"tired\"], action_idx[\"play\"], state_idx[\"dead\"]] = 1.0\n",
    "r[state_idx[\"tired\"], action_idx[\"play\"]] = -100  # Sofortiger Tod\n",
    "\n",
    "p[state_idx[\"tired\"], action_idx[\"sleep\"], state_idx[\"tired\"]] = 0.25\n",
    "p[state_idx[\"tired\"], action_idx[\"sleep\"], state_idx[\"happy\"]] = 0.75\n",
    "r[state_idx[\"tired\"], action_idx[\"sleep\"]] = 0\n",
    "\n",
    "# Zustand dead (index 2) – absorbierender Zustand\n",
    "p[state_idx[\"dead\"], action_idx[\"play\"], state_idx[\"dead\"]] = 1.0\n",
    "p[state_idx[\"dead\"], action_idx[\"sleep\"], state_idx[\"dead\"]] = 1.0\n",
    "r[state_idx[\"dead\"], :] = 0  # Keine Belohnung mehr\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a2545",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "377a6bdc209d3a48019b05005dae3f8b",
     "grade": false,
     "grade_id": "cell-fed20687ef5bac8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Value Iteration (2 Punkte)\n",
    "\n",
    "**Aufgabe:**\n",
    "Implementiere in folgender Zelle einen Value-Iteration-Algorithmus, der ausgehend von einer anfänglichen Value-Funktion $V_0(s)=0\\forall s$ die optimalen Werte für jeden Zustand berechnet.\n",
    "\n",
    "Hier nochmal die Definition der Value Funktion: $$V^\\ast(s) = \\max_a Q^\\ast(s, a)$$ mit $$Q^\\ast(s, a) = r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) \\, V^\\ast(s')$$\n",
    "\n",
    "Hinweis:\n",
    "- Wir stellen die State-, Action-, Zustandsübergangs- und Reward-Definitionen bereit. Du musst nur die Value Iteration implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9101a05",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e478733685e5dadd1a3d900710f8905",
     "grade": false,
     "grade_id": "cell-a2ec27116e32d9ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 3) Value Iteration\n",
    "def value_iteration(p, r, num_iterations, gamma=0.7):\n",
    "    \"\"\"\n",
    "    Performs the Value Iteration algorithm to compute the optimal state-value function.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    p : ndarray\n",
    "        Transition probabilities (n_states, n_actions, n_states).\n",
    "    r : ndarray\n",
    "        Rewards (n_states, n_actions).\n",
    "    num_iterations : int\n",
    "        Number of iterations to run.\n",
    "    gamma : float, optional\n",
    "        Discount factor (default: 0.7).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    V : ndarray\n",
    "        Optimal value function (n_states).\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states, n_actions = r.shape\n",
    "    V = np.zeros(n_states)  # initial V(s)=0\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Kopie des bisherigen V, damit wir V updaten können aber noch die vorherigen Values\n",
    "        # für die Updateberechnung nutzen zu können\n",
    "        V_old = V.copy()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691aedd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c2ca580690f1565ac2080e20c0d5c1e",
     "grade": false,
     "grade_id": "cell-7abc9fe50906ce47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In der folgenden Zelle wird eure `value_iteration` Implementierung genutzt um die Value-Funktion für 2 Iterationen zu berechnen. \n",
    "\n",
    "Selbsttest für euch: Sind die Resultate die gleichen wie bei eurer händischen Berechnung für das Arbeitsblatt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b17458",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85896e88c9921d291cf24738a12c6748",
     "grade": true,
     "grade_id": "Ex2_1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex2_1 - possible points: 2\n",
    "\n",
    "# Hauptprogramm: Führe Value Iteration aus und bestimme Policy\n",
    "V_opt = value_iteration(p, r, num_iterations=2, gamma=0.7)\n",
    "\n",
    "print(\"Optimale Werte (Value Function):\")\n",
    "for i, val in enumerate(V_opt):\n",
    "    print(f\"Zustand {states[i]}: V* = {val:.2f}\")\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57911b4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5425f3bd3132b732e22247c29a53f03",
     "grade": false,
     "grade_id": "cell-6b55aa257c7ccd5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Optimal Policy (1 Punkt)\n",
    "\n",
    "**Aufgabe:**\n",
    "Implementiere in folgender Zelle eine Funktion, die basierend auf einer gegebenen Value-Funktion \\( V \\) die optimale Policy für jeden Zustand berechnet.\n",
    "\n",
    "Hier nochmal die Definition der optimalen Policy:  \n",
    "$$\\pi^\\ast(s) = \\arg\\max_a Q^\\ast(s, a)$$  \n",
    "mit  \n",
    "$$\n",
    "Q^\\ast(s, a) = r(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) \\, V(s')\n",
    "$$\n",
    "\n",
    "Hinweis:\n",
    "- Verwende die bereitgestellten State-, Action-, Reward- und Zustandsübergangswahrscheinlichkeiten.  \n",
    "- Die Funktion soll als Eingabe die Value-Funktion \\( V \\), die Zustandsübergangswahrscheinlichkeiten $P$, die Rewards $R$ und den Diskontfaktor $\\gamma$ erhalten.  \n",
    "- Berechne die $Q$-Werte für jeden Zustand und jede Aktion und extrahiere die Aktion mit dem höchsten $Q$-Wert für jeden Zustand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ffb81",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff47e06305d5ccf4f51bafee1865ad9",
     "grade": false,
     "grade_id": "cell-d0b3e3faf909c3f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_policy(V, p, r, gamma=0.7):\n",
    "    \"\"\"\n",
    "    Extracts the optimal policy given a value function.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    V : ndarray\n",
    "        State-value function (n_states).\n",
    "    p : ndarray\n",
    "        Transition probabilities (n_states, n_actions, n_states).\n",
    "    r : ndarray\n",
    "        Rewards (n_states, n_actions).\n",
    "    gamma : float, optional\n",
    "        Discount factor (default: 0.7).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    policy : ndarray\n",
    "        Optimal policy (n_states), where each entry is the index of the best action.\n",
    "    \"\"\"\n",
    "\n",
    "    n_states, n_actions = r.shape\n",
    "    policy = np.zeros(n_states, dtype=int)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0d165",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccc8a77ad3982f22b8cf0137525b9027",
     "grade": false,
     "grade_id": "cell-365e902ec95cf87a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In der folgenden Zelle wird eure `extract_policy` Implementierung genutzt um die optimale Policy der eben berechneten Value-Funktion. \n",
    "\n",
    "Selbsttest für euch: Sind die Resultate die gleichen wie bei eurer händischen Berechnung für das Arbeitsblatt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aee855",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04a3fc5f0b6b34a355c7261fb5ea2887",
     "grade": true,
     "grade_id": "Ex2_2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE #####\n",
    "# ID: Ex2_2 - possible points: 1\n",
    "\n",
    "policy_opt = extract_policy(V_opt, p, r, gamma=0.7)\n",
    "\n",
    "print(\"\\nOptimale Strategie (Policy):\")\n",
    "for i, act_i in enumerate(policy_opt):\n",
    "    print(f\"Zustand {states[i]} --> Aktion '{actions[act_i]}'\")\n",
    "    \n",
    "\n",
    "##### DO NOT CHANGE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593e562",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44f3168be81e39d95914269216b61c37",
     "grade": false,
     "grade_id": "cell-77a8790e1e1b20b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Zusammenfassung\n",
    "\n",
    "Dies war das Notebook über Hidden Markov Models und Markov Decision Processes. Man kann aus den Aufgaben vor allem herausziehen, dass HMMs ideal sind, um Zustände basierend auf unsicheren Beobachtungen zu schätzen, und dass der Forward-Backward-Algorithmus dabei eine zentrale Rolle spielt. Bei den MDPs zeigt die Value Iteration klar, wie man optimale Strategien für Entscheidungsprobleme ermittelt, selbst in komplexen Szenarien mit unsicheren Übergängen.\n",
    "\n",
    "Vergesst nicht die Theorie-Aufgabe auf dem Übungsblatt!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
