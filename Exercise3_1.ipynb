{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:36.925000+01:00",
     "start_time": "2021-11-05T16:52:32.856Z"
    }
   },
   "outputs": [],
   "source": [
    "# library containing all previously implemented optimization algorithms\n",
    "# --> will be continously expanded within this course\n",
    "include(\"optimization_library.jl\");\n",
    "include(\"mplstyle.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2a\n",
    "Implement the Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:37.107000+01:00",
     "start_time": "2021-11-05T16:52:32.858Z"
    }
   },
   "outputs": [],
   "source": [
    "# f: Objective function\n",
    "# df: Gradient of the objective function\n",
    "# Hf: Hessian of the objective function\n",
    "# x0: Initial point\n",
    "# ls: Use linesearch?\n",
    "# eps: Stopping criterion on the newton decrement\n",
    "# maxiters: Max number of iterations\n",
    "\n",
    "function newton_descent(f, df, Hf, x0; ls=true, eps=0.000001, maxiters=1000)\n",
    "    \n",
    "    # make a copy of initial point to prevent changing x0 by manipulating x\n",
    "    x = copy(x0)\n",
    "    # store the trace of the descent path\n",
    "    trace = [x; f(x)]\n",
    "    \n",
    "    for _=1:maxiters\n",
    "        # ==========================================================\n",
    "        # 1. evaluate the newton step d_nt (see Exercise 3.1)\n",
    "        # 2. compute the newton decrement λ and break the loop if λ^2/2 < eps\n",
    "        # 3. if (ls = true) reduce the step length by performing backtracking linesearch\n",
    "        # 4. update the point x\n",
    "        # ===========================================================\n",
    "        trace = hcat(trace, [x; f(x)])\n",
    "    end\n",
    "    return x,trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2b: Marathon Training Reviewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/laufbahn.jpeg\" width=\"350\">\n",
    "\n",
    "Remember the Marathon training from Exercise 2 where we found the optimal model for the given data by applying gradient descent. Now we want to compare with the result from the Newton Method.\n",
    "\n",
    "We still assume a linear model with velocity $v$ scaling linearly with time $t$:\n",
    "\\begin{equation}\n",
    "    p(t) = vt\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:37.742000+01:00",
     "start_time": "2021-11-05T16:52:32.861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Given Dataset\n",
    "\n",
    "# times in minutes\n",
    "t = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120]\n",
    "\n",
    "# distances in kilometers\n",
    "d = [1.88, 4.47, 5.63, 8.13, 8.54, 11.23, 12.27, 14.23, 15.50, 16.93, 18.69, 21.31];\n",
    "\n",
    "# linear model\n",
    "function m(t,v)\n",
    "    # The times are measured in minutes. We convert them in units of hours by dividing by 60. Like this, the velocity\n",
    "    # has units of km/h\n",
    "    return t.*(v/60)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:37.826000+01:00",
     "start_time": "2021-11-05T16:52:32.864Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss(v) = sum((d-m(t,v)).^2)\n",
    "\n",
    "# Gradient of the loss function\n",
    "function dloss(v)\n",
    "    inner = -t./60\n",
    "    outer = 2*(d-m(t,v))\n",
    "    return inner' * outer\n",
    "end\n",
    "\n",
    "# ===============================================\n",
    "# Define the Hessian of the loss function\n",
    "H_loss(v) = 0\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:38.624000+01:00",
     "start_time": "2021-11-05T16:52:32.866Z"
    }
   },
   "outputs": [],
   "source": [
    "x0 = 1\n",
    "\n",
    "# Result from Gradient Descent\n",
    "result_grd,trace_grd = gradient_descent(loss,dloss, x0,maxiters = 10000, eps = 0.0001, p = 0.5)\n",
    "println(\"Optimal velocity [km/h]: \",result_grd)\n",
    "println(\"Optimal loss: \",trace_grd[end,end])\n",
    "println(\"Gradient at optimum: \",dloss(result_grd))\n",
    "println(\"Iterations: \", size(trace_grd,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:38.628000+01:00",
     "start_time": "2021-11-05T16:52:32.867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Result of Newton Method\n",
    "# ==========================================================\n",
    "# Perform the minimization using the Newton method.\n",
    "# Print the optimal parameters and the number of Iterations.\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:38.628000+01:00",
     "start_time": "2021-11-05T16:52:32.870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test if the results from gradient descent and Newton method are the same\n",
    "# ==========================================================\n",
    "# uncomment the following line\n",
    "#@assert result_nwt - result_grd < 0.0001\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:41.866000+01:00",
     "start_time": "2021-11-05T16:52:32.873Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the trace of the two algorithms\n",
    "\n",
    "# create values of velocities for which the loss function should be evaluated\n",
    "test_velocities = range(1,15,length = 100)\n",
    "\n",
    "# evaluate the loss function for some test velocities\n",
    "losses = zeros(size(test_velocities,1))\n",
    "for (index,test) in enumerate(test_velocities)\n",
    "    losses[index] = loss(test)\n",
    "end\n",
    "\n",
    "plot(test_velocities',losses, color = \"blue\")\n",
    "plot(trace_grd[1,:],trace_grd[2,:], marker = \"o\", color = \"green\", label = \"Gradient descent\")\n",
    "\n",
    "# =============================================================================================\n",
    "# add the trace of the Newton Method to the plot\n",
    "# =============================================================================================\n",
    "\n",
    "xlabel(\"v [km/h]\")\n",
    "ylabel(L\"$l(v)$\")\n",
    "\n",
    "legend()\n",
    "title(\"Fit of linear Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2c\n",
    "\n",
    "The linear model contains only one parameter that is estimated. However, the performance of gradient descent and Newton method differ most severely on high-dimensional problems, i.e. when the model contains a high number of parameters. Hence, we use the dataset from the marathon training an perform a fit of a higher-order polynomial (irrespectively if we know that this makes sense or not). We are interested to see how many iterations each method needs to reach the optimal combination of parameters $\\theta_i$.\n",
    "\n",
    "A polynomial of degree $d$ is given by:\n",
    "\\begin{equation}\n",
    "P_d(\\mathbf{t}) = \\sum_{j = 0}^d \\theta_j \\mathbf{t}^j\n",
    "\\end{equation}\n",
    "Note that $\\mathbf{t}$ is a vector containing all measured times during the marathon training. In this case $\\mathbf{t}^j$ is the vector resulting from evaluating the $j$-th power of each element in $\\mathbf{t}$:\n",
    "\\begin{equation}\n",
    "\\mathbf{t}^j = (t_1^j, t_2^j, \\dots, t_n^j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:41.955000+01:00",
     "start_time": "2021-11-05T16:52:32.875Z"
    }
   },
   "outputs": [],
   "source": [
    "# generic implementation of a polynomial of arbitrary degree d\n",
    "function model_poly(t, theta)\n",
    "    # get the number of parameters. The polynomial degree d is given by d = n - 1\n",
    "    n = size(theta,1)\n",
    "    return ((t./60) .^ [0:n-1;]') * theta\n",
    "end\n",
    "\n",
    "# ===============================================================================\n",
    "# implement the quadratic loss between the datapoints and model_poly(t,params)\n",
    "# ===============================================================================\n",
    "\n",
    "\n",
    "# gradient of the loss function\n",
    "function dloss_poly(theta)\n",
    "    # get the number of parameter of the polynomial model\n",
    "    n = size(theta,1)\n",
    "    dloss = zeros(n)\n",
    "    # ===============================================================================\n",
    "    # Implement the gradient vector\n",
    "    # Note that you always have to divide the measured times t by 60 in order to be consistent\n",
    "    # with the rest of the notebook\n",
    "    # ===============================================================================\n",
    "    return dloss\n",
    "end\n",
    "\n",
    "# Hessian of the loss function\n",
    "function H_loss_poly(theta)\n",
    "    # get the number of parameter of the polynomial model\n",
    "    n = size(theta,1)\n",
    "    H_loss = zeros(n,n)\n",
    "    # ===============================================================================\n",
    "    # Implement the hessian matrix\n",
    "    # Note that you always have to divide the measured times t by 60 in order to be consistent\n",
    "    # with the rest of the notebook\n",
    "    # ===============================================================================\n",
    "    return H_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:42.870000+01:00",
     "start_time": "2021-11-05T16:52:32.878Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the number of paramaters in the polynomial model\n",
    "n_theta = 3\n",
    "\n",
    "# create an array of random numbers to get a starting point\n",
    "x0 = 10*rand(n_theta)\n",
    "\n",
    "# Result from Gradient Descent\n",
    "# ===============================================================================\n",
    "# uncomment the following lines\n",
    "#result_grd2,trace_grd2 = gradient_descent(loss_poly,dloss_poly, x0, maxiters = 10000, eps = 0.0001, p = 0.5)\n",
    "#println(\"Minimizer: \",result_grd2)\n",
    "#println(\"Optimal loss: \",trace_grd2[end,end])\n",
    "#println(\"Gradient at optimum: \",dloss_poly(result_grd2))\n",
    "#println(\"Iterations: \", size(trace_grd2,2))\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:42.870000+01:00",
     "start_time": "2021-11-05T16:52:32.881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Result of Newton Method\n",
    "# ==========================================================\n",
    "# Perform the minimization of the polynomial loss using the Newton method.\n",
    "# Print the optimal parameters and the number of Iterations.\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T17:52:43.424000+01:00",
     "start_time": "2021-11-05T16:52:32.884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot linear and higher order polynomial models in order to compare to data points\n",
    "\n",
    "# create more points for which the model can be plotted in order to get a smooth curve\n",
    "times_plot = range(0,stop = 140)\n",
    "    \n",
    "# plot data\n",
    "scatter(t,d,color = \"black\",zorder=3, label = \"data\")\n",
    "\n",
    "# plot models\n",
    "plot(times_plot,m(times_plot,result_grd),color = \"red\",label=\"linear model\")\n",
    "\n",
    "# ===============================================================================\n",
    "# add the curve of the best fitting polynomial model evaluated by the newton method\n",
    "# ===============================================================================\n",
    "\n",
    "xlabel(\"t [min]\")\n",
    "ylabel(\"d [km]\")\n",
    "\n",
    "legend(loc = \"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
