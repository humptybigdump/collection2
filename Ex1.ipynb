{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Exercise 1 Multilayer Perceptron and Batch Normalization Implementation </center>\n",
    "\n",
    "</br>\n",
    "On this exercise, you are implementing a feedforward neural network and batch normalization layer from scratch to do the MNIST problem. This one is using <code>Numpy</code>. As an exercise for Backpropagation as you learned from the lecture, you should work through the derivation of the loss function to the weights or biases with pen and paper. \n",
    "</br>\n",
    "\n",
    "We will try to modularize our neural network as it consists of $L$ layers. **Read the following description carefully, otherwise you would have problems implementing it later**. Each feed forward layer $l$ has a weight matrix $W^{(l)}$ with the size of $(n^{(l)}, n^{(l-1)})$ where $n^{(l)}$ is the number of neurons (outputs) of the layer $l$, a bias vector $b^{(l)}$ with the size of $(n^{(l)}, 1)$. Each layer $l$ has an activation function $f$. Each layer $l$ can be the last layer or not (the error term is calculated differently in that case). The Batch Normalization layers makes the training more stable and  dramatically reduces the number of training epochs. This layer will follow after the activation's output. The backward pass and forward pass of Batch Normalization layer is much more complicated than a feedforward layer (especiall the backward pass). If you don't want to implement the Batch Normalization layer, don't worry, you still can train the network and get 70 % score for this exercise. Therefore, we start with implement a feedforward layer:\n",
    "    \n",
    "* Initialize a layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization. (This is already given in the code)\n",
    "* Do the forward pass through the layer based on its activation function:\n",
    "    *  $$ z^{(l)} = W^{(l)}\\cdot a^{(l-1)} + b^{(l)} $$\n",
    "        $$ a^{(l)} = f(z^{(l)}) $$\n",
    "    *  Where:<br/>\n",
    "        $z^{(l)}$: intermediate net output of layer $l$<br/>\n",
    "        $a^{(l)}$: activation of layer $l$, is the input of the next layer $l+1$, $a^{(0)} = X$<br/>\n",
    "* Do the backward pass through the layer. Using the convenient notation $\\check{w} = \\frac{\\partial \\mathcal{L}}{\\partial w}$ (in the code, $\\check{w}$ is denoted by dw):<br/>\n",
    "$$\\check{w}^{(l)} = \\check{a}^{(l)} \\frac{\\partial a^{(l)}}{\\partial z^{(l)}}\\frac{\\partial z^{(l)}}{\\partial w^{(l)}}= \\check{a}^{(l)}f^{(l)'}a^{(l-1)}$$\n",
    "* Update the weights/biases based on the gradient descent principle.\n",
    "\n",
    "\n",
    "The implementation uses mini-batches, i.e. multiple input samples are aggregated into one tensor and passed through the forward and backward pass all at once. This means, if a single data sample is a vector $x \\in R^n$, then the vectorized implementation has to work on inputs $x \\in R^{n \\times m}$, where $m$ is the batch size (in this exercise, we put the batch dimension at the end of the tensor). Thanks to `numpy`, such vectorized code often looks the same or very similar to the non-vectorized version. When training with using mini-batches, you can train a network with a Batch Normalization layer. The Batch Normalization layer are describes step by step as below image:\n",
    "![title](images/bn_forward.png)\n",
    "\n",
    "Then training the network is divided into several steps:\n",
    "* Initialize the whole network (by initilizing every layer)\n",
    "* Split training data into minibatches.\n",
    "* Iteratively do following steps in $E$ epochs:\n",
    "    * Shuffle minibatches.\n",
    "    * Do the forward pass through batch normalization layers and feedforward layers ( (the order of all layers is shown in the picture below)). You should notice that the input of first layer doesn't go to the Batch Normalization and the output of the last layer doesn't go to the Batch Normalization. Therefore, the number  of Batch Normalization layers would be less than the numer of Feed Forward layers. And you should work with index number of those layers very carefully.\n",
    "    * Calculate the loss.\n",
    "    * Do the backward pass.\n",
    "    * Update weights.\n",
    "![title](images/model.png)\n",
    "    \n",
    "With the trained model, perform inference steps on the test data. During the inference, the Batch Normalization use the mean and varience of the whole training data as shift mean and scale varience. Or you can make it simpler by using the mini-batches during the inference and use the shift mean and scale varience of Batch-Normalization.\n",
    "\n",
    "  \n",
    "First, let's import `numpy` and explore some basic methods: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement basic activation functions (vectorized version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "        # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperbolic Tangent__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "     # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    # The original formula is less preferred due to numerical instability:\n",
    "    #ez = np.exp(z)\n",
    "    #return ez / np.sum(ez) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is also correct, but suffers from overflows in case the values in z get large. (Try x * 1000 instead of x * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_unstable(z):\n",
    "    ez = np.exp(z)\n",
    "    return ez / np.sum(ez)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(4)\n",
    "print(x)\n",
    "print(softmax(x))\n",
    "print(softmax_unstable(x))\n",
    "print()\n",
    "\n",
    "x = x * 100\n",
    "print(x)\n",
    "print(softmax(x))\n",
    "print(softmax_unstable(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax for batch__:\n",
    "</br>\n",
    "\n",
    "Note that if we do minibatch update, we must compute the softmax along some dimension that is not the batch dimension. From our convention, batch dimension is the second (axis=1), so we must compute the softmax along the first dimension (axis=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_batch(z, axis=0):\n",
    "    ez = np.exp(z - np.max(z, axis=axis, keepdims=True))\n",
    "    return ez / np.sum(ez, axis=axis, keepdims=True)\n",
    "    # The original formula is less preferred due to numerical instability:\n",
    "    #ez = np.exp(z)\n",
    "    #return ez / np.sum(ez, axis=axis, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "z = np.random.randn(100,2)\n",
    "\n",
    "sigmoid_z = sigmoid(z)\n",
    "tanh_z = tanh(z)\n",
    "my_tanh_z = np.tanh(z)\n",
    "relu_z = relu(z)\n",
    "linear_z = linear(z)\n",
    "softmax_z = softmax(z)\n",
    "\n",
    "\n",
    "print(\"Sigmoid results of z[0]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_z[0,0], sigmoid_z[6,0], sigmoid_z[8,0], sigmoid_z[22,0], sigmoid_z[98,0]))\n",
    "print(\"Tanh results of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_z[3,0], tanh_z[9,0], tanh_z[18,0], tanh_z[34,0], tanh_z[99,0]))\n",
    "print(\"Relu results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_z[8,0], relu_z[16,0], relu_z[38,0], relu_z[72,0], relu_z[88,0]))\n",
    "print(\"Linear results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(linear_z[2,0], linear_z[6,0], linear_z[8,0], linear_z[22,0], linear_z[98,0]))\n",
    "print(\"Softmax results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(softmax_z[2,0], softmax_z[6,0], softmax_z[8,0], softmax_z[22,0], softmax_z[98,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "Sigmoid results of z[0]=0.615723, z[6]=0.721783, z[8]=0.599997, z[22]=0.561633, z[98]=0.459803 <br/>\n",
    "Tanh results of z[3]=0.696046, z[9]=0.867072, z[18]=0.124070, z[34]=0.208769, z[99]=0.280640 <br/>\n",
    "Relu results of z[8]=0.405453, z[16]=1.047579, z[38]=0.226963, z[72]=1.176812, z[88]=2.123692 <br/>\n",
    "Linear results of z[2]=-0.720589, z[6]=0.953324, z[8]=0.405453, z[22]=0.247792, z[98]=-0.161137 <br/>\n",
    "Softmax results of z[2]=0.001570, z[6]=0.008374, z[8]=0.004842, z[22]=0.004136, z[98]=0.002748 <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement the derivations of basic activation functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tanh__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectified Linear Unit (ReLU)__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass  # Theoretically if z = 0, relu_prime should return UNDEFINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prime(z):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Softmax__: \n",
    "\n",
    "This is hard to come up with without knowing the loss function. Most of the implementations out there are assuming we have cross entropy (or negative log likelihood) loss function. We will do the same, as the derivative of Softmax followed directly by Cross-Entropy Loss is far easier (see below). So, no need to implement Softmax derivative here. For your interest: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "\n",
    "This constrains the use of softmax to the last layer (where the loss function is applied directly on top of it), but that's fine for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8888)\n",
    "z = np.random.randn(100,1)\n",
    "\n",
    "sigmoid_prime_z = sigmoid_prime(z)\n",
    "tanh_prime_z = tanh_prime(z)\n",
    "relu_prime_z = relu_prime(z)\n",
    "\n",
    "print(\"Sigmoid prime results of z[2]=%f, z[6]=%f, z[8]=%f, z[22]=%f, z[98]=%f\" \\\n",
    "      %(sigmoid_prime_z[2,0], sigmoid_prime_z[6,0], sigmoid_prime_z[8,0], sigmoid_prime_z[22,0], sigmoid_prime_z[98,0]))\n",
    "print(\"Tanh prime of z[3]=%f, z[9]=%f, z[18]=%f, z[34]=%f, z[99]=%f\" \\\n",
    "      %(tanh_prime_z[3,0], tanh_prime_z[9,0], tanh_prime_z[18,0], tanh_prime_z[34,0], tanh_prime_z[99,0]))\n",
    "print(\"Relu prime results of z[8]=%f, z[16]=%f, z[38]=%f, z[72]=%f, z[88]=%f\" \\\n",
    "      %(relu_prime_z[8,0], relu_prime_z[16,0], relu_prime_z[38,0], relu_prime_z[72,0], relu_prime_z[88,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "Sigmoid prime results of z[2]=0.247928, z[6]=0.228343, z[8]=0.181437, z[22]=0.249350, z[98]=0.133364<br/>\n",
    "Tanh prime of z[3]=0.027258, z[9]=0.928484, z[18]=0.357361, z[34]=0.621576, z[99]=0.568175<br/>\n",
    "Relu prime results of z[8]=1.000000, z[16]=0.000000, z[38]=1.000000, z[72]=0.000000, z[88]=1.000000<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implement basic cost (error) function and its derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are going to calculate the error for a (mini)batch of $m$ training examples, which we assume they are [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) (i.i.d), so the (normalized) total errors of those $m$ training examples (or $m$ data points) is $E = \\frac{1}{m}\\sum_{i=1}^mE^{(i)}$, where $E^{(i)}$ is an error value of the $i^{th}$ training example. However, in order to vectorize the error over a (mini)batch, we should come up with the direct version of error function of $E$ instead of $E^{(i)}$.\n",
    "\n",
    "We implement Cross Entropy loss here, since we are trying to solve a classification problem with multiple classes (0-9). In the two class case, we could use Binary Cross Entropy, and for regression problems, Mean Squared Error, for instance. If you want to know more, look e.g. [here](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(Categorical) Cross Entropy Error__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyError(t, y, epsilon=1e-15):\n",
    "    # t: target label of shape(n,m)\n",
    "    # y: predicted value of shape(n,m)\n",
    "    \n",
    "    # To avoid numerical issues\n",
    "\n",
    "    # Remember to divide it by m\n",
    "    \n",
    "   # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: <b>Derivative of Cross Entropy</b><br/>\n",
    "The cross entropy-function expects its input $y$ to be a normalized probability distribution (or a batch thereof). Thus, the activiation function directly before the loss (i.e. the last activation function of the network) is usually the softmax function. You could calculate the derivate $\\frac{dL}{dy}$ and then $\\frac{dy}{dz}$ step by step, where $L = CE(t, y)$ is the CE loss and $y = \\text{softmax}(z)$. The input $z$ to the softmax is usually called the \"logits\". However, if you compute $\\frac{dL}{dz}$ analytically, you can show the following simple formula:<br/>\n",
    "\n",
    "$\\frac{dL}{dz} = y - t$\n",
    "\n",
    "You can implement this directly, just don't forget to properly handle the normalization due to the batched implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dCE_dz(t, y):\n",
    "    # Your implementation here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handy helper function\n",
    "def create_random_onehot(n, m):\n",
    "    index = np.eye(n)\n",
    "    return index[np.random.choice(index.shape[1], size=m)].T\n",
    "t = create_random_onehot(3, 7)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward\n",
    "\n",
    "np.random.seed(1111)\n",
    "z = np.random.randn(100,50)\n",
    "sum_z = np.sum(z, axis=0, keepdims=True)\n",
    "y = z / sum_z # Create softmax-like distribution\n",
    "t = create_random_onehot(100,50)\n",
    "\n",
    "# Test forward\n",
    "myCE = CrossEntropyError(t,y)\n",
    "\n",
    "print(\"My Cross Entropy Error: \" + str(myCE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Cross Entropy Error: 15.8303339039 <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backward\n",
    "\n",
    "dCE_dz1 =  dCE_dz(t, z)\n",
    "    \n",
    "print(\"My Cross Entropy Derivation: dCE_dz at dCE_dz[2,4]=%f, dCE_dz[6,5]=%f, dCE_dz[8,20]=%f, dCE_dz[22,7]=%f, dCE_dz[98,40]=%f\" \\\n",
    "      %(dCE_dz1[2,4], dCE_dz1[6,5], dCE_dz1[8,20], dCE_dz1[22,7], dCE_dz1[98,40]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "My Cross Entropy Derivation: dCE_dz at dCE_dz[2,4]=-0.000196, dCE_dz[6,5]=0.020035, dCE_dz[8,20]=0.025941, dCE_dz[22,7]=-0.001366, dCE_dz[98,40]=0.013498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Layer Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 Initialization of feed forward layer\n",
    "\n",
    "Initialize a feed forward layer: Use Xavier Initialization if the activation is sigmoid or tanh, otherwise use He Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(n_in, n_out, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the initialization for our layer\n",
    "\n",
    "    Arguments:\n",
    "    n_in -- size of previous layer\n",
    "    n_out -- size of current layer\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_string == 'sigmoid' or activation_string == 'tanh' or activation_string == 'softmax':\n",
    "        # Xavier Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(2. / (n_in + n_out)))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    else: \n",
    "        # He Initialization\n",
    "        W = np.random.randn(n_out, n_in) * (np.sqrt(1. / n_in))\n",
    "        b = np.zeros((n_out, 1))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2  Initialization of batch norm layer\n",
    "\n",
    "Initialize a batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_batchnorm(n_in):\n",
    "    \"\"\"\n",
    "    Implement the initialization for our layer\n",
    "\n",
    "    Arguments:\n",
    "    n_in -- size of previous layer\n",
    "    \n",
    "    Returns:\n",
    "    gamma: numpy array of scale varience vector (it would be np.ones vector)\n",
    "    beta: numpy array of shift mean vector (it would be np.zeros vector )\n",
    "    \"\"\"\n",
    "    gamma = np.ones(n_in, dtype=\"float32\") \n",
    "    beta = np.zeros(n_in, dtype=\"float32\")\n",
    "    return gamma, beta \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A_prev, W, b, activation_string):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for our layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data X): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation_string -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- values of (A_prev, Z, W, activation_string) we store for computing backward propagation efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your implementation here!\n",
    "    pass\n",
    "    \n",
    "    #return A, cache\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_batchnorm(A, gamma, beta, eps=0.0001):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for our batch normalization layer\n",
    "\n",
    "    Arguments:\n",
    "    A- activations from previous layer (or input data X): (size of previous layer, number of examples)\n",
    "    gamma -- shift mean parameter\n",
    "    b -- scale variance parameter\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the batch normalization, also called the normalized output\n",
    "    cache -- values of  (Ahat, gamma, Amu, ivar, sqrtvar, var, eps) we store for computing backward propagation efficiently\n",
    "    To store cache for backward pass, you have to compute:\n",
    "    Amu: the subtract of A and mu (mean value of mini-batch)\n",
    "    var: variance of mini-batch\n",
    "    sqrtvar: standard deviation of mini-batch\n",
    "    ivar: 1./sqrtvar\n",
    "    gamma : gamma parameter\n",
    "    Ahat: the normalized value with zero mean and unit varience\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your implementation here!\n",
    "    \n",
    "    \n",
    "    #return out, cache\n",
    " \n",
    "    cache = (Ahat, gamma, Amu, ivar, sqrtvar ,var, eps)\n",
    "    return out, cache\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test forward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2222)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "\n",
    "A, cache = forward(A_prev, W, b, \"sigmoid\")\n",
    "print(\"With sigmoid: Z = %s , A = %s\" % (str(cache[1]), str(A)))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"tanh\")\n",
    "print(\"With tanh: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "\n",
    "A, _ = forward(A_prev, W, b, \"softmax\")\n",
    "print(\"With softmax: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "With sigmoid: Z = [[ 1.1337088  -1.20334105]] , A = [[ 0.75652269  0.2308814 ]]<br/>\n",
    "With tanh: A = [[ 0.81228478 -0.83467086]]<br/>\n",
    "With ReLU: A = [[ 1.1337088  0.       ]]<br/>\n",
    "With softmax: A = [[ 1. 1.]]\n",
    "\n",
    "This exercise doesn't have test cases for batch normalization layers, you have to code and test it by yourself.\n",
    "\n",
    "Can you explain why the softmax output is only ones?\n",
    "What would have to be changed to get more \"useful\" values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that, if the activation function is softmax, you can assume that dA is already dz (see the explanation for CE derivative above). This assumes that softmax is used only as the last layer of the network, but that is ok for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for our layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- activation gradient for current layer l \n",
    "    cache -- values of (A_prev, Z, W, activation_string) we store for computing backward propagation efficiently, where\n",
    "        activation_string is the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\", \"tanh\" or \"softmax\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, Z, W, activation_string = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation_string == 'sigmoid':\n",
    "        dZ = dA * sigmoid_prime(Z)\n",
    "    elif activation_string == 'tanh':\n",
    "        dZ = dA * tanh_prime(Z)\n",
    "    elif activation_string == 'relu':\n",
    "        dZ = dA * relu_prime(Z)\n",
    "    elif activation_string == 'softmax':\n",
    "        dZ = dA\n",
    "    else:\n",
    "        dZ = dA * linear_prime(Z)\n",
    "    \n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward propagation for batch normalization layer is more complicated than the forward pass. You should draw the computation graph with pen and paper, then do the backward propagation for batch normalization layer step by step. It is a good chance the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_batchnorm(dout, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for batch normalization layer\n",
    "    \n",
    "    Arguments:\n",
    "    dout -- Gradient of the output \n",
    "    cache -- values of  (Ahat,gamma,Amu,ivar,sqrtvar,var,eps)  we store for computing backward propagation efficiently, where\n",
    "        activation_string is the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\", \"tanh\" or \"softmax\"\n",
    "    \n",
    "    Returns:\n",
    "    din -- Gradient of the input \n",
    "    dgamma -- Gradient of parameter gamma\n",
    "    dbeta -- Gradient of parameter beta\n",
    "    \"\"\"\n",
    "    Ahat, gamma, Amu, ivar, sqrtvar, var, eps = cache\n",
    "    \n",
    "    D, B = dout.shape[0], dout.shape[1]\n",
    "    ##  your implementation here\n",
    " \n",
    "    ## \n",
    "    \n",
    "    return din, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test Backward__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1111)\n",
    "dA = np.random.randn(1,2)\n",
    "A_prev = np.random.randn(3,2)\n",
    "W = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "Z = np.random.randn(1,2)\n",
    "\n",
    "cache_sigmoid = (A_prev, Z, W, \"sigmoid\")\n",
    "cache_tanh = (A_prev, Z, W, \"tanh\")\n",
    "cache_relu = (A_prev, Z, W, \"relu\")\n",
    "cache_softmax = (A_prev, Z, W, \"softmax\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_sigmoid)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_tanh)\n",
    "print (\"tanh:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_relu)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = backward(dA, cache_softmax)\n",
    "print (\"softmax:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "sigmoid:<br/>\n",
    "dA_prev = [[ 0.38601836  0.4093035 ]<br/>\n",
    " [ 0.04975095  0.05275199]<br/>\n",
    " [ 0.37804364  0.40084773]]<br/>\n",
    "dW = [[ 0.01722623 -0.1803237   0.09144594 ]]<br/>\n",
    "db = [[-0.25163402]]<br/>\n",
    "<br/>\n",
    "tanh:<br/>\n",
    "dA_prev = [[ 0.74460672  1.47719476]<br/>\n",
    " [ 0.09596665  0.19038431]<br/>\n",
    " [ 0.72922394  1.4466775 ]]<br/>\n",
    "dW = [[ 0.22431658 -0.34262738  0.38665635]]<br/>\n",
    "db = [[-0.70296173]]<br/>\n",
    "<br/>\n",
    "relu:<br/>\n",
    "dA_prev = [[ 2.05442539  0.        ]<br/>\n",
    " [ 0.26477914  0.        ]<br/>\n",
    " [ 2.01198317  0.        ]]<br/>\n",
    "dW = [[-0.51363355 -0.97619193 -0.17936819]]<br/>\n",
    "db = [[-0.65000516]]<br/>\n",
    "\n",
    "softmax:<br/>\n",
    "dA_prev = [[2.05442539 1.69566033]<br/>\n",
    " [0.26477914 0.21854066]<br/>\n",
    " [2.01198317 1.66062981]]<br/>\n",
    "dW = [[-0.04244893 -0.96335391  0.3390964 ]]<br/>\n",
    "db = [[-1.18649968]]<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Update parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(lr, W, b, gamma, beta, dW, db, dgamma, dbeta):\n",
    "    W1 = W - lr * dW \n",
    "    b1 = b - lr * db\n",
    "    gamma1 = gamma - lr * dgamma \n",
    "    beta1 = beta - lr * dbeta\n",
    "    return W1, b1, gamma1, beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Network Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_initialize(layer_sizes = [768,100,50,10], activations = [\"relu\",\"relu\",\"softmax\"], seed=9999, batchnorm=False):\n",
    "    \"\"\"\n",
    "    Initialize the parameters of a network.\n",
    "    \n",
    "    Arguments:\n",
    "    layer_sizes -- A list of layer sizes. \n",
    "    activations -- A list of corresponding activation functions.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Dictionary of initialized weights and biases for every layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    parameters = {} \n",
    "    parameters[\"BN\"] = batchnorm\n",
    "    parameters[\"num_layers\"] = len(layer_sizes) -1\n",
    "     #Write your code here\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "parameters = network_initialize([2,3,4],[\"relu\",\"softmax\"])\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <br/>\n",
    "{ &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W1': array([[-0.36418784, &nbsp;  &nbsp;  0.41853418], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [ 0.03385161, &nbsp;  &nbsp; -0.34535427], <br/>\n",
    "   &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.25098956, &nbsp;  &nbsp; -0.27705387]]), <br/>\n",
    " &nbsp;  &nbsp;  &nbsp;  &nbsp;    'W2': array([[ 0.99648945, &nbsp;  &nbsp; -0.73309211, &nbsp;  &nbsp;  1.18999424], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;       [-0.06313226,  &nbsp;  &nbsp; 0.06406165, &nbsp;  &nbsp;  0.09760321], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.35157642, &nbsp;  &nbsp; -0.87994782, &nbsp;  &nbsp; -0.61020235], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [-0.76922564, &nbsp;  &nbsp;  0.39821514, &nbsp;  &nbsp; -0.92965227]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;    'act1': 'relu', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'act2': 'softmax', <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;     'b1': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]]), <br/>\n",
    "&nbsp;  &nbsp;  &nbsp;  &nbsp;   'b2': array([[ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.], <br/>\n",
    "  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;  &nbsp;      [ 0.]])} <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Network Forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Do the forward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- The input of the size (n, m): n features and m instances (m = batch size).\n",
    "    parameters -- The weights and biases of every layers in the network.\n",
    "    test: test mode or training mode \n",
    "    Returns:\n",
    "    A -- Final activations (output activations).\n",
    "    caches -- the cached values for faster calculation of the backward step later.\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev = X\n",
    "    L = parameters[\"num_layers\"]  # Number of layers in the network\n",
    "    caches = []\n",
    "    caches_batchnorm = []\n",
    "    for l in range(L):\n",
    "        W = parameters['W' + str(l+1)]\n",
    "        b = parameters['b' + str(l+1)]\n",
    "        batchnorm = parameters[\"BN\"] \n",
    "        activation_string = parameters['act' + str(l+1)]\n",
    "        # Write your code here\n",
    "        \n",
    "        ###\n",
    "    return A, caches, caches_batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(5,4)\n",
    "W1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "W2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(3,3)\n",
    "b3 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"relu\",\n",
    "              \"act3\": \"softmax\",\n",
    "             \"num_layers\":3,\n",
    "             \"BN\":False}\n",
    "AL, caches, caches_batchnorm = network_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "assert (np.sum(AL,axis=0) - 1.0 < 1e-7).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <pre>\n",
    "AL = [[0.422804   0.04774435 0.01231299 0.3644841 ]\n",
    " [0.15449364 0.18041113 0.0384848  0.05945517]\n",
    " [0.42270236 0.77184451 0.94920221 0.57606073]]</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Calculate the error and error term of the last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(T, AL, error_string):\n",
    "    \"\"\"\n",
    "    Calculate the error and the error term of the last layer.\n",
    "    \n",
    "    Arguments:\n",
    "    T -- The target labels of the size (n, m): n features and m instances (m = batch size).\n",
    "    AL -- Final activations (output activations from the last layer).\n",
    "    error_string -- The string representing the error function: \"bce\", \"mse\" or \"ce\".\n",
    "    \n",
    "    Returns:\n",
    "    error -- The error value.\n",
    "    dAL -- the error term (the derivative of the error w.r.t the logits or activation, depending on loss function)\n",
    "            of the last layer.\n",
    "    \"\"\"\n",
    "    if error_string == 'ce':\n",
    "        error = CrossEntropyError(T, AL)\n",
    "        dAL = dCE_dz(T, AL)\n",
    "    else:\n",
    "        raise NameError(\"Your error string '%s' is undefined!\" % error_string)\n",
    "        \n",
    "    return error, dAL\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Network Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_backward(dAL, caches, caches_bachnorm, batchnorm=False):\n",
    "    \"\"\"\n",
    "    Do the backward pass through the layers.\n",
    "    \n",
    "    Arguments:\n",
    "    dAL -- the error term (the derivative of the error w.r.t the logits/activation) of the last layer.\n",
    "    caches -- the cached values from the forward step before.\n",
    "   \n",
    "    Returns:\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the weights and biases.\n",
    "    \"\"\"\n",
    "    L = len(caches) # Number of layers in the network\n",
    "    grads = {}\n",
    "    \n",
    "    dA = dAL\n",
    "    \n",
    "    for l in reversed(range(1,L+1)):\n",
    "        ##Write your code here\n",
    "       \n",
    "        ##\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,6)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "W3 = np.random.randn(3,3)\n",
    "b3 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"sigmoid\",\n",
    "              \"act3\": \"softmax\",\n",
    "              \"num_layers\": 3,\n",
    "              \"BN\": False}\n",
    "\n",
    "t = create_random_onehot(1,6)\n",
    "\n",
    "\n",
    "\n",
    "AL, caches, caches_batchnorm = network_forward(X, parameters)\n",
    "\n",
    "print(\"My activation last layer AL:\\n\", AL)\n",
    "assert (np.sum(AL,axis=0) - 1.0 < 1e-7).all()\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\" My error:\" + str(error))\n",
    "\n",
    "\n",
    "grads = network_backward(dAL, caches, caches_batchnorm, batchnorm=False)\n",
    "print(\"========================\")\n",
    "print(\"My W1 grad dW1:\\n\" + str(grads['dW1'])) \n",
    "print(\"My b1 grad db1:\\n\" + str(grads['db1']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <pre>\n",
    "My activation last layer AL:\n",
    " [[0.09088608 0.07286784 0.05814823 0.05320167 0.11602662 0.07744425]\n",
    " [0.14055201 0.12887469 0.09788826 0.07209716 0.03170778 0.08802502]\n",
    " [0.76856191 0.79825747 0.84396351 0.87470117 0.8522656  0.83453074]]\n",
    "========================\n",
    " My error:5.247787227725874\n",
    "========================\n",
    "My W1 grad dW1:\n",
    "[[ 0.03775917 -0.13460533  0.00529332  0.01752022 -0.0629297  -0.01215757\n",
    "   0.03408862]\n",
    " [-0.03195254  0.0216311   0.0018742   0.00287615  0.00453508  0.00942083\n",
    "   0.01544174]\n",
    " [-0.00225612 -0.00310389 -0.00039124 -0.00898049 -0.00827201 -0.00106814\n",
    "   0.00962552]\n",
    " [ 0.00673847  0.01248913 -0.00217332  0.00265921  0.00986236 -0.00773551\n",
    "  -0.02430467]\n",
    " [-0.00039606 -0.00028658 -0.0001538  -0.00524617 -0.00466888 -0.00359564\n",
    "   0.00079826]]\n",
    "My b1 grad db1:\n",
    "[[-0.08750011]\n",
    " [ 0.0184781 ]\n",
    " [-0.01685055]\n",
    " [ 0.01922185]\n",
    " [-0.00800348]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4. Update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_update(lr, parameters, grads):\n",
    "    \"\"\"\n",
    "    Update the parameters of all layers.\n",
    "    \n",
    "    Arguments:\n",
    "    lr -- learning rate.\n",
    "    parameters -- the parameters of the network to be updated.\n",
    "    grads -- a dictionary to store the calculated derivatives of the error w.r.t to the parameters.\n",
    "   \n",
    "    Returns:\n",
    "    parameters -- the updated parameters of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = parameters[\"num_layers\"] # Number of layers in the network\n",
    "    batchnorm = parameters[\"BN\"]\n",
    "   \n",
    "    for l in range(L):\n",
    "    ### write your code here\n",
    "    ### \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test your implementations__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(7,42)\n",
    "W1 = np.random.randn(5,7)\n",
    "b1 = np.random.randn(5,1)\n",
    "W2 = np.random.randn(3,5)\n",
    "b2 = np.random.randn(3,1)\n",
    "  \n",
    "parameters = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"act1\": \"relu\",\n",
    "              \"act2\": \"softmax\",\n",
    "             \"num_layers\":2,\n",
    "             \"BN\":False}\n",
    "\n",
    "t = create_random_onehot(3,42)\n",
    "\n",
    "\n",
    "\n",
    "AL, caches, caches_batchnorm = network_forward(X, parameters)\n",
    "assert (np.sum(AL,axis=0) - 1.0 < 1e-7).all()\n",
    "error, dAL = calculate_error(t, AL, \"ce\")\n",
    "grads = network_backward(dAL, caches, caches_batchnorm, batchnorm=False)\n",
    "\n",
    "parameters = network_update(0.5, parameters, grads)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your results should be__: <pre>\n",
    "{   'W1': array([[-0.06249507,  0.1009567 , -1.15371701, -0.67934372, -0.11264929,\n",
    "         0.32518354, -0.16797455],\n",
    "       [ 1.39405268,  0.74271938,  0.81618016, -1.750887  , -0.40265055,\n",
    "         0.20626771, -0.37441317],\n",
    "       [ 0.22714068, -0.56359779, -0.83631361, -0.05282548,  0.08429743,\n",
    "        -0.2912833 , -0.04432314],\n",
    "       [ 0.9678039 ,  0.50732055,  0.81611654,  0.60302219,  0.11759126,\n",
    "        -0.31545384,  1.83709966],\n",
    "       [ 0.15480503,  0.80619532, -0.58644989, -1.38971407, -1.18550834,\n",
    "        -0.27618439, -1.1432203 ]]),\n",
    "    'W2': array([[-0.41137921,  2.71294426,  0.46616392,  0.15284298, -0.38307355],\n",
    "       [ 1.29870494,  1.23561529, -2.34665498,  0.03516782,  0.32032454],\n",
    "       [ 0.61995548, -1.21818809,  0.03049422,  1.76333092,  1.2068194 ]]),\n",
    "    'act1': 'relu',\n",
    "    'act2': 'softmax',\n",
    "    'b1': array([[ 0.82585692],\n",
    "       [-0.3726393 ],\n",
    "       [-0.43115154],\n",
    "       [ 1.65224981],\n",
    "       [-0.31461457]]),\n",
    "    'b2': array([[-0.92573339],\n",
    "       [ 0.39740339],\n",
    "       [-0.66789375]])}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Read MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Data/mnist_seven.csv\"\n",
    "data = np.genfromtxt(data_path, delimiter=\",\", dtype=\"uint8\")\n",
    "train, dev, test = data[:4000], data[4000:4500], data[4500:]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    X = dataset[:, 1:] / 255.     # Normalize input features\n",
    "    Y_temp = dataset[:, 0]\n",
    "    \n",
    "    # Convert labels to one-hot vectors\n",
    "    n_values = np.max(Y_temp) + 1\n",
    "    Y = np.eye(n_values)[Y_temp]\n",
    "\n",
    "    return X.T, Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = normalize(train)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "\n",
    "X_dev, Y_dev = normalize(dev)\n",
    "print(X_dev.shape, Y_dev.shape)\n",
    "\n",
    "X_test, Y_test = normalize(test)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "print(Y_test[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate(parameters):\n",
    "    AL_dev, _ ,_= network_forward(X_dev, parameters)\n",
    "    error, _ = calculate_error(Y_dev, AL_dev, 'ce')\n",
    "    return error\n",
    "\n",
    "\n",
    "def train(epochs=100, batch_size=200, lr=15, \n",
    "          layer_sizes = [784,100,50,10],\n",
    "          activations = [\"sigmoid\", \"sigmoid\", \"softmax\"], verbose=True, batchnorm = False):\n",
    "    # Train\n",
    "    print(\"Training...\")\n",
    "    np.random.seed(1234)\n",
    "    parameters = network_initialize(layer_sizes, activations,batchnorm=batchnorm)\n",
    "    train_loss_log = []\n",
    "    dev_loss_log = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        L = 0\n",
    " \n",
    "        # Split into minibatches into a *list* of sub-arrays\n",
    "        # we want to split along the number of instances, so axis = 1\n",
    "        nmb = X_train.shape[1] // batch_size # drop last minibatch\n",
    "        X_minibatch = np.array_split(X_train, nmb, axis = 1)\n",
    "        Y_minibatch = np.array_split(Y_train, nmb, axis = 1) \n",
    "\n",
    "        # We shuffle the minibatches of X and Y in the same way\n",
    "        np.random.seed(8888)\n",
    "        shuffled_index = np.random.permutation(range(nmb))\n",
    "\n",
    "        # Now we can do the training, we cannot vectorize over different minibatches\n",
    "        # They are like our \"epochs\"\n",
    "        for i in range(nmb):\n",
    "            X_current = X_minibatch[shuffled_index[i]]\n",
    "            Y_current = Y_minibatch[shuffled_index[i]]         \n",
    "\n",
    "        #   Those two commented lines are for training Batch GD   \n",
    "        #   AL, caches = network_forward(X_train, parameters)\n",
    "        #   error, dAL = calculate_error(Y_train, AL, \"ce\")\n",
    "            AL, caches, caches_batchnorm = network_forward(X_current, parameters)\n",
    "            error, dAL = calculate_error(Y_current, AL, \"ce\")\n",
    "            grads = network_backward(dAL, caches, caches_batchnorm, batchnorm=batchnorm)\n",
    "            parameters = network_update(lr, parameters, grads)\n",
    "            L += error\n",
    "\n",
    "        train_loss_log.append(L/nmb)\n",
    "        dev_loss_log.append(validate(parameters))\n",
    "        if verbose:\n",
    "            print(\"Error of the epoch {0}: train {1:.8f}, val {2:.8f}\".format(\n",
    "                epoch + 1, train_loss_log[-1], dev_loss_log[-1]))\n",
    "        else:\n",
    "            print(epoch + 1, end='\\r')\n",
    "\n",
    "    print(\"...Done!\")\n",
    "    return parameters, (train_loss_log, dev_loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bfe2bb114dec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m run1_parameters, run1_logs = train(lr=1, epochs=10, batch_size=128, \n\u001b[0m\u001b[0;32m      2\u001b[0m                                    \u001b[0mlayer_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m6\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                    \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m7\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                    verbose=True, batchnorm = False )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "run1_parameters, run1_logs = train(lr=1, epochs=15, batch_size=64, \n",
    "                                   layer_sizes = [784] + [100]*6 +[50, 10],\n",
    "                                   activations = ['relu'] * 7 + [\"softmax\"],\n",
    "                                   verbose=True, batchnorm = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# plot the data\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "xdata = list(range(100))\n",
    "\n",
    "def plot(ax, logs, name):\n",
    "    epochs = len(logs[0])\n",
    "    ax.plot(xdata[:epochs], logs[0], color='tab:blue')\n",
    "    ax.plot(xdata[:epochs], logs[1], color='tab:orange')\n",
    "    ax.set(xlabel='epoch', ylabel='loss', title=name)\n",
    "    ax.set_ylim([0, 2.5])\n",
    "    #ax.set_ylim([1e-1, 2.5])\n",
    "    #ax.set_yscale('log')\n",
    "\n",
    "plot(ax1, run1_logs, 'without_batchnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run2_parameters, run2_logs = train(lr=1, epochs=15, batch_size=64, \n",
    "                                   layer_sizes = [784] + [100]*6 +[50, 10],\n",
    "                                   activations = ['relu'] * 7 + [\"softmax\"],\n",
    "                                   verbose=True, batchnorm = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "# plot the data\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "xdata = list(range(100))\n",
    "\n",
    "def plot(ax, logs, name):\n",
    "    epochs = len(logs[0])\n",
    "    ax.plot(xdata[:epochs], logs[0], color='tab:blue')\n",
    "    ax.plot(xdata[:epochs], logs[1], color='tab:orange')\n",
    "    ax.set(xlabel='epoch', ylabel='loss', title=name)\n",
    "    ax.set_ylim([0, 2.5])\n",
    "    #ax.set_ylim([1e-1, 2.5])\n",
    "    #ax.set_yscale('log')\n",
    "\n",
    "plot(ax1, run1_logs, 'with_batchnorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test = X_test.shape[1]\n",
    "AL_test, _,_ = network_forward(X_test, run1_parameters)\n",
    "correct = (np.argmax(Y_test, axis=0) == np.argmax(AL_test, axis=0)).sum()\n",
    "\n",
    "print('Accuracy on the test images: %d %%' % (100 * correct / m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_test = X_test.shape[1]\n",
    "AL_test, _,_ = network_forward(X_test, run2_parameters)\n",
    "correct = (np.argmax(Y_test, axis=0) == np.argmax(AL_test, axis=0)).sum(())\n",
    "\n",
    "print('Accuracy on the test images: %d %%' % (100 * correct / m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should produce around 90% accuracy with batch normolization, which is not bad (you can get less accuracy without batch normalization). Notice the learning rate. Change it and play with it. In our simple SGD-based model, it is an important hyperparameter and the performance of our model is learning rate-sensitive. To document your experiments with the hyperparameters, you can also refactor the train and test steps into a function with corresponding parameters, and then call it multiple times. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
