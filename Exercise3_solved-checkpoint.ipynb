{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T15:14:43.910000+01:00",
     "start_time": "2022-11-18T14:14:43.723Z"
    }
   },
   "outputs": [],
   "source": [
    "# library containing all previously implemented optimization algorithms\n",
    "# --> will be continously expanded within this course\n",
    "include(\"optimization_library.jl\");\n",
    "include(\"mplstyle.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2a\n",
    "Implement the newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:04:56.214000+01:00",
     "start_time": "2022-11-18T15:04:56.202Z"
    }
   },
   "outputs": [],
   "source": [
    "# f: Objective function\n",
    "# df: Gradient of the objective function\n",
    "# Hf: Hessian of the objective function\n",
    "# x0: Initial point\n",
    "# ls: Use linesearch?\n",
    "# ε: Stopping criterion on the newton decrement\n",
    "# maxiters: Max number of iterations\n",
    "\n",
    "function newton_descent(f, df, Hf, x0; ls=true, eps=0.000001, maxiters=1000)\n",
    "    # make a copy of initial point to prevent changing x0 by manipulating x\n",
    "    x = copy(x0)\n",
    "    # store the trace of the descent path\n",
    "    trace = [x; f(x)]\n",
    "    \n",
    "    for _=1:maxiters\n",
    "        # ==========================================================\n",
    "        # 1. evaluate the newton step d_nt (see Exercise 3.1)\n",
    "        \n",
    "        # hint for inverse divison operator in Julia: inv(A)*x  is equivalent to A \\ x\n",
    "        d = Hf(x) \\ (-df(x))\n",
    "        \n",
    "        # 2. compute the newton decrement λ and break the loop if λ^2/2 < ϵ\n",
    "        nwt_decrement2 = -df(x)'*d\n",
    "        \n",
    "        if 0.5*nwt_decrement2 < eps\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        # 3. if (ls = true) reduce the step length by performing backtracking linesearch\n",
    "        if ls\n",
    "            d = linesearch(f, df, x, d) * d\n",
    "        end\n",
    "        \n",
    "        # 4. update the point x\n",
    "        x += d\n",
    "        # ===========================================================        \n",
    "        trace = hcat(trace, [x; f(x)])\n",
    "    end\n",
    "    return x,trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2b: Marathon Training Reviewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/laufbahn.jpeg\" width=\"350\">\n",
    "\n",
    "Remember the Marathon training from Exercise 2 where we found the optimal model for the given data by applying gradient descent. Now we want to compare with the result from the Newton Method.\n",
    "\n",
    "We still assume a linear model with velocity $v$ scaling linearly with time $t$:\n",
    "\\begin{equation}\n",
    "    m(t) = v t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:05:11.876000+01:00",
     "start_time": "2022-11-18T15:05:11.857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Given Dataset\n",
    "\n",
    "# times in minutes\n",
    "t = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120]\n",
    "\n",
    "# distances in kilometers\n",
    "d = [1.88, 4.47, 5.63, 8.13, 8.54, 11.23, 12.27, 14.23, 15.50, 16.93, 18.69, 21.31];\n",
    "\n",
    "# linear model\n",
    "function m(t, v)\n",
    "    # The times are measured in minutes. We convert them in units of hours by dividing by 60. Like this, the velocity\n",
    "    # has units of km/h\n",
    "    return t .* (v/60)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:05:47.648000+01:00",
     "start_time": "2022-11-18T15:05:47.642Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss(v) = sum((d - m(t,v)).^2)\n",
    "\n",
    "# Derivative of the loss function\n",
    "function ∇loss(v)\n",
    "    ∇inner = -t./60\n",
    "    ∇outer = 2*(d - m(t, v))\n",
    "    return ∇inner' * ∇outer\n",
    "end\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# Define the Hessian of the loss function\n",
    "H_loss(v) = 2*sum((t./60).^2)\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:05:57.700000+01:00",
     "start_time": "2022-11-18T15:05:57.012Z"
    }
   },
   "outputs": [],
   "source": [
    "x0 = 1\n",
    "\n",
    "# Result from Gradient Descent\n",
    "result_grd,trace_grd = gradient_descent(loss, ∇loss, x0, maxiters=10000, ϵ=0.0001, ls = true)\n",
    "println(\"Optimal velocity [km/h]: \", result_grd)\n",
    "println(\"Optimal loss: \", trace_grd[end,end])\n",
    "println(\"Gradient at optimum: \", ∇loss(result_grd))\n",
    "println(\"Iterations: \", size(trace_grd,2)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:06:00.868000+01:00",
     "start_time": "2022-11-18T15:05:59.890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Result of Newton Method\n",
    "# ==========================================================\n",
    "result_nwt, trace_nwt = newton_descent(loss, ∇loss, H_loss, x0; ls=true, eps=0.0001, maxiters=1000)\n",
    "println(\"Optimal velocity [km/h]: \", result_nwt)\n",
    "println(\"Optimal loss: \", trace_nwt[end,end])\n",
    "println(\"Gradient at optimum: \", ∇loss(result_nwt))\n",
    "println(\"Iterations: \", size(trace_nwt,2)-1)\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:06:15.420000+01:00",
     "start_time": "2022-11-18T15:06:15.409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test if the results from gradient descent and Newton method are the same\n",
    "# ==========================================================\n",
    "@assert result_nwt - result_grd < 0.0001\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:06:20.759000+01:00",
     "start_time": "2022-11-18T15:06:20.179Z"
    }
   },
   "outputs": [],
   "source": [
    "fontsize = 15\n",
    "# plot the trace\n",
    "\n",
    "# create values of velocities for which the loss function should be evaluated\n",
    "test_velocities = range(1,15,length = 100)\n",
    "\n",
    "# evaluate the loss function for some test velocities\n",
    "losses = zeros(size(test_velocities,1))\n",
    "for (index,test) in enumerate(test_velocities)\n",
    "    losses[index] = loss(test)\n",
    "end\n",
    "\n",
    "plot(test_velocities',losses, color = \"blue\")\n",
    "plot(trace_grd[1,:],trace_grd[2,:], marker = \"o\", color = \"green\", label = \"Gradient descent\")\n",
    "\n",
    "# =============================================================================================\n",
    "# add the trace of the Newton Method to the plot\n",
    "plot(trace_nwt[1,:],trace_nwt[2,:], marker = \"o\", color = \"red\", label = \"Newton method\")\n",
    "# =============================================================================================\n",
    "\n",
    "\n",
    "xlabel(\"v [km/h]\", fontsize=fontsize)\n",
    "ylabel(L\"$l(v)$\", fontsize=fontsize)\n",
    "\n",
    "#xlim(9,12)\n",
    "#ylim(0,50)\n",
    "\n",
    "legend(fontsize = fontsize)\n",
    "title(\"Fit of linear Model\", fontsize = fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.2c\n",
    "\n",
    "The linear model contains only one parameter that is estimated. However, the performance of gradient descent and Newton method differ most severely on high-dimensional problems, i.e. when the model contains a high number of parameters. Hence, we use the dataset from the marathon training an perform a fit of a higher-order polynomial (irrespectively if we know that this makes sense or not). We are interested to see how many iterations each method needs to reach the optimal combination of parameters $\\theta_i$.\n",
    "\n",
    "A polynomial of degree $d$ is given by:\n",
    "\\begin{equation}\n",
    "P_d(\\mathbf{t}) = \\sum_{j = 0}^d \\theta_j \\mathbf{t}^j\n",
    "\\end{equation}\n",
    "Note that $\\mathbf{t}$ is a vector containing all measured times during the marathon training. In this case $\\mathbf{t}^j$ is the vector resulting from evaluating the $j$-th power of each element in $\\mathbf{t}$:\n",
    "\\begin{equation}\n",
    "\\mathbf{t}^j = (t_1^j, t_2^j, \\dots, t_n^j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:07:59.317000+01:00",
     "start_time": "2022-11-18T15:07:59.310Z"
    }
   },
   "outputs": [],
   "source": [
    "# generic implementation of a polynomial of arbitrary degree d\n",
    "function model_poly(t, θ)\n",
    "    # get the number of parameters. The polynomial degree d is given by d = n - 1\n",
    "    n = size(θ,1)\n",
    "    return ((t./60) .^ [0:n-1;]') * θ\n",
    "end\n",
    "\n",
    "# ===============================================================================\n",
    "# implement the quadratic loss between the datapoints and model_poly(t,params)\n",
    "\n",
    "loss_poly(θ) = sum((d .- model_poly(t, θ)).^2)\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "\n",
    "# gradient of the loss function\n",
    "function ∇loss_poly(θ)\n",
    "    # get the number of parameter of the polynomial model\n",
    "    n = size(θ,1)\n",
    "    # ===============================================================================\n",
    "    # Implement the gradient vector\n",
    "    # Note that you always have to divide the measured times t by 60 in order to be consistent\n",
    "    # with the rest of the notebook\n",
    "    \n",
    "    ∇outer = (2 .* (d .- model_poly(t, θ)))'\n",
    "    ∇inner = (t./60) .^ [0:n-1;]'  # == ∇model\n",
    "    \n",
    "    ∇loss = ∇outer * ∇inner\n",
    "    \n",
    "    return - ∇loss'\n",
    "    \n",
    "    # ===============================================================================\n",
    "end\n",
    "\n",
    "# Hessian of the loss function\n",
    "function H_loss_poly(θ)\n",
    "    # get the number of parameter of the polynomial model\n",
    "    n = size(θ,1)\n",
    "    # ===============================================================================\n",
    "    # Implement the hessian matrix\n",
    "    # Note that you always have to divide the measured times t by 60 in order to be consistent\n",
    "    # with the rest of the notebook\n",
    "    \n",
    "    ∇model = (t./60) .^ [0:n-1;]'\n",
    "    return 2 .* (∇model' * ∇model)\n",
    "    \n",
    "    # ===============================================================================\n",
    "    # get the number of parameter of the polynomial model\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:10:17.818000+01:00",
     "start_time": "2022-11-18T15:10:17.783Z"
    }
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(42);\n",
    "# define the number of paramaters in the polynomial model\n",
    "n_θ = 2\n",
    "\n",
    "# create an array of random numbers to get a starting point\n",
    "θ_0 = 10*rand(n_θ)\n",
    "\n",
    "# Result from Gradient Descent\n",
    "# ===============================================================================\n",
    "# uncomment the following lines\n",
    "result_grd2, trace_grd2 = gradient_descent(loss_poly, ∇loss_poly, θ_0, maxiters=10000, ϵ=0.0001, p=0.5)\n",
    "println(\"Minimizer: \", result_grd2)\n",
    "println(\"Starting point: \" ,(trace_grd2[1,1], trace_grd2[2,1]))\n",
    "println(\"Optimal loss: \", trace_grd2[end,end])\n",
    "println(\"Gradient at optimum: \",∇loss_poly(result_grd2))\n",
    "println(\"Iterations: \", size(trace_grd2,2)-1)\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:10:18.833000+01:00",
     "start_time": "2022-11-18T15:10:18.813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Result of Newton Method\n",
    "# ==========================================================\n",
    "result_nwt2, trace_nwt2 = newton_descent(loss_poly, ∇loss_poly, H_loss_poly, θ_0; ls=true, eps=0.0001, maxiters=1000)\n",
    "println(\"Minimizer: \", result_nwt2)\n",
    "println(\"Starting point: \", (trace_nwt2[1,1], trace_nwt2[2,1]))\n",
    "println(\"Optimal loss: \", trace_nwt2[end,end])\n",
    "println(\"Gradient at optimum: \", ∇loss_poly(result_nwt2))\n",
    "println(\"Iterations: \", size(trace_nwt2,2)-1)\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:10:19.668000+01:00",
     "start_time": "2022-11-18T15:10:19.666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test if the results from gradient descent and Newton method are the same\n",
    "# ==========================================================\n",
    "# uncomment the following line\n",
    "@assert sum(result_nwt2 - result_grd2) < 0.0001\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:10:22.031000+01:00",
     "start_time": "2022-11-18T15:10:21.412Z"
    }
   },
   "outputs": [],
   "source": [
    "pygui(false)\n",
    "# Plot linear and higher order polynomial models in order to compare to data points\n",
    "\n",
    "# create more points for which the model can be plotted in order to get a smooth curve\n",
    "times_plot = range(0, stop=140)\n",
    "    \n",
    "# plot data\n",
    "scatter(t, d, color=\"black\",zorder=3, label=\"data\")\n",
    "\n",
    "# plot models\n",
    "plot(times_plot, m(times_plot, result_grd), color=\"red\", label=\"linear model\")\n",
    "\n",
    "# plot the result of the newton method for high degree polynomial\n",
    "plot(times_plot, model_poly(times_plot, result_grd2), color=\"blue\", label=\"high degree polynomial (gradient)\")\n",
    "\n",
    "# ===============================================================================\n",
    "# add the curve of the best fitting polynomial model evaluated by the newton method\n",
    "plot(times_plot, model_poly(times_plot, result_nwt2)', color=\"green\", label=\"high degree polynomial (newton)\")\n",
    "# ===============================================================================\n",
    "\n",
    "xlabel(\"t [min]\")\n",
    "ylabel(\"d [km]\")\n",
    "\n",
    "legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:10:24.355000+01:00",
     "start_time": "2022-11-18T15:10:22.922Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the loss function for the case that the model is of degree 2, i.e. has 2 parameters\n",
    "\n",
    "if n_θ == 2\n",
    "\n",
    "    n_points = 50\n",
    "    θ1_plot = range(minimum(trace_grd2[1,:])-0.1*minimum(trace_grd2[1,:]),maximum(trace_grd2[1,:])+0.1*maximum(trace_grd2[1,:]),length = n_points)\n",
    "    θ2_plot = range(minimum(trace_grd2[2,:])-0.1*minimum(trace_grd2[2,:]),maximum(trace_grd2[2,:])+0.1*maximum(trace_grd2[2,:]),length = n_points)\n",
    "\n",
    "    losses_plot = [loss_poly([θ1,θ2]) for θ1 = θ1_plot, θ2 = θ2_plot]\n",
    "\n",
    "    # create grid points\n",
    "    xgrid = repeat(θ1_plot',n_points,1)\n",
    "    ygrid = repeat(θ2_plot,1,n_points)\n",
    "\n",
    "    # allow window to open for interactions\n",
    "    using3D()\n",
    "    pygui(true);\n",
    "    fig = figure(\"pyplot_surfaceplot\",figsize=(15,10))\n",
    "    ax = fig.add_subplot(1,1,1,projection=\"3d\")\n",
    "\n",
    "    # plot loss function\n",
    "    plot_surface(xgrid', ygrid', losses_plot, rstride=2,edgecolors=\"k\", cstride=2, cmap=ColorMap(\"gray\"), alpha=0.3, linewidth=0.25)\n",
    "    \n",
    "    # plot traces of gradient and newton descent\n",
    "    ax[:plot](trace_nwt2[1,:], trace_nwt2[2,:], trace_nwt2[3,:], color=\"red\", zorder = 3)\n",
    "    ax[:scatter](trace_nwt2[1,:], trace_nwt2[2,:], trace_nwt2[3,:], color=\"red\")\n",
    "    ax[:plot](trace_grd2[1,:], trace_grd2[2,:], trace_grd2[3,:], color=\"orange\")\n",
    "    ax[:scatter](trace_grd2[1,:], trace_grd2[2,:], trace_grd2[3,:], color=\"orange\")\n",
    "    \n",
    "    xlabel(L\"\\theta_1\", fontsize=fontsize)\n",
    "    ylabel(L\"\\theta_2\", fontsize=fontsize)\n",
    "    zlabel(L\"loss\", fontsize=fontsize)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:11:25.317000+01:00",
     "start_time": "2022-11-18T15:11:25.309Z"
    }
   },
   "outputs": [],
   "source": [
    "# f: objective function\n",
    "# ∇f: gradient of the objective function\n",
    "# x0: initial point\n",
    "# ε: stopping criterion on the norm of the gradient\n",
    "# maxiters: max numbers of gradient descent steps\n",
    "\n",
    "function gradient_descent_unchecked(f, ∇f, x0; ϵ=0.0001, maxiters = 1000, p = 0.5, lr=nothing)\n",
    "\n",
    "    x = copy(x0)\n",
    "    \n",
    "    # store initial point and evaluated function at initial point \n",
    "    trace = [x; f(x)]\n",
    "    for _=1:maxiters\n",
    "        \n",
    "        # ======================================================\n",
    "        # 1. evaluate the descent step direction at the point x\n",
    "        d = -∇f(x)\n",
    "        # 2. check if norm of the gradient is close enough to zero (use precision of ϵ). If yes break from the\n",
    "        #    loop.\n",
    "        if LA.norm(d) < ϵ\n",
    "            break\n",
    "        end\n",
    "        # 3. Evaluate the next point x by stepping into the descent direction.\n",
    "        # Use the result of the backtracking linesearch to guarantee convergence.\n",
    "        if lr === nothing\n",
    "            d = linesearch(f, ∇f, x, d, p = p) * d\n",
    "        else\n",
    "            d = lr * d\n",
    "        end\n",
    "        x += d\n",
    "        \n",
    "        # ======================================================\n",
    "        \n",
    "        trace = hcat(trace,[x; f(x)])\n",
    "        # @assert trace[end,end] <= trace[end,end-1]\n",
    "    end\n",
    "    return x,trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:12:52.215000+01:00",
     "start_time": "2022-11-18T15:12:52.196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Result from Overshooting Gradient Descent\n",
    "# ===============================================================================\n",
    "# uncomment the following lines\n",
    "result_grd2_bad,trace_grd2_bad = gradient_descent_unchecked(loss_poly, ∇loss_poly, θ_0, maxiters=100, ϵ=0.0001, lr=0.02)\n",
    "println(\"Minimizer: \",result_grd2_bad)\n",
    "println(\"Optimal loss: \",trace_grd2_bad[end,end])\n",
    "println(\"Gradient at optimum: \",∇loss_poly(result_grd2_bad))\n",
    "println(\"Iterations: \", size(trace_grd2_bad,2))\n",
    "# ==============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T16:12:55.171000+01:00",
     "start_time": "2022-11-18T15:12:53.554Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the loss function for the case that the model is of degree 2, i.e. has 2 parameters\n",
    "#pygui(false)\n",
    "if n_θ == 2\n",
    "\n",
    "    n_points = 50\n",
    "    θ1_plot = range(minimum(trace_grd2[1,:])-0.1*minimum(trace_grd2[1,:]),maximum(trace_grd2[1,:])+0.1*maximum(trace_grd2[1,:]),length = n_points)\n",
    "    θ2_plot = range(minimum(trace_grd2[2,:])-0.1*minimum(trace_grd2[2,:]),maximum(trace_grd2[2,:])+0.1*maximum(trace_grd2[2,:]),length = n_points)\n",
    "\n",
    "    losses_plot = [loss_poly([θ1,θ2]) for θ1 = θ1_plot, θ2 = θ2_plot]\n",
    "\n",
    "    # create grid points\n",
    "    xgrid = repeat(θ1_plot',n_points,1)\n",
    "    ygrid = repeat(θ2_plot,1,n_points)\n",
    "\n",
    "    # allow window to open for interactions\n",
    "    using3D()\n",
    "    pygui(true);\n",
    "    fig = figure(\"pyplot_surfaceplot\",figsize=(15,10))\n",
    "    ax = fig.add_subplot(1,1,1,projection=\"3d\")\n",
    "\n",
    "    # plot loss function\n",
    "    plot_surface(xgrid', ygrid', losses_plot, rstride=2,edgecolors=\"k\", cstride=2, cmap=ColorMap(\"gray\"), alpha=0.3, linewidth=0.25)\n",
    "    \n",
    "    # plot traces of gradient and newton descent\n",
    "    ax[:plot](trace_nwt2[1,:], trace_nwt2[2,:], trace_nwt2[3,:], color=\"red\", zorder = 3)\n",
    "    ax[:scatter](trace_nwt2[1,:], trace_nwt2[2,:], trace_nwt2[3,:], color=\"red\")\n",
    "    ax[:plot](trace_grd2[1,:], trace_grd2[2,:], trace_grd2[3,:], color=\"orange\")\n",
    "    ax[:scatter](trace_grd2[1,:], trace_grd2[2,:], trace_grd2[3,:], color=\"orange\")\n",
    "    ax[:plot](trace_grd2_bad[1,:], trace_grd2_bad[2,:], trace_grd2_bad[3,:], color=\"blue\")\n",
    "    ax[:scatter](trace_grd2_bad[1,:], trace_grd2_bad[2,:], trace_grd2_bad[3,:], color=\"blue\")\n",
    "\n",
    "    xlabel(L\"\\theta_1\", fontsize=fontsize)\n",
    "    ylabel(L\"\\theta_2\", fontsize=fontsize)\n",
    "    zlabel(L\"loss\", fontsize=fontsize)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
